---
title: "Final Project Script"
author: "Christian Conroy & Lawrence Doppelt"
date: "May 6, 2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

## The Data

For this project, we use data provided by Yelp for the Yelp Dataset challenge, which can be found in JSON format from this website: 

https://www.yelp.com/dataset/challenge

Documentation describing the variables and information contained in each json file comprising the yelp challenge dataset can be found here:

https://www.yelp.com/dataset/documentation/json

We then merge it with Zillow data on rental values across the United States by postal code. The Zillow rental value dataset can be downloaded from this website:

https://www.zillow.com/research/data/

## Setting the Working Directory and Loading in Required Packages

```{r setup, include=FALSE}
library(knitr)
library(zoo)
library(readr)
library(tidyr)
library(dplyr)
library(Amelia)
library(lattice)
library(stringr)
library(jsonlite)
library(mitools)
library(plm)
library(ggplot2)
# install.packages('directlabels')
library(directlabels)
library(plm)

opts_knit$set(root.dir ="~/GeorgetownMPPMSFS/McCourtMPP/Semester4MPP/DataScienceIntro")

```

## Importing the data

First, we import the Yelp Check-In data and flatten it. 
```{r import, results="hide", warning=FALSE, message=FALSE}
# Stream in Check-In Data
yelp_checkin <- as.data.frame(jsonlite::stream_in(file("dataset/checkin.json")), flatten = TRUE)
# Flatten Check-In Data 
renquote <- function(l) if (is.list(l)) lapply(l, renquote) else enquote(l)
yelp_checkin_flat <- as.data.frame(lapply(unlist(renquote(yelp_checkin)), eval))
```

## Reshaping the Data

We clean the time period variable names by collapsing the data to long form and using string functions to isolate the name of the weekday in string format.
```{r reshape}
# Convert from wide to long
yelp_checkin_flat_long <- reshape(yelp_checkin_flat, varying = list(names(yelp_checkin_flat[1:168])), times = names(yelp_checkin_flat[1:168]), idvar = 'business_id', v.names = 'checkin' , direction = 'long')

# Eliminate punctuation and digits
yelp_checkin_flat_long$time <- str_replace(yelp_checkin_flat_long$time, "time.", "")
yelp_checkin_flat_long$time <- gsub('[[:digit:]]+', '', yelp_checkin_flat_long$time)

# Isolate name of weekday
yelp_checkin_flat_long$time = substr(yelp_checkin_flat_long$time,1,nchar(yelp_checkin_flat_long$time)-2)
```


Here, we collapse the data, reshape it from long to wide, then merge them together. 


```{r collapse convert and merge}
# Aggregate Check-In by business and time period to get Check-In average and total by business for each day of the week. 
yelp_checkin_collapse_mean <- as.data.frame(aggregate(checkin ~ business_id + time, yelp_checkin_flat_long , mean))
yelp_checkin_collapse_sum <- as.data.frame(aggregate(checkin ~ business_id + time, yelp_checkin_flat_long , sum))

# Convert from long to wide
yelp_checkin_wide_mean <- spread(yelp_checkin_collapse_mean, key = time, value = checkin)
yelp_checkin_wide_sum <- spread(yelp_checkin_collapse_sum, key = time, value = checkin)

# Merge averages and totals
yelp_checkin_wide <- inner_join(yelp_checkin_wide_mean, yelp_checkin_wide_sum, by='business_id', match='all')
colnames(yelp_checkin_wide) <- c("business_id", "Friday_ave", "Monday_ave", "Saturday_ave", "Sunday_ave", "Thursday_ave", "Tuesday_ave", "Wednesday_ave", "Friday_total", "Monday_total", "Saturday_total", "Sunday_total", "Thursday_total", "Tuesday_total", "Wednesday_total")
```

## Loading in Business Dataset to Merge with Check-In Data and Aggregate by Zip Code. Eliminate Useless Columns and Merge.

```{r load business and mege, warning = FALSE, message=FALSE}
# Import business dataset
yelp_business <- jsonlite::stream_in(file("dataset/business.json"), flatten = TRUE)

# Merge check-in data with business data by business
checkinbiz <- inner_join(yelp_business, yelp_checkin_wide, by=c('business_id'), match='all')

# Eliminate unnecessary columns
checkinbiz <- checkinbiz[-c(2:6, 8:101)]

# Collapse check-in data by zipcode to get total and average check-ins for each weekday and zipcode
checkinzipmean <- as.data.frame(aggregate(. ~ postal_code, checkinbiz[2:9], mean))
checkinzipsum <- as.data.frame(aggregate(. ~ postal_code, checkinbiz[c(2, 10:16)], sum))

checkinfull <- inner_join(checkinzipmean, checkinzipsum, by=c('postal_code'), match='all')
```
## Importing the Yelp Review Data

Due to the large size of the Yelp Review JSON file, The Yelp Review dataset was collapsed to the zipcode level through merging, aggregation, and collapsing within the google cloud platform. 

```{r loadin yelp}
# Import data
yelp_review_long <- read.csv("yelplongPC_updated2.csv", header = T, na.strings=c("NA"))

# Check missingness
sapply(yelp_review_long, function(x) sum(is.na(x)))

# Check how many unique zipcodes
length(unique(yelp_review_long$postal_code))

# Convert YearMonth from character to yearmon class
yelp_review_long$YearMonth <- as.yearmon(yelp_review_long$YearMonth)
```


Notice that there are no missing postal code values, with 15,980 unique postal codes. 

## Zillow data

Here, we read in the Zillow data with information on all rental values across the US and Canada. We rename the "RegionName"" variable to "postal_code" then convert the data from wide to long in order to merge it with the Yelp dataset. We change the "time" variable to a date class and cut the Zillow data to match the dates of the Yelp data (while Zillow data goes back to the 1990s, Yelp business and review data only go back to 2010).

```{r import reshape and cut zillow, message=FALSE, warning=FALSE}
# Import data
zillow <- read_csv("zecon/Zip_Zri_AllHomesPlusMultifamily.csv", col_names = TRUE)

# Reformat to prepare for merge with Yelp dataset. 
names (zillow)[2] <- "postal_code"
zillow <- as.data.frame(zillow)
zillow_long <- reshape(zillow, varying = list(names(zillow[8:95])), times = names(zillow[8:95]), idvar = 'postal_code', v.names = 'rentprice' , direction = 'long')

sapply(zillow_long, function(x) sum(is.na(x)))

zillow_long$time<- as.Date(strptime(paste(1, zillow_long$time),"%d %Y-%m"))

zillow_long <- zillow_long[zillow_long$time >= "2010-11-01" & zillow_long$time <= "2017-12-31",]

zillow_long$month <- match(months(zillow_long$time), month.name)
zillow_long$year <- format(zillow_long$time,format="%Y")
zillow_long$YearMonth <- as.yearmon(paste(zillow_long$year, zillow_long$month), "%Y %m")
names(zillow_long)

```


Notice that there are 28,354 missing values for "rentprice". We will attempt to recitfy this through imputation later. 

## Merging all datasets 

Here, we create the official merged dataset from which we conduct the analysis.


```{r final merge, warning=FALSE}
# Merge Check-In with Yelp 
yelp_review_long <- left_join(yelp_review_long, checkinfull, by=c('postal_code'), match='all')

# Merge Zillow with Yelp
Full_data_long <- inner_join(yelp_review_long, zillow_long, by=c('postal_code', 'YearMonth'), match='all')
length(unique(Full_data_long$postal_code))
sapply(Full_data_long, function(x) sum(is.na(x)))

```

The final frame consists of 37492 observations and 39 variables. 
  
## Explortatory Data Analysis

Here is where we conduct the official analysis portion of the project


```{r eda, warning=FALSE}
# First, subset business data based on eventual merging with Zillow
Full <- unique(Full_data_long$postal_code)
business_zillow <- dplyr::filter(yelp_business, postal_code %in% Full)

# Assess the count of states 
Full_data_long$State <- Full_data_long$State %>% as.factor
Full_data_long$State %>% summary

ggplot(Full_data_long, aes(State)) + geom_bar() + labs(title= " Frequency of States in Yelp Challenge Dataset") 
```

The states most represented in the dataset are Arizona, Ohio, Pennsylvania, Nevada, North Carolina, Wisconsin, and Illinois. 


```{r, warning=FALSE}
Full_data_long$City <- Full_data_long$City %>% as.factor
Full_data_long$City %>% summary
```

```{r, warning=FALSE}
temp <- row.names(as.data.frame(summary(Full_data_long$City, max=12))) 
Full_data_long$City <- as.character(Full_data_long$City) 
Full_data_long$top <- ifelse(
  Full_data_long$City %in% temp, 
  Full_data_long$City, 
  "Other" 
)
Full_data_long$top <- as.factor(Full_data_long$top) 

ggplot(Full_data_long[Full_data_long$top!="Other",],aes(x=factor(top, levels=names(sort(table(top),increasing=TRUE))))) + geom_bar() + labs(title="Frequency of Top Cities in Yelp Challenge Dataset") + xlab("City") + ylab("Count") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

This assesses the count of cities in the dataset. Most frequent cities with Yelp information include Phoenix, Las Vegas, Charlotte, Pittsburgh, Cleveland.

```{r, warning=FALSE}
# Reformat data to suit plot
catplot <- business_zillow%>%select(-starts_with("hours"), -starts_with("attribute")) %>% unnest(categories) %>%
  select(name, categories)%>%group_by(categories)%>%summarise(n=n())%>%arrange(desc(n))%>%head(20)
catplot <- as.data.frame(catplot)

ggplot(data=catplot, aes(x=categories, y=n)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(title="Frequency of Categories in Dataset") + xlab("Category") + ylab("Count")

```

This counts the "categories" included in the dataset. These categories are tags that users might use to describe various businesses. We can see that the top category is "Restuarants" followed by "Shopping," "Food," "Home Services," and "Beauty & Spa."

The Yelp dataset also includes information on businesses that may have been open but are currently closed. The previous analyses included all of them, but here we assess the counts of categories only for businesses that are open.

```{r, warning=FALSE}
catplot_open <- business_zillow %>% 
  select(-starts_with("hours"), -starts_with("attribute")) %>% 
  filter(is_open==1) %>% 
  unnest(categories) %>% 
  select(name, categories) %>% 
  group_by(categories) %>% 
  summarise(n=n()) %>% 
  arrange(desc(n)) %>% 
  head(20)

catplot_open  <- as.data.frame(catplot_open )

ggplot(data=catplot_open , aes(x=categories, y=n)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(title="Frequency of Open Categories in Dataset") + xlab("Category") + ylab("Count")

```

Top categories include: "Restaurants," "Shopping," "Home Services," "Food," and "Health & Medical." Interestingly, it seems like "Food" and "Beauty & Spas" might be closing at a higher rate than other categories. 

Next, we compare the rent prices across cities. We use cities as the categorizing variable because there are far too many zip codes in the dataset. 

```{r, warning=FALSE, message=FALSE}

# Reimport data for plotting
zillow <- read_csv("zecon/Zip_Zri_AllHomesPlusMultifamily.csv", col_names = TRUE)

# Eliminate unnecessary columns
zillow <- zillow[-c(94:95)]

# Convert from long to wide by city
zillow_collapse_wide <- zillow %>% 
  group_by(City) %>% 
  summarize_all(funs(mean))
names(zillow_long)

# Collapse to long by State and time period. 
zillow_collapse_long <- zillow_long %>% 
  group_by(State, time) %>% 
  summarize(rentprice = mean(rentprice))
```

Here, we compose a graph illustrating rents over time:


```{r, warning=FALSE, message=FALSE}
ggplot(zillow_collapse_long, aes(x = time, y=rentprice, group = State, colour = State)) + geom_line()  + scale_colour_discrete(guide = 'none')  + scale_x_date(expand=c(0.1, 0)) + geom_dl(aes(label = State), method = list(dl.trans(x = x + .2), "last.points")) + geom_dl(aes(label = State), method = list(dl.trans(x = x - .2), "first.points")) + labs(title = "Rent Prices over Time by State", xlab = "Rent Price (USD)")

```


Few states in the dataset appear to have experienced overall declines in rent over the periods in question. California, Oregon, Colorado, Massachussets, and Washington appear to have experienced significant increases over the time period. DC, California, New Jersey, Hawaii, and Massachussets are consistently plagued by high rent prices.

## Comparing the Average Star Values Across Cities 

```{r, warning=FALSE}
# First, we collapse by state
FDL_collapse_long <- Full_data_long %>% 
  group_by(State, time) %>% 
  summarize(starsav = mean(starsav))

ggplot(FDL_collapse_long, aes(x = time, y=starsav, group = State, colour = State)) + geom_line()  + scale_colour_discrete(guide = 'none')  + scale_x_date(expand=c(0.1, 0)) + geom_dl(aes(label = State), method = list(dl.trans(x = x + .2), "last.points")) +
  geom_dl(aes(label = State), method = list(dl.trans(x = x - .2), "first.points")) + labs(title = "Average Stars over Time by State", xlab = "Avg. Stars (1-5)")
```

As we can see, there don't appear to be any trends or patterns - it seems like a very random relationship. 

## Modeling and Panel Data Regression Analysis

Now comes the fun part. We split the data into train and test sets where our test set comprises the last year (12 months) of our data. We run a Hausman test to determine whether we should run a fixed effects ("within") or a random effects model. Then we develop a Panel Linear Regression model to predict housing prices.

```{r plm, warning=FALSE}
# Create test and train
Full_data_long_train <- Full_data_long[Full_data_long$time < "2017-01-01",]
Full_data_long_test <- Full_data_long[Full_data_long$time >= "2017-01-01",]

# Set parameters
my.formula <- rentprice ~ starsav + is_openave + funnyav + coolav + usefulav 
my.index <- c('postal_code','time')

# Conduct test
my.hausman.test.train <- phtest(x = my.formula, 
                          data = Full_data_long_train,
                          model = c('within', 'random'),
                          index = my.index)
print(my.hausman.test.train)

```


The high p-value of 0.937 indicates that we should use a random effects model instead of a fixed effects model. 

Now, we build our random effects model on the training dataset and predict on the test set. We calculate the Mean Averege Percent Error (MAPE) to see how accurately our model uses training values to predict rental prices in the test set.  


```{r, warning=FALSE}
my.pdm.train <- plm(data = Full_data_long_train, 
              formula = my.formula, 
              model = 'random',
              index = my.index)
summary(my.pdm.train)

Full_data_long_test$pred.plm.test <- predict(my.pdm.train, Full_data_long_test, type='response')

plmmape <- 100*mean(abs(Full_data_long_test$pred.plm.test/Full_data_long_test$rentprice-1), na.rm = T)
print(plmmape)

```


MAPE is only 21.39% right now. We will continue working on the model to get this error lower. 


```{r, warning=FALSE}

ggplot(Full_data_long_test, aes(x=rentprice, y=pred.plm.test)) +geom_point() + labs(title="Predicted vs. Actual Real Estate Prices") + xlab("Actual") + ylab("Predicted")

```


The plot above shows a significant discrepency between actual and predicted rent prices. 

## Generating Lag

We first have to ensure that both sets have the same postal codes in order to conduct the fixed effects analysis dictated by the hausman test. 
```{r lag model, warning=FALSE}
Full_data_long_train <- Full_data_long[Full_data_long$time < "2017-01-01",]
Full_data_long_test <- Full_data_long[Full_data_long$time >= "2017-01-01",]
Full_data_long_test <- Full_data_long_test[Full_data_long_test$postal_code!="05440",]

train <- unique(Full_data_long_train$postal_code)
test<- unique(Full_data_long_test$postal_code)

Full_data_long_test <- dplyr::filter(Full_data_long_test, postal_code %in% train)
Full_data_long_train <- dplyr::filter(Full_data_long_train, postal_code %in% test)

length(unique(Full_data_long_test$postal_code))
length(unique(Full_data_long_train$postal_code))
```

To fine-tune the model, we decided to lag the dependent variable to consider the possibility that last month's rent could be the best predictor of this month's rent. We follow a similar process to the one above for training, testing, and predicting. 

```{r lag, warning=FALSE}
my.lag.formula <- rentprice ~ lag(rentprice, 1) + starsav + is_openave + funnyav + coolav + usefulav + Number_of_reviews 

# Conduct Hausman Test
my.hausman.test.train.lag <- phtest(x = my.lag.formula, 
                                data = Full_data_long_train,
                                model = c('within', 'random'),
                                index = my.index)

print(my.hausman.test.train.lag)

```


Then, we build the model on the training set and predict on the test set in order to calculate the MAPE.


```{r, warning=FALSE}

# Regular LM using zip dummies
my.pdm.train.lag.lm <- lm (rentprice ~ lag(rentprice, 1) + starsav + is_openave + funnyav + coolav + usefulav + Number_of_reviews + postal_code + time, data = Full_data_long_train) 

# Predict 
Full_data_long_test$pred.plm.test.lag <- predict(my.pdm.train.lag.lm, Full_data_long_test)

# MAPE
plmmape.lag <- 100*mean(abs(Full_data_long_test$pred.plm.test.lag/Full_data_long_test$rentprice-1), na.rm = T)
print(plmmape.lag)
```


Now we get a MAPE of 5.181216e-13, far lower than the non-lagged model. This supports the hypothesis that last month's rent could be the best predictor of this month's rent price.


```{r plot, warning=FALSE}

ggplot(Full_data_long_test, aes(x=rentprice, y=pred.plm.test.lag)) +geom_point() + labs(title="Predicted vs. Actual Real Estate Prices") + xlab("Actual") + ylab("Predicted")

```

## Multiple Imputation for Missing Values Using the Amelia Package 

This process uses bootstrapping and an Expectation-Maximization algorithm to impute the missing values in a data set. In our model, we will be able to throw in almost all of our independent variables.

```{r imputation, warning=FALSE}
# Look at missingness to get a sense of what needs to be imputed.
sapply(Full_data_long, function(x) sum(is.na(x)))
Full_data_long <- Full_data_long[-c(40)]
Imputed_Full_data_long <-amelia(Full_data_long,ts= 'time', cs= 'postal_code', p2s=0, intercs = FALSE, idvars=c('City', 'State', 'Metro', 'CountyName', 'year', 'month', 'YearMonth'))

write.amelia(obj=Imputed_Full_data_long, file.stem="imputedfull")

data1 <- read.csv("imputedfull1.csv")
data2 <- read.csv("imputedfull2.csv")
data3 <- read.csv("imputedfull3.csv")
data4 <- read.csv("imputedfull4.csv")
data5 <- read.csv("imputedfull5.csv")

data1 <- pdata.frame(data1, index = c("postal_code", "time"))
data2 <- pdata.frame(data2, index = c("postal_code", "time"))
data3 <- pdata.frame(data3, index = c("postal_code", "time"))
data4 <- pdata.frame(data4, index = c("postal_code", "time"))
data5 <- pdata.frame(data5, index = c("postal_code", "time"))

allimp <- imputationList(list(data1,data2,data3,data4,data5))

```

We create the train and tests set using the last 12 months (1 year) for the test set, but with imputed values from an Amelia imputation iteration. 
```{r, warning=FALSE}

data5$time <- as.Date(data5$time, "%Y-%m-%d")
data5_train <- data5[data5$time < "2017-01-01",]
data5_test <- data5[data5$time >= "2017-01-01",]

# Adjust to ensure postal codes aere same for fixed effects analysis 
# 5440 and 8054 were staying in as factor in test for some reason. Take out here. 
data5_test <- data5_test[data5_test$postal_code!="5440",]
data5_test <- data5_test[data5_test$postal_code!="8054",]

trainimp <- unique(data5_train$postal_code)
testimp <- unique(data5_test$postal_code)

data5_test <- dplyr::filter(data5_test, postal_code %in% trainimp)
data5_train <- dplyr::filter(data5_train, postal_code %in% testimp)

my.formula.impute.lag <- rentprice ~ lag(rentprice, 12) + starsav + starssd + is_openave + funnyav + coolav + usefulav + Number_of_reviews + Number_of_businesses + Friday_ave + Monday_ave + Saturday_ave + Sunday_ave + Thursday_ave + Tuesday_ave + Wednesday_ave + Friday_total + Monday_total + Saturday_total + Sunday_total + Thursday_total + Tuesday_total + Wednesday_total

my.index <- c('postal_code','time')

# Conduct Hausman Test
my.hausman.test.train.impute.lag <- phtest(x = my.formula.impute.lag, 
                                           data = data5_train, 
                                           model = c('within', 'random'),
                                           index = my.index)

print(my.hausman.test.train.impute.lag)

```


Build random effects model on train and predict on test.


```{r, warning=FALSE}
# Regular LM using zip dummies
my.pdm.train.lag.lm.immpute <- lm (rentprice ~ lag(rentprice, 12) + starsav + starssd + is_openave + funnyav + coolav + usefulav + Number_of_reviews + Number_of_businesses + Friday_ave + Monday_ave + Saturday_ave + Sunday_ave + Thursday_ave + Tuesday_ave + Wednesday_ave + Friday_total + Monday_total + Saturday_total + Sunday_total + Thursday_total + Tuesday_total + Wednesday_total + postal_code + time, data = data5_train) 

data5_test$my.pdm.train.lag.lm.immpute <- predict(my.pdm.train.lag.lm.immpute, data5_test)

plmmape_impute_lag <- 100*mean(abs(data5_test$pred.plm.test.impute.lag/data5_test$rentprice-1), na.rm = T)
print(plmmape_impute_lag)

```


Imputation gives us a MAPE of 1.535932e-12. However, it's important to note that the MAPE could vary slightly depending on which imputed dataset we test on - for example, if we trained on Imputed datasets 2-5 and tested on 1. 


```{r, warning=FALSE}
ggplot(data5_test, aes(x=rentprice, y=my.pdm.train.lag.lm.immpute)) + geom_point() + labs(title="Predicted vs. Actual Real Estate Prices") + xlab("Actual") + ylab("Predicted")

```

Now, we conduct a reduced imputed model, which excludes checkin data

```{r, warning=FALSE}
my.formula.impute.lag.Simple <- rentprice ~ lag(rentprice, 12) + starsav + starssd + is_openave + funnyav + coolav + usefulav + Number_of_reviews + Number_of_businesses

my.hausman.test.train.impute.lag.Simple <- phtest(x = my.formula.impute.lag.Simple, 
                                                  data = data5_train, 
                                                  model = c('within', 'random'),
                                                  index = my.index)

print(my.hausman.test.train.impute.lag.Simple)

```


```{r, warning=FALSE}
# Build random effects model on train and predict on test
my.pdm.train.impute.lag.Simple  <- lm (rentprice ~ lag(rentprice, 12) + starsav + starssd + is_openave + funnyav + coolav + usefulav + Number_of_reviews + Number_of_businesses + postal_code + time, data = data5_train) 

# Predict 
data5_test$pred.plm.test.impute.lag.Simple <- predict(my.pdm.train.impute.lag.Simple, data5_test)

plmmape_impute_lag.Simple <- 100*mean(abs(data5_test$pred.plm.test.impute.lag.Simple/data5_test$rentprice-1), na.rm = T)
print(plmmape_impute_lag.Simple)
```

Here, our imputation process gives us a MAPE of 4.75.


```{r, warning=FALSE}
ggplot(data5_test, aes(x=rentprice, y=pred.plm.test.impute.lag.Simple)) +geom_point() + labs(title="Predicted vs. Actual Real Estate Prices") + xlab("Actual") + ylab("Predicted")
```

## Final Model

The last thing we do is subset the Business dataset to include only businesses categorized as "food" or "bars." We do this because we expect these businesses will have a stronger relationship to rent prices than others, such as "Beauty & Spa" businesses.

First we set up the new subset data 
```{r food, results="hide", warning=FALSE, message=FALSE}
yelp_review_long_food <- read.csv("yelplongPC_food.csv", header = T, na.strings=c("NA"))
sapply(yelp_review_long_food, function(x) sum(is.na(x)))
# No missing postal codes
length(unique(yelp_review_long_food$postal_code))
# 3274 unique postal codes 
yelp_review_long_food$YearMonth <- as.yearmon(yelp_review_long_food$YearMonth)

############### Merge Checkin with Yelp 
# Nonexpanded
yelp_review_long_food <- left_join(yelp_review_long_food, checkinfull, by=c('postal_code'), match='all')

############### Merge Zillow with Yelp

# Non-Expanded
Full_data_long_food <- inner_join(yelp_review_long_food, zillow_long, by=c('postal_code', 'YearMonth'), match='all')
length(unique(Full_data_long_food$postal_code))
sapply(Full_data_long_food, function(x) sum(is.na(x)))
```

```{r plm 2, warning=FALSE}
# Create train and test set using the last 12 months (1 year) for the test set 
Full_data_long_food_train <- Full_data_long_food[Full_data_long_food$time < "2017-01-01",]
Full_data_long_food_test <- Full_data_long_food[Full_data_long_food$time >= "2017-01-01",]

# Build the mixed effects model 
# Hausman Test
# set options for Hausman test
names(Full_data_long_food)
my.formula <- rentprice ~ starsav + is_openave + funnyav + coolav + usefulav 

my.index <- c('postal_code','time')
# Conduct Hausman Test
my.hausman.test.train.food <- phtest(x = my.formula, 
                                data = Full_data_long_food_train,
                                model = c('within', 'random'),
                                index = my.index)

# print result
print(my.hausman.test.train.food)

```


The high p-value  indicates that we should use a random effects model instead of a fixed effects model. 

Now, we build our random effects model on the training dataset and predict on the test set. We calculate the Mean Averege Percent Error (MAPE) to see how accurately our model uses training values to predict rental prices in the test set.  

```{r, warning=FALSE}
# Built random effects model on train
my.pdm.train.food <- plm(data = Full_data_long_food_train, 
                    formula = my.formula, 
                    model = 'random',
                    index = my.index)
summary(my.pdm.train.food)

# Predict 
Full_data_long_food_test$pred.plm.test <- predict(my.pdm.train.food, Full_data_long_food_test, type='response')

# MAPE
plmmape.food <- 100*mean(abs(Full_data_long_food_test$pred.plm.test/Full_data_long_food_test$rentprice-1), na.rm = T)
print(plmmape.food)

```


MAPE is only 18.69% right now. We will continue working on the model to get this error lower. 

```{r, warning=FALSE}
ggplot(Full_data_long_food_test, aes(x=rentprice, y=pred.plm.test)) +geom_point() + labs(title="Predicted vs. Actual Real Estate Prices") + xlab("Actual") + ylab("Predicted")
```

## Generating the Lag Model

We first have to ensure that both sets have the same postal codes in order to conduct the fixed effects analysis dictated by the hausman test. 
```{r lagged, warning=FALSE}
# Other zips were staying in as factors in test for some reason. Take out here. 
Full_data_long_food_test <- Full_data_long_food_test[Full_data_long_food_test$postal_code!="28803",]
Full_data_long_food_test <- Full_data_long_food_test[Full_data_long_food_test$postal_code!="85266",]

trainimpfood <- unique(Full_data_long_food_train$postal_code)
testimpfood <- unique(data5_test$postal_code)

Full_data_long_food_test <- dplyr::filter(Full_data_long_food_test, postal_code %in% trainimpfood)
Full_data_long_food_train <- dplyr::filter(Full_data_long_food_train, postal_code %in% testimpfood)
```

To fine-tune the model, we decided to lag the dependent variable to consider the possibility that last month's rent could be the best predictor of this month's rent. We follow a similar process to the one above for training, testing, and predicting. 

```{r lag2, warning=FALSE}
my.lag.formula <- rentprice ~ lag(rentprice, 1) + starsav + is_openave + funnyav + coolav + usefulav + Number_of_reviews 

# Conduct Hausman Test
my.hausman.test.food.train.lag <- phtest(x = my.lag.formula, 
                                    data = Full_data_long_food_train,
                                    model = c('within', 'random'),
                                    index = my.index)

# print result
print(my.hausman.test.food.train.lag)

```


Then, we build the model on the training set and predict on the test set in order to calculate the MAPE.


```{r, warning=FALSE}

# Built fixed effects model on train
my.pdm.train.impute.lag.Simple  <- lm (rentprice ~ lag(rentprice, 1) + starsav + is_openave + funnyav + coolav + usefulav + Number_of_reviews + postal_code + time, data = Full_data_long_food_train) 

# Predict 
Full_data_long_food_test$pred.plm.test.lag <- predict(my.pdm.train.impute.lag.Simple, Full_data_long_food_test)

# MAPE
plmmape.lag.food <- 100*mean(abs(Full_data_long_food_test$pred.plm.test.lag/Full_data_long_food_test$rentprice-1), na.rm = T)
print(plmmape.lag.food)

```

MAPE is now 3.37%, which is the lowest MAPE that we've arrived to. 

```{r, warning=FALSE}
ggplot(Full_data_long_food_test, aes(x=rentprice, y=pred.plm.test.lag)) +geom_point() + labs(title="Predicted vs. Actual Real Estate Prices") + xlab("Actual") + ylab("Predicted")
```


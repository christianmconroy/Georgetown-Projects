# Predicting Undervaluation and Overvaluation from Financial Statement Data

With 2019 will come the IPO of ride-sharing company Uber, which many are now calling a “decacorn” due to its $10 billion valuation. The much anticipated IPO will represent the third largest IPO in the history of NYSE behind Facebook and Alibaba, and other hot tech companies like Airbnb and Pinterest will not be far behind Uber in their own respective paths to IPOs. While these companies have acquired massive userbases through competitive strategies in monopolistic markets, what is most interesting about these companies is that of the dozen unicorns (valuation of at least $1 billion) that went public last year, they posted a combined $14 billion in losses. Ten years ago, only about 33% of companies pursuing IPOs had no recorded profits. Today that number is a staggering 84% of companies pursuing IPOs. 

Naturally, many who suffered through the dot-com bubble see the current trend in revenue-neutral or revenue-negative IPOs as eerily familiar. However, the major difference is that while there was only $6 billion in VC funds circling between startups in the US in 1994, that number has now reached over $130 billon. Despite the negative profit and loss statements, the allure of massive user bases and the greater scale of VC funding means that companies are able to carry on in the red for far longer than they used to be before funders start to demand a return. As we saw with the disastrous IPO of ephemeral messaging platform Snapchat in 2017, however, the great wall protecting revenue-negative firms like Uber is starting to crack, causing many to focus more on whether these companies might be massively overvalued. 

The objective of my proposal therefore is to use financial statement data on hundreds of public companies to predict whether firms or over or undervalued and identify which features on balance sheets, income statements, and cash flow statements have the greatest predictive power in determining overvaluation and undervaluation. Armed with this knowledge, investors can better protect themselves from looming bubbles arising from a critical mass of overvaluations. At the same time, by accessing financial metrics beyond the questionable daily active users (DAUs) indicators, investors can better identify value investment opportunities that can guarantee significant returns in the long-run. This of course all rests on the well accepted argument that the stock market does not in fact behave in accordance with the efficient market hypothesis and it is therefore possible to produce predictions the market has not taken into account.  

To accomplish the above objective, I will use the eXtensible Business Reporting Language (XBRL) dataset that scrapes line-item information from the financial statements publically traded companies are required to publish annually under SEC regulations. The SEC established the XBRL data collection program to address the issues that cause stock mispricing and it is therefore perfect for accomplishing my objective. The full dataset includes information on 122,815 financial statement items collected across 5,711 companies dating back to 1985 and up through 2018. There are a number of potential target variables we can use, but we will mainly focus on the common stock price and the earnings per share price initially.  

Within the realm of supervised learning, there are a number of different data science methodologies that can be applied to accomplishing my core objective, and I will initially organize those approaches based on the two ways of organizing the data for specific insights. 
In the first approach, I will organize the data into a panel consisting of a cross-section of publically traded companies and a time series of years dating back 20 years capturing 1998 to 2018. Because there is missingness across companies and years, I will attempt two approaches to address missingness. In the first approach, I can conduct multiple imputation for missing values using the Amelia Package in R. This process uses bootstrapping and an Expectation-Maximization algorithm to impute the missing values in a data set. In the second approach, I will evaluate which financial statement variables are best represented in the data by calculating a proportion through dividing the frequency of an indicator and the data and dividing it by how much the indicator would appear if it was included for every company and every year. After eliminating the indicators that fall below 50% coverage, I will then subset the dataset to eliminate the companies that do not have 50% of the subsetted indicators across the time periods in the data. 

Once I’ve addressed missingness, I will split the data into train and test sets where the test set comprises the last year of the data. At this point, I will employ Lasso regularization to evaluate which financial statement indicators to include in the middle to predict the common stock price or earnings per share. After dropping the variables that are kicked out in the lasso process, I will then develop a Panel Linear Regression model to predict the stock prices. To fine-tune the model, I will lag the dependent variable to consider the possibility that the previous year’s stock price could be the best predictor of this year’s stock price. To asses how well the model performs over all, I will then calculate the Mean Averege Percent Error (MAPE) to see accurately the model uses training values to predict rental prices in the test set.

In the second approach, I will restrict the data to one year of data. After applying the same lasso process laid out above for feature selection, we will employ a variety of methodologies to identify the best model to train. We will use decision trees to subset the population into smaller more homogenous subpopulations using the input features. We will grow a forest of decision trees where observations are bootstrapped and variables are sampled using the random forest approach. We will evaluate a number of other approaches as well, including gradient boosting and bagging approaches. 

The plot below is the result of the initial exploratory data analysis process.

In the plot below, we are surprised to see a rising profit-loss depist the increasing number of companies IPOing in the negative revenue. This can be explained by the fact that the dataset is capturing publically traded companies regarldess of when they went public. Future analysis will seek to identifying profit-loss for each company at the time of IPO. 

![ANLPNL](https://user-images.githubusercontent.com/29176570/57197445-5c0fcd00-6f35-11e9-86c4-897165df2710.png)

In the plot below, we would expect to see a number of points in the top-left quadrant to reflect high valuations despite negative profit-loss. However, we do not see this but instead see exactly what we would expect traditionally, namely that negative profit-loss amounts in negative EPS and positive profit-loss results in positive EPS. This could potentially be explained by our choice of dependent variable and it is possible that common stock price or public float share price would have been more reflective of current trends. 

![EPSPNL](https://user-images.githubusercontent.com/29176570/57197469-9f6a3b80-6f35-11e9-866a-8229e0d383f5.png)


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying BERT Multilingual Classifier to Predict Account Suspension \n",
    "\n",
    "Guidance from: https://github.com/kacossio/TeamPython/blob/master/Bert%20Multilingual%20Embedding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Load Packages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "import emoji\n",
    "import random \n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters \n",
    "########## Set Parameters\n",
    "\n",
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "filepath = \"data/\"\n",
    "\n",
    "import_bucket = \"joe-exotic-2020\"\n",
    "\n",
    "embedding_bucket = \"modeling/embeddings\"\n",
    "\n",
    "key = 'full_clean' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "results_bucket = 'full_clean' # already created on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load in Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe.\n",
    "    Inputs: Filelist \n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "\n",
    "    # Read in single file\n",
    "    object_key = filelist[0].split('/', 1)[1]\n",
    "    csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_string), error_bad_lines=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load in data from S3\n",
    "\n",
    "# Import Train and Measure Balance\n",
    "# Import Flattened Data\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"train_updated\",obj.key)]\n",
    "\n",
    "df_train = import_data(filelist)\n",
    "\n",
    "df_train['suspended'] = pd.to_numeric(df_train['suspended'], errors='coerce')\n",
    "df_train = df_train[df_train['suspended'].notna()]\n",
    "\n",
    "# Import Test and Measure Balance\n",
    "\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"test_updated\",obj.key)]\n",
    "\n",
    "df_test = import_data(filelist)\n",
    "\n",
    "df_test['suspended'] = pd.to_numeric(df_test['suspended'], errors='coerce')\n",
    "df_test = df_test[df_test['suspended'].notna()]\n",
    "\n",
    "# Import Validation and Measure Balance\n",
    "# Import Flattened Data\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"valid_updated\",obj.key)]\n",
    "\n",
    "df_valid = import_data(filelist)\n",
    "\n",
    "df_valid['suspended'] = pd.to_numeric(df_valid['suspended'], errors='coerce')\n",
    "df_valid = df_valid[df_valid['suspended'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that Target Variable is Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['suspended'] = df_train['suspended'].astype(int)\n",
    "df_valid['suspended'] = df_valid['suspended'].astype(int)\n",
    "df_test['suspended'] = df_test['suspended'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['id', 'created_at', 'text'])\n",
    "df_valid = df_valid.drop_duplicates(subset=['id', 'created_at', 'text'])\n",
    "df_test = df_test.drop_duplicates(subset=['id', 'created_at', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure binary possibly_sensitive vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['possibly_sensitive'][df_train['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan\n",
    "df_valid['possibly_sensitive'][df_valid['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan\n",
    "df_test['possibly_sensitive'][df_test['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages \n",
    "from translate import Translator\n",
    "import spacy\n",
    "import langid\n",
    "import keras_bert\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime as dt\n",
    "import pytz\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Dates to Unix Epoch Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to convert dates into float (Unix Epoch Times )\n",
    "def convert_dates_float(df):\n",
    "    '''\n",
    "    Convert key input data variables to numeric format for tensors. Uses unix epoch time in seconds. \n",
    "    '''\n",
    "    # created_at (tweet)\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    my_datetime = dt.datetime(1970,1,1) \n",
    "    good_dt = pytz.timezone('UTC').localize(my_datetime)\n",
    "    df['created_at'] = (df['created_at'] - good_dt).dt.total_seconds()\n",
    "\n",
    "    # User.created_at (account)\n",
    "    df['user.created_at'] = pd.to_datetime(df['user.created_at'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    my_datetime = dt.datetime(1970,1,1) \n",
    "    good_dt = pytz.timezone('UTC').localize(my_datetime)\n",
    "    df['user.created_at'] = (df['user.created_at'] - good_dt).dt.total_seconds()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert binary and categorical variables to one-hot encoded (not sure this is best or not)\n",
    "\n",
    "Options \n",
    "\n",
    "- Integer Encoding: Where each unique label is mapped to an integer.\n",
    "- One Hot Encoding: Where each label is mapped to a binary vector.\n",
    "- Learned Embedding: Where a distributed representation of the categories is learned.\n",
    "\n",
    "We use one hot encoding below. \n",
    "\n",
    "#### We use get_dummies below instead of one_hot_encoder as get dummies knows how to deal with missingness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One Hote Encoding (Unix Epoch Times )\n",
    "def one_hot(df_train, df_valid, df_test): \n",
    "    '''\n",
    "    One hot encoding requires the full dataset in order to ensure that there end up the same amount of columns for test, validation and train.\n",
    "    We therefore combine train, valid, and test, fill nas with 0 where necessary, and one hot encode categorical vars. \n",
    "    '''\n",
    "    df_train['split'] = \"train\"\n",
    "    df_valid['split'] = \"valid\"\n",
    "    df_test['split'] = \"test\"\n",
    "    df = pd.concat([df_train, df_test, df_valid], ignore_index=True, sort=False)\n",
    "    df = convert_dates_float(df)\n",
    "    # Extra layer of Processing \n",
    "    df = df[df['retweet_count'] != \"False\"] \n",
    "    df['quoted_status.user.followers_count'] = df['quoted_status.user.followers_count'].fillna(0) \n",
    "    df['quoted_status.user.friends_count'] = df['quoted_status.user.friends_count'].fillna(0) \n",
    "    df['retweeted_status.user.followers_count'] = df['retweeted_status.user.followers_count'].fillna(0) \n",
    "    df['retweeted_status.user.friends_count'] = df['retweeted_status.user.friends_count'].fillna(0) \n",
    "    # One-hot\n",
    "    df = df.drop([\"user.protected.1\", \"user.protected.2\", \"user.protected.3\"], axis=1)\n",
    "    df = pd.get_dummies(df, columns=[\"source\", \"lang\", \"possibly_sensitive\", \"withheld_in_countries\", \"place.country\", \n",
    "                                         \"user.geo_enabled\", \"user.lang\", \"user.verified\", \"user.has_extended_profile\",\n",
    "                                        \"user.lang\", \"user.protected\", \"user.time_zone\", \"user.verified\", \"user.default_profile\",\n",
    "                                        \"is_quote_status\"])\n",
    "    return df\n",
    "# Tp get rid of: Text, user.protected.1, user.protected.2, user.protected.3, \n",
    "# To concat (or get rid of): user.description, user.location, user.name, user.screen_name\n",
    "# to potentially take out entirely - user.id (This would explain everything)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split one-hot encoded df back apart into train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot(df_train, df_valid, df_test)\n",
    "df_train_f = df[df['split'] == \"train\"]\n",
    "df_valid_f = df[df['split'] == \"valid\"]\n",
    "df_test_f = df[df['split'] == \"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove other text fields (may concatanate with tweets in future iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_f.drop(['user.description', \"user.location\", \"user.name\", \"user.screen_name\", \"split\"], axis=1)\n",
    "df_valid = df_valid_f.drop(['user.description', \"user.location\", \"user.name\", \"user.screen_name\", \"split\"], axis=1)\n",
    "df_test = df_test_f.drop(['user.description', \"user.location\", \"user.name\", \"user.screen_name\", \"split\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>...</th>\n",
       "      <th>user.geo_enabled_False</th>\n",
       "      <th>user.geo_enabled_True</th>\n",
       "      <th>user.verified_False</th>\n",
       "      <th>user.has_extended_profile_False</th>\n",
       "      <th>user.has_extended_profile_True</th>\n",
       "      <th>user.protected_False</th>\n",
       "      <th>user.verified_False</th>\n",
       "      <th>user.default_profile_True</th>\n",
       "      <th>is_quote_status_False</th>\n",
       "      <th>is_quote_status_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304799e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>containcontrast you are as worse as nazi germ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>RT bcelyj nevernever maryann CCP CCCP CCCP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>RT maryann CCP CCP https co EAFQGqFQ</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304795e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>https co oBzs zO https co Si btzc U</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.304794e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>https co oBzs zO Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  \\\n",
       "1  1.304799e+18  1.599923e+09   \n",
       "2  1.304796e+18  1.599923e+09   \n",
       "3  1.304796e+18  1.599923e+09   \n",
       "4  1.304795e+18  1.599922e+09   \n",
       "5  1.304794e+18  1.599922e+09   \n",
       "\n",
       "                                                text retweet_count  \\\n",
       "1   containcontrast you are as worse as nazi germ...           0.0   \n",
       "2        RT bcelyj nevernever maryann CCP CCCP CCCP            1.0   \n",
       "3               RT maryann CCP CCP https co EAFQGqFQ         802.0   \n",
       "4                https co oBzs zO https co Si btzc U           0.0   \n",
       "5                                 https co oBzs zO Z           0.0   \n",
       "\n",
       "  favorite_count  quoted_status_id       user.id  user.created_at  \\\n",
       "1            1.0               0.0  1.278120e+18     1.593563e+09   \n",
       "2            0.0               0.0  1.278120e+18     1.593563e+09   \n",
       "3            0.0               0.0  1.278120e+18     1.593563e+09   \n",
       "4           15.0               1.0  1.278120e+18     1.593563e+09   \n",
       "5           10.0               1.0  1.278120e+18     1.593563e+09   \n",
       "\n",
       "   user.favourites_count  user.followers_count  ...  user.geo_enabled_False  \\\n",
       "1                25355.0                 377.0  ...                       1   \n",
       "2                25355.0                 377.0  ...                       1   \n",
       "3                25355.0                 377.0  ...                       1   \n",
       "4                25355.0                 377.0  ...                       1   \n",
       "5                25355.0                 377.0  ...                       1   \n",
       "\n",
       "   user.geo_enabled_True  user.verified_False  \\\n",
       "1                      0                    1   \n",
       "2                      0                    1   \n",
       "3                      0                    1   \n",
       "4                      0                    1   \n",
       "5                      0                    1   \n",
       "\n",
       "   user.has_extended_profile_False  user.has_extended_profile_True  \\\n",
       "1                                0                               1   \n",
       "2                                0                               1   \n",
       "3                                0                               1   \n",
       "4                                0                               1   \n",
       "5                                0                               1   \n",
       "\n",
       "   user.protected_False  user.verified_False  user.default_profile_True  \\\n",
       "1                     1                    1                          1   \n",
       "2                     1                    1                          1   \n",
       "3                     1                    1                          1   \n",
       "4                     1                    1                          1   \n",
       "5                     1                    1                          1   \n",
       "\n",
       "   is_quote_status_False  is_quote_status_True  \n",
       "1                      1                     0  \n",
       "2                      1                     0  \n",
       "3                      1                     0  \n",
       "4                      0                     1  \n",
       "5                      0                     1  \n",
       "\n",
       "[5 rows x 162 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import cuda \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x, pl):\n",
    "    return int(math.ceil(x / pl)) * pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Import Data\n",
    "def import_list_data(filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe. Concatenates multiple files if necessary. \n",
    "    Inputs: Directory path, number of rows to skip\n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "    dataframes = []\n",
    "    # Iterate through files of the directory\n",
    "    for filename in filelist:\n",
    "        object_key = filename.split('/', 1)[1]\n",
    "        csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "        body = csv_obj['Body']\n",
    "        csv_string = body.read().decode('utf-8')\n",
    "        dataframe = pd.read_csv(StringIO(csv_string))\n",
    "        dataframes.append(dataframe)\n",
    "    df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Base Models: Fully Connected Models (No Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "import random as rn\n",
    "from keras.models import Model,save_model, load_model, Sequential\n",
    "from keras.layers import Flatten,AveragePooling1D,Dropout,Dense,Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import IPython\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "import pydot\n",
    "import graphviz\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kerastuner as kt\n",
    "from kerastuner import HyperModel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Full Dataframes with Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilingual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"ml_train_embeddings_df_full\",obj.key)]\n",
    "\n",
    "bert_embeddings_df_train = import_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_train['suspended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"ml_valid_embeddings_df_full\",obj.key)]\n",
    "\n",
    "bert_embeddings_df_valid = import_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39537"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_valid['suspended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"ml_test_embeddings_df_full\",obj.key)]\n",
    "\n",
    "bert_embeddings_df_test = import_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29139"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_test['suspended'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"labse_train_embeddings_df\",obj.key)]\n",
    "\n",
    "bert_embeddings_df_train_labse = import_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_train_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_train_labse['suspended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"labse_valid_embeddings_df\",obj.key)]\n",
    "\n",
    "bert_embeddings_df_valid_labse = import_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38537"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_valid_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_valid_labse['suspended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"labse_test_embeddings_df\",obj.key)]\n",
    "\n",
    "bert_embeddings_df_test_labse = import_data(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29139"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_test_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_test_labse['suspended'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data to Convert to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>user.listed_count</th>\n",
       "      <th>user.statuses_count</th>\n",
       "      <th>quoted_status.user.followers_count</th>\n",
       "      <th>quoted_status.user.friends_count</th>\n",
       "      <th>retweeted_status.user.followers_count</th>\n",
       "      <th>retweeted_status.user.friends_count</th>\n",
       "      <th>user_age</th>\n",
       "      <th>tweets_per_day</th>\n",
       "      <th>since_last_tweet_mins</th>\n",
       "      <th>since_last_tweet_mins_min</th>\n",
       "      <th>since_last_tweet_mins_max</th>\n",
       "      <th>since_last_tweet_mins_mean</th>\n",
       "      <th>avg_tweets_per_hr</th>\n",
       "      <th>avg_tweets_per_day</th>\n",
       "      <th>no_hashtags</th>\n",
       "      <th>no_mentions</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>tw_len</th>\n",
       "      <th>followers_per_followees</th>\n",
       "      <th>containsURL</th>\n",
       "      <th>user.urls_per_tweet</th>\n",
       "      <th>no_hashtags_per_tweet</th>\n",
       "      <th>no_mentions_per_tweet</th>\n",
       "      <th>no_urls_per_tweet</th>\n",
       "      <th>user.followers_countdailychange</th>\n",
       "      <th>user.friends_countdailychange</th>\n",
       "      <th>user.friend_rate</th>\n",
       "      <th>user.followers_rate</th>\n",
       "      <th>user.has_url</th>\n",
       "      <th>user.has_location</th>\n",
       "      <th>user.screen_name.digit_length</th>\n",
       "      <th>user.screen_name.length</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>suspended</th>\n",
       "      <th>source_        Round   Year Fun!</th>\n",
       "      <th>source_      Round      Year   Fum</th>\n",
       "      <th>source_     Round    Year Fum</th>\n",
       "      <th>source_  Round     Year Fum</th>\n",
       "      <th>source_ Round      Year   Fum</th>\n",
       "      <th>source_ 「モンストからツイート」</th>\n",
       "      <th>source_Affinitweet.com</th>\n",
       "      <th>source_BIGO LIVE</th>\n",
       "      <th>source_Blog2Social APP</th>\n",
       "      <th>source_CallApp</th>\n",
       "      <th>source_Etsy</th>\n",
       "      <th>source_Instagram</th>\n",
       "      <th>source_Joinfsocial</th>\n",
       "      <th>source_Mobile Web (M2)</th>\n",
       "      <th>source_Nintendo Switch Share</th>\n",
       "      <th>source_Paper.li</th>\n",
       "      <th>source_Peing</th>\n",
       "      <th>source_SocialPilot.co</th>\n",
       "      <th>source_TeamSight Publisher</th>\n",
       "      <th>source_TweetDeck</th>\n",
       "      <th>source_Twibbon</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter Web Client</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for Mac</th>\n",
       "      <th>source_Twitter for iPad</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "      <th>source_WShare</th>\n",
       "      <th>source_WordPress.com</th>\n",
       "      <th>source_漫威超級戰爭（MARVEL Super War）</th>\n",
       "      <th>lang_False</th>\n",
       "      <th>lang_am</th>\n",
       "      <th>lang_ar</th>\n",
       "      <th>lang_bg</th>\n",
       "      <th>lang_bn</th>\n",
       "      <th>lang_bo</th>\n",
       "      <th>lang_ca</th>\n",
       "      <th>lang_ckb</th>\n",
       "      <th>lang_cs</th>\n",
       "      <th>lang_cy</th>\n",
       "      <th>lang_da</th>\n",
       "      <th>lang_de</th>\n",
       "      <th>lang_el</th>\n",
       "      <th>lang_en</th>\n",
       "      <th>lang_es</th>\n",
       "      <th>lang_et</th>\n",
       "      <th>lang_eu</th>\n",
       "      <th>lang_fa</th>\n",
       "      <th>lang_fi</th>\n",
       "      <th>lang_fr</th>\n",
       "      <th>lang_gu</th>\n",
       "      <th>lang_hi</th>\n",
       "      <th>lang_ht</th>\n",
       "      <th>lang_hu</th>\n",
       "      <th>lang_hy</th>\n",
       "      <th>lang_in</th>\n",
       "      <th>lang_is</th>\n",
       "      <th>lang_it</th>\n",
       "      <th>lang_iw</th>\n",
       "      <th>lang_ja</th>\n",
       "      <th>lang_km</th>\n",
       "      <th>lang_ko</th>\n",
       "      <th>lang_lt</th>\n",
       "      <th>lang_lv</th>\n",
       "      <th>lang_ml</th>\n",
       "      <th>lang_mr</th>\n",
       "      <th>lang_ne</th>\n",
       "      <th>lang_nl</th>\n",
       "      <th>lang_no</th>\n",
       "      <th>lang_pl</th>\n",
       "      <th>lang_ps</th>\n",
       "      <th>lang_pt</th>\n",
       "      <th>lang_ro</th>\n",
       "      <th>lang_ru</th>\n",
       "      <th>lang_sd</th>\n",
       "      <th>lang_si</th>\n",
       "      <th>lang_sl</th>\n",
       "      <th>lang_sr</th>\n",
       "      <th>lang_sv</th>\n",
       "      <th>lang_ta</th>\n",
       "      <th>lang_th</th>\n",
       "      <th>lang_tl</th>\n",
       "      <th>lang_tr</th>\n",
       "      <th>lang_uk</th>\n",
       "      <th>lang_und</th>\n",
       "      <th>lang_ur</th>\n",
       "      <th>lang_vi</th>\n",
       "      <th>lang_zh</th>\n",
       "      <th>possibly_sensitive_False</th>\n",
       "      <th>possibly_sensitive_True</th>\n",
       "      <th>withheld_in_countries_['IN']</th>\n",
       "      <th>withheld_in_countries_['TR']</th>\n",
       "      <th>place.country_Finland</th>\n",
       "      <th>place.country_Hong Kong</th>\n",
       "      <th>place.country_India</th>\n",
       "      <th>place.country_Indonesia</th>\n",
       "      <th>place.country_Italy</th>\n",
       "      <th>place.country_Mongolia</th>\n",
       "      <th>place.country_Pakistan</th>\n",
       "      <th>place.country_People's Republic of China</th>\n",
       "      <th>place.country_Portugal</th>\n",
       "      <th>place.country_Republic of Korea</th>\n",
       "      <th>place.country_Russia</th>\n",
       "      <th>place.country_Singapore</th>\n",
       "      <th>place.country_Sri Lanka</th>\n",
       "      <th>place.country_United Arab Emirates</th>\n",
       "      <th>place.country_United States</th>\n",
       "      <th>user.geo_enabled_False</th>\n",
       "      <th>user.geo_enabled_True</th>\n",
       "      <th>user.verified_False</th>\n",
       "      <th>user.has_extended_profile_False</th>\n",
       "      <th>user.has_extended_profile_True</th>\n",
       "      <th>user.protected_False</th>\n",
       "      <th>user.verified_False.1</th>\n",
       "      <th>user.default_profile_True</th>\n",
       "      <th>is_quote_status_False</th>\n",
       "      <th>is_quote_status_True</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>612</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>625</th>\n",
       "      <th>626</th>\n",
       "      <th>627</th>\n",
       "      <th>628</th>\n",
       "      <th>629</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "      <th>659</th>\n",
       "      <th>660</th>\n",
       "      <th>661</th>\n",
       "      <th>662</th>\n",
       "      <th>663</th>\n",
       "      <th>664</th>\n",
       "      <th>665</th>\n",
       "      <th>666</th>\n",
       "      <th>667</th>\n",
       "      <th>668</th>\n",
       "      <th>669</th>\n",
       "      <th>670</th>\n",
       "      <th>671</th>\n",
       "      <th>672</th>\n",
       "      <th>673</th>\n",
       "      <th>674</th>\n",
       "      <th>675</th>\n",
       "      <th>676</th>\n",
       "      <th>677</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>700</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "      <th>711</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>721</th>\n",
       "      <th>722</th>\n",
       "      <th>723</th>\n",
       "      <th>724</th>\n",
       "      <th>725</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.304799e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.114942</td>\n",
       "      <td>-0.134353</td>\n",
       "      <td>0.173331</td>\n",
       "      <td>0.136512</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>0.017566</td>\n",
       "      <td>0.039284</td>\n",
       "      <td>-0.023467</td>\n",
       "      <td>0.081272</td>\n",
       "      <td>0.364327</td>\n",
       "      <td>0.242406</td>\n",
       "      <td>-0.285628</td>\n",
       "      <td>0.187173</td>\n",
       "      <td>-0.057560</td>\n",
       "      <td>-0.566414</td>\n",
       "      <td>-0.101790</td>\n",
       "      <td>-0.213441</td>\n",
       "      <td>0.038836</td>\n",
       "      <td>-0.056996</td>\n",
       "      <td>0.019255</td>\n",
       "      <td>0.119526</td>\n",
       "      <td>0.101794</td>\n",
       "      <td>0.047508</td>\n",
       "      <td>0.254573</td>\n",
       "      <td>0.219492</td>\n",
       "      <td>-0.140189</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>-0.057930</td>\n",
       "      <td>0.317335</td>\n",
       "      <td>0.130507</td>\n",
       "      <td>0.283876</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>0.070263</td>\n",
       "      <td>0.260213</td>\n",
       "      <td>0.203483</td>\n",
       "      <td>0.198592</td>\n",
       "      <td>-1.972591</td>\n",
       "      <td>-0.319086</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>-0.155292</td>\n",
       "      <td>-0.136676</td>\n",
       "      <td>0.215126</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.230340</td>\n",
       "      <td>1.312637</td>\n",
       "      <td>0.217007</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>1.049731</td>\n",
       "      <td>-0.427167</td>\n",
       "      <td>0.133239</td>\n",
       "      <td>-0.618760</td>\n",
       "      <td>0.321044</td>\n",
       "      <td>-1.421141</td>\n",
       "      <td>0.516384</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>0.289944</td>\n",
       "      <td>-0.139161</td>\n",
       "      <td>-0.093265</td>\n",
       "      <td>0.153029</td>\n",
       "      <td>0.050548</td>\n",
       "      <td>0.228823</td>\n",
       "      <td>0.085134</td>\n",
       "      <td>0.013365</td>\n",
       "      <td>-0.350873</td>\n",
       "      <td>-0.073694</td>\n",
       "      <td>0.102264</td>\n",
       "      <td>-0.034420</td>\n",
       "      <td>0.115044</td>\n",
       "      <td>0.182258</td>\n",
       "      <td>0.293416</td>\n",
       "      <td>-0.052503</td>\n",
       "      <td>0.231046</td>\n",
       "      <td>-0.170732</td>\n",
       "      <td>-0.038697</td>\n",
       "      <td>-0.282374</td>\n",
       "      <td>0.060193</td>\n",
       "      <td>-0.242756</td>\n",
       "      <td>-0.020662</td>\n",
       "      <td>-0.004166</td>\n",
       "      <td>0.102102</td>\n",
       "      <td>0.383095</td>\n",
       "      <td>-0.341194</td>\n",
       "      <td>-0.073726</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>-0.022729</td>\n",
       "      <td>0.039218</td>\n",
       "      <td>-0.025650</td>\n",
       "      <td>-0.293690</td>\n",
       "      <td>-0.182859</td>\n",
       "      <td>-0.288709</td>\n",
       "      <td>-0.271488</td>\n",
       "      <td>-0.614931</td>\n",
       "      <td>-0.333310</td>\n",
       "      <td>-0.298701</td>\n",
       "      <td>-0.683803</td>\n",
       "      <td>-0.058847</td>\n",
       "      <td>-0.198511</td>\n",
       "      <td>-0.017378</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>0.134216</td>\n",
       "      <td>1.623948</td>\n",
       "      <td>0.309160</td>\n",
       "      <td>0.016629</td>\n",
       "      <td>-2.841270</td>\n",
       "      <td>0.057761</td>\n",
       "      <td>-0.121874</td>\n",
       "      <td>-0.301343</td>\n",
       "      <td>0.591229</td>\n",
       "      <td>0.122781</td>\n",
       "      <td>0.157077</td>\n",
       "      <td>-0.075809</td>\n",
       "      <td>0.289568</td>\n",
       "      <td>0.025465</td>\n",
       "      <td>0.224462</td>\n",
       "      <td>0.050155</td>\n",
       "      <td>-0.220560</td>\n",
       "      <td>-0.423566</td>\n",
       "      <td>-0.298882</td>\n",
       "      <td>-0.003763</td>\n",
       "      <td>0.089865</td>\n",
       "      <td>0.116149</td>\n",
       "      <td>-0.066239</td>\n",
       "      <td>-0.116553</td>\n",
       "      <td>-0.211578</td>\n",
       "      <td>-0.026433</td>\n",
       "      <td>0.066474</td>\n",
       "      <td>-0.172347</td>\n",
       "      <td>0.114439</td>\n",
       "      <td>1.008800</td>\n",
       "      <td>0.048970</td>\n",
       "      <td>2.113558</td>\n",
       "      <td>0.183062</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>0.328721</td>\n",
       "      <td>1.391015</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.246612</td>\n",
       "      <td>0.259096</td>\n",
       "      <td>0.428238</td>\n",
       "      <td>-0.113854</td>\n",
       "      <td>-0.072313</td>\n",
       "      <td>0.136662</td>\n",
       "      <td>-0.262657</td>\n",
       "      <td>-0.196306</td>\n",
       "      <td>-0.065022</td>\n",
       "      <td>-0.073605</td>\n",
       "      <td>0.112380</td>\n",
       "      <td>0.153970</td>\n",
       "      <td>-0.234338</td>\n",
       "      <td>-0.253047</td>\n",
       "      <td>-0.042749</td>\n",
       "      <td>-0.030998</td>\n",
       "      <td>-0.203711</td>\n",
       "      <td>0.182217</td>\n",
       "      <td>-0.255710</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>-0.058575</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>0.192080</td>\n",
       "      <td>1.438943</td>\n",
       "      <td>0.128215</td>\n",
       "      <td>-0.054786</td>\n",
       "      <td>0.577039</td>\n",
       "      <td>0.131675</td>\n",
       "      <td>0.065016</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>0.418629</td>\n",
       "      <td>-0.098543</td>\n",
       "      <td>-0.443419</td>\n",
       "      <td>-0.315794</td>\n",
       "      <td>-0.119009</td>\n",
       "      <td>0.308256</td>\n",
       "      <td>-0.164729</td>\n",
       "      <td>0.024635</td>\n",
       "      <td>0.097390</td>\n",
       "      <td>-0.153226</td>\n",
       "      <td>0.066648</td>\n",
       "      <td>-0.089602</td>\n",
       "      <td>-0.012601</td>\n",
       "      <td>-0.169174</td>\n",
       "      <td>0.033604</td>\n",
       "      <td>-0.017636</td>\n",
       "      <td>0.225650</td>\n",
       "      <td>-0.003460</td>\n",
       "      <td>0.091388</td>\n",
       "      <td>-0.043985</td>\n",
       "      <td>-0.090680</td>\n",
       "      <td>-0.060891</td>\n",
       "      <td>0.044943</td>\n",
       "      <td>0.080476</td>\n",
       "      <td>0.194946</td>\n",
       "      <td>0.252762</td>\n",
       "      <td>0.111750</td>\n",
       "      <td>-0.045780</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>-0.100169</td>\n",
       "      <td>0.108905</td>\n",
       "      <td>-0.034522</td>\n",
       "      <td>0.117903</td>\n",
       "      <td>0.236024</td>\n",
       "      <td>0.185837</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>-0.124239</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.456760</td>\n",
       "      <td>0.130609</td>\n",
       "      <td>0.084755</td>\n",
       "      <td>0.107448</td>\n",
       "      <td>0.221918</td>\n",
       "      <td>-0.272671</td>\n",
       "      <td>-0.175078</td>\n",
       "      <td>1.045756</td>\n",
       "      <td>0.088317</td>\n",
       "      <td>0.269874</td>\n",
       "      <td>0.043521</td>\n",
       "      <td>-0.258605</td>\n",
       "      <td>0.022063</td>\n",
       "      <td>0.277678</td>\n",
       "      <td>-2.092627</td>\n",
       "      <td>-0.238210</td>\n",
       "      <td>0.068849</td>\n",
       "      <td>-0.393595</td>\n",
       "      <td>0.038905</td>\n",
       "      <td>-0.137781</td>\n",
       "      <td>-0.100083</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>0.408899</td>\n",
       "      <td>-0.204873</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>0.469248</td>\n",
       "      <td>0.156101</td>\n",
       "      <td>0.150499</td>\n",
       "      <td>-1.912055</td>\n",
       "      <td>0.029725</td>\n",
       "      <td>0.057159</td>\n",
       "      <td>0.352324</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>-0.168845</td>\n",
       "      <td>-0.216576</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.256714</td>\n",
       "      <td>0.271091</td>\n",
       "      <td>-0.324324</td>\n",
       "      <td>0.253692</td>\n",
       "      <td>-0.162768</td>\n",
       "      <td>1.048868</td>\n",
       "      <td>-0.015548</td>\n",
       "      <td>0.101220</td>\n",
       "      <td>-0.398941</td>\n",
       "      <td>0.156095</td>\n",
       "      <td>0.285644</td>\n",
       "      <td>0.588194</td>\n",
       "      <td>-0.006593</td>\n",
       "      <td>0.381711</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>-0.123698</td>\n",
       "      <td>-0.237995</td>\n",
       "      <td>0.217746</td>\n",
       "      <td>0.134358</td>\n",
       "      <td>-0.252432</td>\n",
       "      <td>0.151119</td>\n",
       "      <td>-0.462669</td>\n",
       "      <td>-0.318928</td>\n",
       "      <td>-0.183064</td>\n",
       "      <td>-0.243318</td>\n",
       "      <td>0.218916</td>\n",
       "      <td>0.216158</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>0.127511</td>\n",
       "      <td>0.165406</td>\n",
       "      <td>-0.006228</td>\n",
       "      <td>0.055674</td>\n",
       "      <td>-0.137860</td>\n",
       "      <td>-0.120376</td>\n",
       "      <td>0.075913</td>\n",
       "      <td>-0.085324</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>-0.205171</td>\n",
       "      <td>-0.048181</td>\n",
       "      <td>-0.212470</td>\n",
       "      <td>0.058060</td>\n",
       "      <td>0.034922</td>\n",
       "      <td>0.342037</td>\n",
       "      <td>-0.033949</td>\n",
       "      <td>0.027961</td>\n",
       "      <td>-0.168917</td>\n",
       "      <td>-0.254982</td>\n",
       "      <td>-0.527160</td>\n",
       "      <td>-1.113713</td>\n",
       "      <td>-0.229061</td>\n",
       "      <td>-0.034499</td>\n",
       "      <td>-0.110246</td>\n",
       "      <td>-0.217912</td>\n",
       "      <td>-0.049210</td>\n",
       "      <td>-0.355850</td>\n",
       "      <td>-0.580077</td>\n",
       "      <td>-0.110057</td>\n",
       "      <td>-0.213163</td>\n",
       "      <td>0.030453</td>\n",
       "      <td>0.203308</td>\n",
       "      <td>-0.270568</td>\n",
       "      <td>-0.007047</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>-1.640332</td>\n",
       "      <td>0.327920</td>\n",
       "      <td>0.058750</td>\n",
       "      <td>0.041248</td>\n",
       "      <td>0.390960</td>\n",
       "      <td>0.216196</td>\n",
       "      <td>-0.291855</td>\n",
       "      <td>0.139326</td>\n",
       "      <td>0.134371</td>\n",
       "      <td>-0.194382</td>\n",
       "      <td>-0.130841</td>\n",
       "      <td>-0.088784</td>\n",
       "      <td>0.056905</td>\n",
       "      <td>0.100911</td>\n",
       "      <td>-0.171897</td>\n",
       "      <td>-0.006919</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>-1.802850</td>\n",
       "      <td>0.140893</td>\n",
       "      <td>0.054359</td>\n",
       "      <td>0.064364</td>\n",
       "      <td>0.045542</td>\n",
       "      <td>-0.069615</td>\n",
       "      <td>0.170746</td>\n",
       "      <td>0.135491</td>\n",
       "      <td>0.113001</td>\n",
       "      <td>0.106351</td>\n",
       "      <td>0.503941</td>\n",
       "      <td>0.023236</td>\n",
       "      <td>-0.149475</td>\n",
       "      <td>-0.038604</td>\n",
       "      <td>-0.028201</td>\n",
       "      <td>0.180176</td>\n",
       "      <td>-0.152957</td>\n",
       "      <td>0.044299</td>\n",
       "      <td>0.061128</td>\n",
       "      <td>-0.151400</td>\n",
       "      <td>-0.176841</td>\n",
       "      <td>0.180401</td>\n",
       "      <td>0.139347</td>\n",
       "      <td>0.093173</td>\n",
       "      <td>0.067539</td>\n",
       "      <td>-0.130477</td>\n",
       "      <td>-0.445037</td>\n",
       "      <td>0.109330</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.085073</td>\n",
       "      <td>0.250069</td>\n",
       "      <td>-0.570612</td>\n",
       "      <td>-0.318602</td>\n",
       "      <td>-0.086646</td>\n",
       "      <td>0.042726</td>\n",
       "      <td>-0.010509</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>-0.375788</td>\n",
       "      <td>-0.136379</td>\n",
       "      <td>-0.087900</td>\n",
       "      <td>-0.180236</td>\n",
       "      <td>0.029235</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>-0.085967</td>\n",
       "      <td>-0.072250</td>\n",
       "      <td>-0.199525</td>\n",
       "      <td>0.235542</td>\n",
       "      <td>-0.436713</td>\n",
       "      <td>0.120687</td>\n",
       "      <td>0.205707</td>\n",
       "      <td>-0.210220</td>\n",
       "      <td>-0.710811</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>-0.010203</td>\n",
       "      <td>0.080827</td>\n",
       "      <td>0.053471</td>\n",
       "      <td>-0.311711</td>\n",
       "      <td>-0.168204</td>\n",
       "      <td>0.067390</td>\n",
       "      <td>-0.205203</td>\n",
       "      <td>-1.537679</td>\n",
       "      <td>0.305504</td>\n",
       "      <td>0.329089</td>\n",
       "      <td>-1.564162</td>\n",
       "      <td>0.075029</td>\n",
       "      <td>0.076229</td>\n",
       "      <td>-0.200819</td>\n",
       "      <td>-0.131148</td>\n",
       "      <td>0.026027</td>\n",
       "      <td>-0.256942</td>\n",
       "      <td>0.252053</td>\n",
       "      <td>0.298461</td>\n",
       "      <td>-0.086356</td>\n",
       "      <td>-0.193547</td>\n",
       "      <td>0.055482</td>\n",
       "      <td>-0.099840</td>\n",
       "      <td>-0.198960</td>\n",
       "      <td>-0.060246</td>\n",
       "      <td>-0.017196</td>\n",
       "      <td>-0.160717</td>\n",
       "      <td>0.112565</td>\n",
       "      <td>-0.132643</td>\n",
       "      <td>-0.005622</td>\n",
       "      <td>-1.633456</td>\n",
       "      <td>-0.062534</td>\n",
       "      <td>0.014561</td>\n",
       "      <td>-0.104622</td>\n",
       "      <td>-0.117503</td>\n",
       "      <td>0.257080</td>\n",
       "      <td>-0.060662</td>\n",
       "      <td>0.244352</td>\n",
       "      <td>-0.127829</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.158270</td>\n",
       "      <td>-0.318931</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.168723</td>\n",
       "      <td>-0.508360</td>\n",
       "      <td>0.119342</td>\n",
       "      <td>0.455592</td>\n",
       "      <td>0.114864</td>\n",
       "      <td>-0.104670</td>\n",
       "      <td>-0.052394</td>\n",
       "      <td>0.173794</td>\n",
       "      <td>-0.036887</td>\n",
       "      <td>-0.176584</td>\n",
       "      <td>0.258948</td>\n",
       "      <td>0.020489</td>\n",
       "      <td>-0.154116</td>\n",
       "      <td>-0.135886</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>-1.483996</td>\n",
       "      <td>-1.929655</td>\n",
       "      <td>1.082304</td>\n",
       "      <td>-0.263823</td>\n",
       "      <td>0.088481</td>\n",
       "      <td>-0.150787</td>\n",
       "      <td>2.084621</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.093464</td>\n",
       "      <td>-0.220641</td>\n",
       "      <td>-0.145591</td>\n",
       "      <td>0.160669</td>\n",
       "      <td>3.216524</td>\n",
       "      <td>0.394509</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>1.465430</td>\n",
       "      <td>-0.133346</td>\n",
       "      <td>0.297071</td>\n",
       "      <td>-0.410105</td>\n",
       "      <td>-1.875664</td>\n",
       "      <td>-1.967450</td>\n",
       "      <td>0.234505</td>\n",
       "      <td>-0.215000</td>\n",
       "      <td>-0.006932</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>0.131632</td>\n",
       "      <td>0.053364</td>\n",
       "      <td>0.147339</td>\n",
       "      <td>0.183002</td>\n",
       "      <td>-0.291611</td>\n",
       "      <td>-0.386891</td>\n",
       "      <td>-0.199754</td>\n",
       "      <td>-0.120723</td>\n",
       "      <td>-0.042630</td>\n",
       "      <td>-0.073798</td>\n",
       "      <td>-0.144431</td>\n",
       "      <td>-0.067715</td>\n",
       "      <td>0.656683</td>\n",
       "      <td>-0.059617</td>\n",
       "      <td>0.201287</td>\n",
       "      <td>0.015651</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>-2.168596</td>\n",
       "      <td>0.243330</td>\n",
       "      <td>0.085551</td>\n",
       "      <td>-0.002216</td>\n",
       "      <td>0.098574</td>\n",
       "      <td>0.123953</td>\n",
       "      <td>-0.171547</td>\n",
       "      <td>-0.316257</td>\n",
       "      <td>-0.210408</td>\n",
       "      <td>0.078679</td>\n",
       "      <td>-0.031711</td>\n",
       "      <td>0.191861</td>\n",
       "      <td>-0.265081</td>\n",
       "      <td>-0.073679</td>\n",
       "      <td>-0.140103</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.231685</td>\n",
       "      <td>-0.068803</td>\n",
       "      <td>2.217354</td>\n",
       "      <td>-0.132934</td>\n",
       "      <td>-0.264120</td>\n",
       "      <td>0.372043</td>\n",
       "      <td>0.232893</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>-0.252251</td>\n",
       "      <td>-0.175052</td>\n",
       "      <td>-0.270504</td>\n",
       "      <td>-0.036600</td>\n",
       "      <td>-0.178605</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>0.087022</td>\n",
       "      <td>-0.117753</td>\n",
       "      <td>0.218738</td>\n",
       "      <td>-0.096903</td>\n",
       "      <td>0.275393</td>\n",
       "      <td>0.604561</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>-0.763375</td>\n",
       "      <td>0.110329</td>\n",
       "      <td>-0.116213</td>\n",
       "      <td>-0.015277</td>\n",
       "      <td>-0.148767</td>\n",
       "      <td>-0.252354</td>\n",
       "      <td>-0.096613</td>\n",
       "      <td>0.401045</td>\n",
       "      <td>-0.020862</td>\n",
       "      <td>-0.197651</td>\n",
       "      <td>-0.107114</td>\n",
       "      <td>-0.308751</td>\n",
       "      <td>-0.054740</td>\n",
       "      <td>-0.230146</td>\n",
       "      <td>0.045499</td>\n",
       "      <td>-0.159947</td>\n",
       "      <td>0.225794</td>\n",
       "      <td>-0.156760</td>\n",
       "      <td>-0.579758</td>\n",
       "      <td>-0.016450</td>\n",
       "      <td>1.404797</td>\n",
       "      <td>0.111808</td>\n",
       "      <td>0.040975</td>\n",
       "      <td>1.208381</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>-0.256045</td>\n",
       "      <td>-0.036632</td>\n",
       "      <td>0.121336</td>\n",
       "      <td>0.029794</td>\n",
       "      <td>0.034173</td>\n",
       "      <td>-0.368353</td>\n",
       "      <td>0.110052</td>\n",
       "      <td>0.257424</td>\n",
       "      <td>0.194618</td>\n",
       "      <td>-0.150148</td>\n",
       "      <td>-0.149690</td>\n",
       "      <td>0.092321</td>\n",
       "      <td>0.076165</td>\n",
       "      <td>0.303099</td>\n",
       "      <td>0.231132</td>\n",
       "      <td>0.345945</td>\n",
       "      <td>-0.165437</td>\n",
       "      <td>-0.085704</td>\n",
       "      <td>-0.069272</td>\n",
       "      <td>0.213369</td>\n",
       "      <td>-0.076859</td>\n",
       "      <td>0.020564</td>\n",
       "      <td>0.252402</td>\n",
       "      <td>0.226426</td>\n",
       "      <td>0.024572</td>\n",
       "      <td>-0.391727</td>\n",
       "      <td>-0.023108</td>\n",
       "      <td>-0.128336</td>\n",
       "      <td>-0.245829</td>\n",
       "      <td>0.394729</td>\n",
       "      <td>0.139937</td>\n",
       "      <td>0.088682</td>\n",
       "      <td>-0.077467</td>\n",
       "      <td>0.054343</td>\n",
       "      <td>-0.262566</td>\n",
       "      <td>-0.132284</td>\n",
       "      <td>0.137676</td>\n",
       "      <td>-0.164990</td>\n",
       "      <td>-0.137680</td>\n",
       "      <td>0.301046</td>\n",
       "      <td>0.086288</td>\n",
       "      <td>-0.124058</td>\n",
       "      <td>0.171924</td>\n",
       "      <td>-0.223275</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>-0.124004</td>\n",
       "      <td>-0.681465</td>\n",
       "      <td>-0.288272</td>\n",
       "      <td>-0.068686</td>\n",
       "      <td>-0.182465</td>\n",
       "      <td>-0.211806</td>\n",
       "      <td>-0.017527</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.360638</td>\n",
       "      <td>0.385750</td>\n",
       "      <td>0.120876</td>\n",
       "      <td>0.146029</td>\n",
       "      <td>0.455177</td>\n",
       "      <td>0.031480</td>\n",
       "      <td>-0.157944</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>0.117251</td>\n",
       "      <td>-0.098581</td>\n",
       "      <td>0.153611</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>-0.095238</td>\n",
       "      <td>-0.047519</td>\n",
       "      <td>-0.058652</td>\n",
       "      <td>0.204454</td>\n",
       "      <td>0.212656</td>\n",
       "      <td>-0.146449</td>\n",
       "      <td>-0.052718</td>\n",
       "      <td>-0.051662</td>\n",
       "      <td>0.239247</td>\n",
       "      <td>0.109455</td>\n",
       "      <td>0.389533</td>\n",
       "      <td>0.144415</td>\n",
       "      <td>-2.333261</td>\n",
       "      <td>-0.317807</td>\n",
       "      <td>0.222099</td>\n",
       "      <td>0.255289</td>\n",
       "      <td>-0.029469</td>\n",
       "      <td>0.230554</td>\n",
       "      <td>0.694520</td>\n",
       "      <td>-0.193432</td>\n",
       "      <td>-0.095616</td>\n",
       "      <td>-0.238705</td>\n",
       "      <td>0.110119</td>\n",
       "      <td>-0.050350</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.101708</td>\n",
       "      <td>-0.483683</td>\n",
       "      <td>-0.019196</td>\n",
       "      <td>0.287094</td>\n",
       "      <td>-0.011222</td>\n",
       "      <td>-0.136490</td>\n",
       "      <td>-0.056817</td>\n",
       "      <td>-1.835776</td>\n",
       "      <td>-0.228524</td>\n",
       "      <td>-0.072501</td>\n",
       "      <td>0.066729</td>\n",
       "      <td>-0.063621</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>-0.096271</td>\n",
       "      <td>-0.187056</td>\n",
       "      <td>-0.086765</td>\n",
       "      <td>0.066112</td>\n",
       "      <td>-0.191554</td>\n",
       "      <td>-0.213342</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>-0.192471</td>\n",
       "      <td>0.234207</td>\n",
       "      <td>0.094020</td>\n",
       "      <td>-0.042235</td>\n",
       "      <td>-0.058626</td>\n",
       "      <td>-0.110245</td>\n",
       "      <td>-0.129554</td>\n",
       "      <td>0.059742</td>\n",
       "      <td>0.274113</td>\n",
       "      <td>-0.104256</td>\n",
       "      <td>0.148161</td>\n",
       "      <td>0.265457</td>\n",
       "      <td>0.165809</td>\n",
       "      <td>0.107775</td>\n",
       "      <td>0.040481</td>\n",
       "      <td>-0.212443</td>\n",
       "      <td>0.078095</td>\n",
       "      <td>-1.782096</td>\n",
       "      <td>-0.026157</td>\n",
       "      <td>-0.226626</td>\n",
       "      <td>0.102626</td>\n",
       "      <td>-0.115495</td>\n",
       "      <td>0.192710</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>-0.160434</td>\n",
       "      <td>0.253892</td>\n",
       "      <td>0.322352</td>\n",
       "      <td>-0.386042</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.194733</td>\n",
       "      <td>0.058368</td>\n",
       "      <td>-0.090867</td>\n",
       "      <td>-0.101546</td>\n",
       "      <td>0.386789</td>\n",
       "      <td>0.093716</td>\n",
       "      <td>-0.267528</td>\n",
       "      <td>0.145146</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>-0.096692</td>\n",
       "      <td>0.502369</td>\n",
       "      <td>-0.192930</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>0.422873</td>\n",
       "      <td>0.265714</td>\n",
       "      <td>0.146159</td>\n",
       "      <td>-0.319094</td>\n",
       "      <td>0.222661</td>\n",
       "      <td>0.161907</td>\n",
       "      <td>-1.055999</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.101913</td>\n",
       "      <td>-0.295660</td>\n",
       "      <td>-0.045099</td>\n",
       "      <td>-0.244384</td>\n",
       "      <td>0.333828</td>\n",
       "      <td>0.459999</td>\n",
       "      <td>0.292852</td>\n",
       "      <td>-0.083219</td>\n",
       "      <td>0.515633</td>\n",
       "      <td>1.913599</td>\n",
       "      <td>-0.029188</td>\n",
       "      <td>-0.211715</td>\n",
       "      <td>-0.010307</td>\n",
       "      <td>0.258732</td>\n",
       "      <td>0.023835</td>\n",
       "      <td>0.310256</td>\n",
       "      <td>-0.192884</td>\n",
       "      <td>1.750768</td>\n",
       "      <td>-0.040066</td>\n",
       "      <td>-0.166193</td>\n",
       "      <td>-0.322148</td>\n",
       "      <td>0.147801</td>\n",
       "      <td>0.167517</td>\n",
       "      <td>1.832676</td>\n",
       "      <td>0.259535</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.121710</td>\n",
       "      <td>0.106218</td>\n",
       "      <td>-0.111246</td>\n",
       "      <td>0.175421</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>-0.058007</td>\n",
       "      <td>0.013651</td>\n",
       "      <td>-1.100867</td>\n",
       "      <td>0.038646</td>\n",
       "      <td>-0.059722</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>-0.131453</td>\n",
       "      <td>-0.741544</td>\n",
       "      <td>0.613290</td>\n",
       "      <td>0.141397</td>\n",
       "      <td>-0.180642</td>\n",
       "      <td>0.241143</td>\n",
       "      <td>0.359595</td>\n",
       "      <td>-0.112447</td>\n",
       "      <td>0.081373</td>\n",
       "      <td>0.075066</td>\n",
       "      <td>-0.066500</td>\n",
       "      <td>-0.070188</td>\n",
       "      <td>0.090865</td>\n",
       "      <td>-0.291487</td>\n",
       "      <td>-0.171487</td>\n",
       "      <td>0.192506</td>\n",
       "      <td>-0.285695</td>\n",
       "      <td>0.516094</td>\n",
       "      <td>-0.241200</td>\n",
       "      <td>-0.054998</td>\n",
       "      <td>0.609387</td>\n",
       "      <td>0.204573</td>\n",
       "      <td>0.157941</td>\n",
       "      <td>0.254089</td>\n",
       "      <td>-0.092161</td>\n",
       "      <td>-0.177658</td>\n",
       "      <td>-0.322701</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.462816</td>\n",
       "      <td>0.165087</td>\n",
       "      <td>-0.072143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052672</td>\n",
       "      <td>-0.058918</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.277223</td>\n",
       "      <td>0.259454</td>\n",
       "      <td>0.100490</td>\n",
       "      <td>-0.096110</td>\n",
       "      <td>0.028599</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>-0.022418</td>\n",
       "      <td>0.131286</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.163917</td>\n",
       "      <td>0.129935</td>\n",
       "      <td>-0.749790</td>\n",
       "      <td>-0.128104</td>\n",
       "      <td>-0.170851</td>\n",
       "      <td>0.145990</td>\n",
       "      <td>0.114734</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>-0.283772</td>\n",
       "      <td>-0.137989</td>\n",
       "      <td>-0.118955</td>\n",
       "      <td>-0.095648</td>\n",
       "      <td>-0.122314</td>\n",
       "      <td>-0.228784</td>\n",
       "      <td>0.025816</td>\n",
       "      <td>0.096372</td>\n",
       "      <td>0.057047</td>\n",
       "      <td>0.349612</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.196444</td>\n",
       "      <td>-0.050039</td>\n",
       "      <td>0.327058</td>\n",
       "      <td>0.184806</td>\n",
       "      <td>0.012651</td>\n",
       "      <td>-1.862826</td>\n",
       "      <td>-0.201311</td>\n",
       "      <td>-0.131837</td>\n",
       "      <td>-0.297519</td>\n",
       "      <td>0.315354</td>\n",
       "      <td>0.075185</td>\n",
       "      <td>-0.315775</td>\n",
       "      <td>-0.008864</td>\n",
       "      <td>0.124587</td>\n",
       "      <td>0.907268</td>\n",
       "      <td>0.184001</td>\n",
       "      <td>-0.308740</td>\n",
       "      <td>1.200685</td>\n",
       "      <td>-0.390387</td>\n",
       "      <td>0.443256</td>\n",
       "      <td>-0.695511</td>\n",
       "      <td>0.064472</td>\n",
       "      <td>-1.346388</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>0.126622</td>\n",
       "      <td>0.291861</td>\n",
       "      <td>-0.188345</td>\n",
       "      <td>-0.143834</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.190193</td>\n",
       "      <td>0.128031</td>\n",
       "      <td>0.121911</td>\n",
       "      <td>-0.067801</td>\n",
       "      <td>-0.373287</td>\n",
       "      <td>-0.057563</td>\n",
       "      <td>0.357599</td>\n",
       "      <td>0.221048</td>\n",
       "      <td>-0.121907</td>\n",
       "      <td>0.192224</td>\n",
       "      <td>-0.014816</td>\n",
       "      <td>0.303250</td>\n",
       "      <td>0.206961</td>\n",
       "      <td>-0.121902</td>\n",
       "      <td>0.172318</td>\n",
       "      <td>0.275762</td>\n",
       "      <td>0.074032</td>\n",
       "      <td>0.073718</td>\n",
       "      <td>0.082032</td>\n",
       "      <td>-0.076715</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.311564</td>\n",
       "      <td>-0.495520</td>\n",
       "      <td>0.135617</td>\n",
       "      <td>-0.105971</td>\n",
       "      <td>-0.114640</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>-0.026936</td>\n",
       "      <td>0.065985</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.273669</td>\n",
       "      <td>-0.065600</td>\n",
       "      <td>-0.768612</td>\n",
       "      <td>0.052605</td>\n",
       "      <td>-0.348544</td>\n",
       "      <td>-0.940609</td>\n",
       "      <td>-0.053573</td>\n",
       "      <td>0.093908</td>\n",
       "      <td>0.200630</td>\n",
       "      <td>-0.055771</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>1.204730</td>\n",
       "      <td>0.665214</td>\n",
       "      <td>0.238307</td>\n",
       "      <td>-2.531948</td>\n",
       "      <td>0.051051</td>\n",
       "      <td>0.189210</td>\n",
       "      <td>-0.537894</td>\n",
       "      <td>0.488715</td>\n",
       "      <td>-0.026767</td>\n",
       "      <td>0.235405</td>\n",
       "      <td>-0.035871</td>\n",
       "      <td>0.320023</td>\n",
       "      <td>0.032610</td>\n",
       "      <td>0.025327</td>\n",
       "      <td>0.045891</td>\n",
       "      <td>0.200612</td>\n",
       "      <td>-0.094909</td>\n",
       "      <td>-0.383914</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>-0.278634</td>\n",
       "      <td>0.095051</td>\n",
       "      <td>0.084782</td>\n",
       "      <td>-0.127206</td>\n",
       "      <td>-0.124167</td>\n",
       "      <td>-0.214576</td>\n",
       "      <td>-0.098237</td>\n",
       "      <td>-0.354313</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>0.631516</td>\n",
       "      <td>0.098180</td>\n",
       "      <td>1.734454</td>\n",
       "      <td>0.078826</td>\n",
       "      <td>-0.433104</td>\n",
       "      <td>0.204309</td>\n",
       "      <td>1.436106</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>0.421982</td>\n",
       "      <td>0.505524</td>\n",
       "      <td>0.497787</td>\n",
       "      <td>-0.072330</td>\n",
       "      <td>-0.060478</td>\n",
       "      <td>0.230735</td>\n",
       "      <td>-0.030728</td>\n",
       "      <td>-0.236866</td>\n",
       "      <td>0.057978</td>\n",
       "      <td>0.192613</td>\n",
       "      <td>0.188570</td>\n",
       "      <td>0.201878</td>\n",
       "      <td>-0.129033</td>\n",
       "      <td>-0.254614</td>\n",
       "      <td>-0.172734</td>\n",
       "      <td>0.153413</td>\n",
       "      <td>-0.106675</td>\n",
       "      <td>0.317641</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>0.110751</td>\n",
       "      <td>-0.294944</td>\n",
       "      <td>-0.223167</td>\n",
       "      <td>0.704600</td>\n",
       "      <td>1.681405</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>-0.202991</td>\n",
       "      <td>0.510207</td>\n",
       "      <td>0.068040</td>\n",
       "      <td>-0.075538</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>0.091109</td>\n",
       "      <td>-0.222204</td>\n",
       "      <td>-0.408947</td>\n",
       "      <td>-0.335609</td>\n",
       "      <td>-0.332711</td>\n",
       "      <td>-0.113538</td>\n",
       "      <td>-0.086438</td>\n",
       "      <td>-0.219503</td>\n",
       "      <td>-0.354867</td>\n",
       "      <td>-0.105105</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>-0.037553</td>\n",
       "      <td>-0.089969</td>\n",
       "      <td>0.027282</td>\n",
       "      <td>0.029195</td>\n",
       "      <td>-0.048773</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.241397</td>\n",
       "      <td>0.057202</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>-0.132713</td>\n",
       "      <td>-0.009519</td>\n",
       "      <td>0.188325</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.135744</td>\n",
       "      <td>0.133174</td>\n",
       "      <td>0.131778</td>\n",
       "      <td>0.254248</td>\n",
       "      <td>-0.340015</td>\n",
       "      <td>0.174643</td>\n",
       "      <td>0.053142</td>\n",
       "      <td>0.103338</td>\n",
       "      <td>0.149088</td>\n",
       "      <td>0.293034</td>\n",
       "      <td>-0.181952</td>\n",
       "      <td>-0.105761</td>\n",
       "      <td>-0.033977</td>\n",
       "      <td>0.536524</td>\n",
       "      <td>0.067477</td>\n",
       "      <td>-0.321624</td>\n",
       "      <td>0.067708</td>\n",
       "      <td>0.358331</td>\n",
       "      <td>-0.217633</td>\n",
       "      <td>-0.080630</td>\n",
       "      <td>0.992161</td>\n",
       "      <td>0.162348</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.146003</td>\n",
       "      <td>-0.078902</td>\n",
       "      <td>0.326103</td>\n",
       "      <td>-2.175950</td>\n",
       "      <td>0.074236</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.088233</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>-0.213741</td>\n",
       "      <td>0.015710</td>\n",
       "      <td>0.087525</td>\n",
       "      <td>0.259136</td>\n",
       "      <td>-0.068323</td>\n",
       "      <td>0.082989</td>\n",
       "      <td>0.368531</td>\n",
       "      <td>-0.357657</td>\n",
       "      <td>0.271537</td>\n",
       "      <td>-1.579786</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.161659</td>\n",
       "      <td>-0.129009</td>\n",
       "      <td>-0.005902</td>\n",
       "      <td>0.190349</td>\n",
       "      <td>0.467461</td>\n",
       "      <td>-0.093247</td>\n",
       "      <td>-0.032525</td>\n",
       "      <td>-0.231185</td>\n",
       "      <td>0.262532</td>\n",
       "      <td>-0.227358</td>\n",
       "      <td>1.034526</td>\n",
       "      <td>-0.204923</td>\n",
       "      <td>-0.084587</td>\n",
       "      <td>-0.122245</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>0.071010</td>\n",
       "      <td>0.479335</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>-0.138671</td>\n",
       "      <td>0.275043</td>\n",
       "      <td>-0.053618</td>\n",
       "      <td>-0.344126</td>\n",
       "      <td>0.070405</td>\n",
       "      <td>-0.138441</td>\n",
       "      <td>0.040745</td>\n",
       "      <td>0.141255</td>\n",
       "      <td>-0.336352</td>\n",
       "      <td>-0.371722</td>\n",
       "      <td>-0.066489</td>\n",
       "      <td>-0.310397</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>0.146001</td>\n",
       "      <td>-0.142373</td>\n",
       "      <td>-0.140524</td>\n",
       "      <td>-0.097830</td>\n",
       "      <td>-0.011692</td>\n",
       "      <td>0.127692</td>\n",
       "      <td>-0.196047</td>\n",
       "      <td>0.147516</td>\n",
       "      <td>-0.194761</td>\n",
       "      <td>0.066085</td>\n",
       "      <td>0.051742</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>-0.071259</td>\n",
       "      <td>-0.082067</td>\n",
       "      <td>0.221838</td>\n",
       "      <td>0.036886</td>\n",
       "      <td>0.193118</td>\n",
       "      <td>0.069551</td>\n",
       "      <td>-0.088746</td>\n",
       "      <td>-0.201686</td>\n",
       "      <td>-0.903562</td>\n",
       "      <td>-0.063673</td>\n",
       "      <td>0.091515</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>-0.218143</td>\n",
       "      <td>0.026177</td>\n",
       "      <td>0.047984</td>\n",
       "      <td>-0.249656</td>\n",
       "      <td>-0.097899</td>\n",
       "      <td>-0.105714</td>\n",
       "      <td>0.259082</td>\n",
       "      <td>0.325662</td>\n",
       "      <td>-0.098979</td>\n",
       "      <td>0.079754</td>\n",
       "      <td>0.274477</td>\n",
       "      <td>-1.577111</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.445884</td>\n",
       "      <td>-0.122460</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.045718</td>\n",
       "      <td>-0.253436</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>-0.145037</td>\n",
       "      <td>-0.244011</td>\n",
       "      <td>-0.048429</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.295289</td>\n",
       "      <td>0.201259</td>\n",
       "      <td>-0.096990</td>\n",
       "      <td>-0.035488</td>\n",
       "      <td>-0.201499</td>\n",
       "      <td>-1.669044</td>\n",
       "      <td>0.178489</td>\n",
       "      <td>0.094926</td>\n",
       "      <td>0.038488</td>\n",
       "      <td>-0.138048</td>\n",
       "      <td>0.035850</td>\n",
       "      <td>0.125822</td>\n",
       "      <td>0.153879</td>\n",
       "      <td>0.104066</td>\n",
       "      <td>0.075819</td>\n",
       "      <td>-0.107096</td>\n",
       "      <td>-0.275737</td>\n",
       "      <td>-0.271589</td>\n",
       "      <td>-0.227140</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>-0.106995</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.096130</td>\n",
       "      <td>-0.297114</td>\n",
       "      <td>-0.370665</td>\n",
       "      <td>-0.090220</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>0.246668</td>\n",
       "      <td>-0.106050</td>\n",
       "      <td>0.273808</td>\n",
       "      <td>0.218396</td>\n",
       "      <td>-0.227423</td>\n",
       "      <td>0.292198</td>\n",
       "      <td>0.360163</td>\n",
       "      <td>0.128740</td>\n",
       "      <td>0.179784</td>\n",
       "      <td>-0.361277</td>\n",
       "      <td>-0.108207</td>\n",
       "      <td>-0.017846</td>\n",
       "      <td>0.124008</td>\n",
       "      <td>-0.091146</td>\n",
       "      <td>0.145459</td>\n",
       "      <td>-0.043438</td>\n",
       "      <td>0.153791</td>\n",
       "      <td>-0.177207</td>\n",
       "      <td>-0.155849</td>\n",
       "      <td>0.046694</td>\n",
       "      <td>0.126901</td>\n",
       "      <td>0.327502</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>-0.177967</td>\n",
       "      <td>-0.251835</td>\n",
       "      <td>-0.165355</td>\n",
       "      <td>0.395987</td>\n",
       "      <td>0.420862</td>\n",
       "      <td>0.351807</td>\n",
       "      <td>-0.331257</td>\n",
       "      <td>-0.673759</td>\n",
       "      <td>0.336020</td>\n",
       "      <td>-0.122305</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>-0.293467</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>-0.158813</td>\n",
       "      <td>0.100868</td>\n",
       "      <td>-0.089790</td>\n",
       "      <td>-1.169701</td>\n",
       "      <td>0.179054</td>\n",
       "      <td>0.248582</td>\n",
       "      <td>-1.394230</td>\n",
       "      <td>-0.149468</td>\n",
       "      <td>-0.023500</td>\n",
       "      <td>-0.021593</td>\n",
       "      <td>-0.020985</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.082202</td>\n",
       "      <td>0.122629</td>\n",
       "      <td>-0.391835</td>\n",
       "      <td>0.009212</td>\n",
       "      <td>-0.009983</td>\n",
       "      <td>-0.022500</td>\n",
       "      <td>-0.196205</td>\n",
       "      <td>-0.122086</td>\n",
       "      <td>0.161794</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>-0.256333</td>\n",
       "      <td>0.271447</td>\n",
       "      <td>0.161287</td>\n",
       "      <td>-0.187338</td>\n",
       "      <td>-1.482814</td>\n",
       "      <td>-0.154344</td>\n",
       "      <td>0.286548</td>\n",
       "      <td>-0.042049</td>\n",
       "      <td>-0.071985</td>\n",
       "      <td>0.049052</td>\n",
       "      <td>-0.204481</td>\n",
       "      <td>0.259334</td>\n",
       "      <td>0.021722</td>\n",
       "      <td>-0.145066</td>\n",
       "      <td>0.033043</td>\n",
       "      <td>-0.414365</td>\n",
       "      <td>0.147750</td>\n",
       "      <td>0.182834</td>\n",
       "      <td>-0.569998</td>\n",
       "      <td>0.227454</td>\n",
       "      <td>0.411193</td>\n",
       "      <td>0.059790</td>\n",
       "      <td>-0.568324</td>\n",
       "      <td>-0.017131</td>\n",
       "      <td>0.123660</td>\n",
       "      <td>0.232415</td>\n",
       "      <td>0.094009</td>\n",
       "      <td>0.175487</td>\n",
       "      <td>0.028380</td>\n",
       "      <td>-0.485617</td>\n",
       "      <td>0.106544</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>-1.039344</td>\n",
       "      <td>-1.771620</td>\n",
       "      <td>0.636875</td>\n",
       "      <td>-0.175646</td>\n",
       "      <td>-0.013233</td>\n",
       "      <td>0.218205</td>\n",
       "      <td>1.987606</td>\n",
       "      <td>0.102627</td>\n",
       "      <td>0.197109</td>\n",
       "      <td>0.071595</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>-0.156831</td>\n",
       "      <td>2.751392</td>\n",
       "      <td>0.377630</td>\n",
       "      <td>-0.027311</td>\n",
       "      <td>1.131103</td>\n",
       "      <td>-0.120357</td>\n",
       "      <td>0.394758</td>\n",
       "      <td>-0.361440</td>\n",
       "      <td>-1.504752</td>\n",
       "      <td>-2.058141</td>\n",
       "      <td>-0.205147</td>\n",
       "      <td>-0.599994</td>\n",
       "      <td>-0.011699</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>0.049833</td>\n",
       "      <td>-0.184557</td>\n",
       "      <td>0.057525</td>\n",
       "      <td>-0.205613</td>\n",
       "      <td>-0.124114</td>\n",
       "      <td>-0.085659</td>\n",
       "      <td>0.163440</td>\n",
       "      <td>-0.101729</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>-0.034334</td>\n",
       "      <td>-0.174804</td>\n",
       "      <td>0.395555</td>\n",
       "      <td>0.413684</td>\n",
       "      <td>0.021974</td>\n",
       "      <td>-0.141392</td>\n",
       "      <td>0.359166</td>\n",
       "      <td>0.051719</td>\n",
       "      <td>-1.513369</td>\n",
       "      <td>0.264356</td>\n",
       "      <td>0.192586</td>\n",
       "      <td>0.110194</td>\n",
       "      <td>0.202126</td>\n",
       "      <td>0.204930</td>\n",
       "      <td>-0.076260</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>-0.265937</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>0.034266</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.067865</td>\n",
       "      <td>-0.109329</td>\n",
       "      <td>-0.018298</td>\n",
       "      <td>0.438693</td>\n",
       "      <td>-0.028649</td>\n",
       "      <td>-0.264353</td>\n",
       "      <td>1.436473</td>\n",
       "      <td>-0.416519</td>\n",
       "      <td>-0.035952</td>\n",
       "      <td>0.159192</td>\n",
       "      <td>-0.059729</td>\n",
       "      <td>0.706937</td>\n",
       "      <td>-0.230198</td>\n",
       "      <td>0.058927</td>\n",
       "      <td>-0.134497</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.006109</td>\n",
       "      <td>-0.131867</td>\n",
       "      <td>-0.114722</td>\n",
       "      <td>-0.357298</td>\n",
       "      <td>0.408232</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>0.090089</td>\n",
       "      <td>0.632461</td>\n",
       "      <td>0.565319</td>\n",
       "      <td>0.191118</td>\n",
       "      <td>-0.867875</td>\n",
       "      <td>0.207628</td>\n",
       "      <td>0.123648</td>\n",
       "      <td>-0.037126</td>\n",
       "      <td>-0.260453</td>\n",
       "      <td>-0.321050</td>\n",
       "      <td>0.164743</td>\n",
       "      <td>0.344055</td>\n",
       "      <td>0.087359</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>-0.024404</td>\n",
       "      <td>0.024432</td>\n",
       "      <td>-0.204412</td>\n",
       "      <td>-0.026046</td>\n",
       "      <td>-0.240043</td>\n",
       "      <td>-0.064821</td>\n",
       "      <td>0.183053</td>\n",
       "      <td>-0.224395</td>\n",
       "      <td>-0.376258</td>\n",
       "      <td>0.081533</td>\n",
       "      <td>1.017830</td>\n",
       "      <td>0.100157</td>\n",
       "      <td>0.178653</td>\n",
       "      <td>0.943344</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>-0.113825</td>\n",
       "      <td>0.040710</td>\n",
       "      <td>0.314431</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>-0.320023</td>\n",
       "      <td>-0.094417</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.196241</td>\n",
       "      <td>0.189659</td>\n",
       "      <td>0.047881</td>\n",
       "      <td>-0.127979</td>\n",
       "      <td>-0.229229</td>\n",
       "      <td>0.044677</td>\n",
       "      <td>0.266998</td>\n",
       "      <td>0.295038</td>\n",
       "      <td>0.322375</td>\n",
       "      <td>-0.101064</td>\n",
       "      <td>0.114283</td>\n",
       "      <td>-0.107116</td>\n",
       "      <td>0.213579</td>\n",
       "      <td>0.163793</td>\n",
       "      <td>0.054643</td>\n",
       "      <td>0.082962</td>\n",
       "      <td>0.080243</td>\n",
       "      <td>0.342241</td>\n",
       "      <td>-0.103329</td>\n",
       "      <td>-0.087903</td>\n",
       "      <td>-0.164677</td>\n",
       "      <td>-0.130176</td>\n",
       "      <td>0.100276</td>\n",
       "      <td>-0.073228</td>\n",
       "      <td>0.117260</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>-0.145715</td>\n",
       "      <td>0.087322</td>\n",
       "      <td>-0.323063</td>\n",
       "      <td>0.143101</td>\n",
       "      <td>-0.176359</td>\n",
       "      <td>-0.398334</td>\n",
       "      <td>0.261797</td>\n",
       "      <td>0.372226</td>\n",
       "      <td>0.115816</td>\n",
       "      <td>0.046890</td>\n",
       "      <td>-0.281192</td>\n",
       "      <td>-0.257780</td>\n",
       "      <td>-0.030262</td>\n",
       "      <td>-0.368841</td>\n",
       "      <td>-0.087002</td>\n",
       "      <td>-0.158140</td>\n",
       "      <td>-0.133311</td>\n",
       "      <td>-0.131062</td>\n",
       "      <td>-0.172555</td>\n",
       "      <td>-0.217656</td>\n",
       "      <td>0.278188</td>\n",
       "      <td>0.030150</td>\n",
       "      <td>0.142261</td>\n",
       "      <td>0.224161</td>\n",
       "      <td>0.316851</td>\n",
       "      <td>0.135279</td>\n",
       "      <td>0.030255</td>\n",
       "      <td>0.156185</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0.292328</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.195537</td>\n",
       "      <td>0.036159</td>\n",
       "      <td>-0.276614</td>\n",
       "      <td>-0.207048</td>\n",
       "      <td>0.166374</td>\n",
       "      <td>0.228283</td>\n",
       "      <td>-0.568953</td>\n",
       "      <td>-0.030577</td>\n",
       "      <td>0.187095</td>\n",
       "      <td>0.398820</td>\n",
       "      <td>-0.276514</td>\n",
       "      <td>0.068632</td>\n",
       "      <td>0.116219</td>\n",
       "      <td>-2.113282</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>-0.041259</td>\n",
       "      <td>0.180413</td>\n",
       "      <td>-0.061698</td>\n",
       "      <td>0.361656</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>-0.058756</td>\n",
       "      <td>-0.228537</td>\n",
       "      <td>0.046178</td>\n",
       "      <td>-0.108297</td>\n",
       "      <td>0.047795</td>\n",
       "      <td>-0.139194</td>\n",
       "      <td>-0.462275</td>\n",
       "      <td>-0.130494</td>\n",
       "      <td>-0.243649</td>\n",
       "      <td>0.220655</td>\n",
       "      <td>-0.339361</td>\n",
       "      <td>0.073952</td>\n",
       "      <td>-1.444949</td>\n",
       "      <td>-0.447369</td>\n",
       "      <td>-0.151190</td>\n",
       "      <td>0.221576</td>\n",
       "      <td>0.178028</td>\n",
       "      <td>-0.024075</td>\n",
       "      <td>-0.097401</td>\n",
       "      <td>-0.086004</td>\n",
       "      <td>-0.504357</td>\n",
       "      <td>0.056902</td>\n",
       "      <td>-0.103230</td>\n",
       "      <td>-0.195016</td>\n",
       "      <td>0.216620</td>\n",
       "      <td>-0.432201</td>\n",
       "      <td>0.073675</td>\n",
       "      <td>-0.058388</td>\n",
       "      <td>-0.300429</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>-0.107911</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>-0.120935</td>\n",
       "      <td>-0.223063</td>\n",
       "      <td>-0.339355</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>0.262837</td>\n",
       "      <td>0.363263</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>-0.174584</td>\n",
       "      <td>0.103692</td>\n",
       "      <td>-1.399578</td>\n",
       "      <td>-0.114160</td>\n",
       "      <td>-0.117055</td>\n",
       "      <td>0.166249</td>\n",
       "      <td>-0.172455</td>\n",
       "      <td>-0.004891</td>\n",
       "      <td>-0.016554</td>\n",
       "      <td>-0.024926</td>\n",
       "      <td>0.182688</td>\n",
       "      <td>0.155108</td>\n",
       "      <td>-0.163391</td>\n",
       "      <td>-0.100343</td>\n",
       "      <td>0.173562</td>\n",
       "      <td>-0.032489</td>\n",
       "      <td>-0.094124</td>\n",
       "      <td>-0.132808</td>\n",
       "      <td>0.472829</td>\n",
       "      <td>0.166019</td>\n",
       "      <td>-0.081053</td>\n",
       "      <td>0.213492</td>\n",
       "      <td>0.196840</td>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.223372</td>\n",
       "      <td>-0.336534</td>\n",
       "      <td>0.187136</td>\n",
       "      <td>0.284269</td>\n",
       "      <td>-0.018717</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>-0.181287</td>\n",
       "      <td>0.421302</td>\n",
       "      <td>0.170856</td>\n",
       "      <td>-0.858100</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>-0.146746</td>\n",
       "      <td>-0.012470</td>\n",
       "      <td>0.064857</td>\n",
       "      <td>-0.095487</td>\n",
       "      <td>0.155275</td>\n",
       "      <td>0.096208</td>\n",
       "      <td>0.568612</td>\n",
       "      <td>-0.117829</td>\n",
       "      <td>0.487051</td>\n",
       "      <td>1.664372</td>\n",
       "      <td>0.268089</td>\n",
       "      <td>0.060689</td>\n",
       "      <td>0.253742</td>\n",
       "      <td>0.154707</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-0.268022</td>\n",
       "      <td>-0.328705</td>\n",
       "      <td>1.512110</td>\n",
       "      <td>-0.157075</td>\n",
       "      <td>-0.032671</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>0.480546</td>\n",
       "      <td>0.047024</td>\n",
       "      <td>1.439537</td>\n",
       "      <td>0.091194</td>\n",
       "      <td>0.193451</td>\n",
       "      <td>-0.224989</td>\n",
       "      <td>0.241381</td>\n",
       "      <td>-0.017364</td>\n",
       "      <td>0.445531</td>\n",
       "      <td>0.124944</td>\n",
       "      <td>-0.017264</td>\n",
       "      <td>0.024864</td>\n",
       "      <td>-1.381781</td>\n",
       "      <td>-0.134459</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.351040</td>\n",
       "      <td>0.057493</td>\n",
       "      <td>-0.482758</td>\n",
       "      <td>0.404899</td>\n",
       "      <td>-0.045970</td>\n",
       "      <td>-0.130372</td>\n",
       "      <td>0.065098</td>\n",
       "      <td>0.099730</td>\n",
       "      <td>-0.543596</td>\n",
       "      <td>0.153142</td>\n",
       "      <td>-0.132317</td>\n",
       "      <td>0.248659</td>\n",
       "      <td>-0.178446</td>\n",
       "      <td>0.072395</td>\n",
       "      <td>-0.323997</td>\n",
       "      <td>0.151082</td>\n",
       "      <td>-0.027633</td>\n",
       "      <td>0.117040</td>\n",
       "      <td>0.360472</td>\n",
       "      <td>-0.200037</td>\n",
       "      <td>0.166041</td>\n",
       "      <td>0.654701</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.113429</td>\n",
       "      <td>0.114728</td>\n",
       "      <td>-0.077202</td>\n",
       "      <td>-0.269712</td>\n",
       "      <td>-0.151349</td>\n",
       "      <td>0.033660</td>\n",
       "      <td>0.276272</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>0.664379</td>\n",
       "      <td>-0.040229</td>\n",
       "      <td>-0.356698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58746.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080426</td>\n",
       "      <td>-0.134674</td>\n",
       "      <td>0.112707</td>\n",
       "      <td>0.176269</td>\n",
       "      <td>0.488535</td>\n",
       "      <td>-0.147479</td>\n",
       "      <td>0.186054</td>\n",
       "      <td>-0.220972</td>\n",
       "      <td>0.044768</td>\n",
       "      <td>0.135161</td>\n",
       "      <td>0.122971</td>\n",
       "      <td>-0.212626</td>\n",
       "      <td>0.327361</td>\n",
       "      <td>0.303039</td>\n",
       "      <td>-0.856692</td>\n",
       "      <td>-0.036403</td>\n",
       "      <td>-0.016536</td>\n",
       "      <td>0.346242</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>0.203156</td>\n",
       "      <td>-0.120113</td>\n",
       "      <td>-0.064584</td>\n",
       "      <td>-0.072145</td>\n",
       "      <td>-0.281763</td>\n",
       "      <td>-0.095024</td>\n",
       "      <td>-0.598254</td>\n",
       "      <td>-0.139197</td>\n",
       "      <td>0.418660</td>\n",
       "      <td>0.406367</td>\n",
       "      <td>0.134682</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.143624</td>\n",
       "      <td>-0.023052</td>\n",
       "      <td>0.107601</td>\n",
       "      <td>0.081439</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-1.778479</td>\n",
       "      <td>-0.099808</td>\n",
       "      <td>-0.007258</td>\n",
       "      <td>-0.227994</td>\n",
       "      <td>-0.162111</td>\n",
       "      <td>0.179612</td>\n",
       "      <td>-0.119347</td>\n",
       "      <td>-0.176773</td>\n",
       "      <td>0.274527</td>\n",
       "      <td>0.815549</td>\n",
       "      <td>0.259377</td>\n",
       "      <td>-0.013182</td>\n",
       "      <td>0.902782</td>\n",
       "      <td>-0.291883</td>\n",
       "      <td>-0.044108</td>\n",
       "      <td>-0.737716</td>\n",
       "      <td>-0.184261</td>\n",
       "      <td>-1.206096</td>\n",
       "      <td>0.026269</td>\n",
       "      <td>-0.086320</td>\n",
       "      <td>0.294976</td>\n",
       "      <td>-0.311108</td>\n",
       "      <td>-0.130351</td>\n",
       "      <td>-0.131200</td>\n",
       "      <td>0.221982</td>\n",
       "      <td>0.262959</td>\n",
       "      <td>0.214355</td>\n",
       "      <td>0.097085</td>\n",
       "      <td>-0.189611</td>\n",
       "      <td>-0.189078</td>\n",
       "      <td>0.397775</td>\n",
       "      <td>0.184169</td>\n",
       "      <td>-0.027201</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0.032806</td>\n",
       "      <td>0.244333</td>\n",
       "      <td>0.224702</td>\n",
       "      <td>-0.027513</td>\n",
       "      <td>0.132306</td>\n",
       "      <td>0.013470</td>\n",
       "      <td>0.132776</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>-0.378284</td>\n",
       "      <td>0.277806</td>\n",
       "      <td>-0.015235</td>\n",
       "      <td>-0.011454</td>\n",
       "      <td>0.042170</td>\n",
       "      <td>0.388840</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>-0.275472</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>-0.024743</td>\n",
       "      <td>0.110728</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>-0.165665</td>\n",
       "      <td>-0.255867</td>\n",
       "      <td>-0.486603</td>\n",
       "      <td>-0.042945</td>\n",
       "      <td>-0.137272</td>\n",
       "      <td>-0.663345</td>\n",
       "      <td>0.206857</td>\n",
       "      <td>0.038714</td>\n",
       "      <td>0.249513</td>\n",
       "      <td>-0.051702</td>\n",
       "      <td>-0.126937</td>\n",
       "      <td>1.299432</td>\n",
       "      <td>0.290933</td>\n",
       "      <td>-0.113339</td>\n",
       "      <td>-2.107814</td>\n",
       "      <td>0.302757</td>\n",
       "      <td>0.242091</td>\n",
       "      <td>-0.312805</td>\n",
       "      <td>0.515373</td>\n",
       "      <td>0.225225</td>\n",
       "      <td>0.194329</td>\n",
       "      <td>0.051338</td>\n",
       "      <td>0.381716</td>\n",
       "      <td>-0.136601</td>\n",
       "      <td>-0.098521</td>\n",
       "      <td>-0.048946</td>\n",
       "      <td>0.035652</td>\n",
       "      <td>-0.289012</td>\n",
       "      <td>-0.104529</td>\n",
       "      <td>-0.076477</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.250727</td>\n",
       "      <td>0.488066</td>\n",
       "      <td>-0.163111</td>\n",
       "      <td>0.074161</td>\n",
       "      <td>-0.492105</td>\n",
       "      <td>0.141187</td>\n",
       "      <td>-0.429315</td>\n",
       "      <td>-0.015018</td>\n",
       "      <td>0.875712</td>\n",
       "      <td>-0.125413</td>\n",
       "      <td>1.471851</td>\n",
       "      <td>-0.132518</td>\n",
       "      <td>-0.057724</td>\n",
       "      <td>0.034620</td>\n",
       "      <td>1.556313</td>\n",
       "      <td>0.174178</td>\n",
       "      <td>0.398473</td>\n",
       "      <td>0.318080</td>\n",
       "      <td>0.213674</td>\n",
       "      <td>-0.270714</td>\n",
       "      <td>0.020475</td>\n",
       "      <td>0.138001</td>\n",
       "      <td>-0.262408</td>\n",
       "      <td>-0.151041</td>\n",
       "      <td>0.176879</td>\n",
       "      <td>-0.110966</td>\n",
       "      <td>0.306894</td>\n",
       "      <td>0.031218</td>\n",
       "      <td>-0.280083</td>\n",
       "      <td>0.090888</td>\n",
       "      <td>-0.005017</td>\n",
       "      <td>0.234349</td>\n",
       "      <td>-0.301008</td>\n",
       "      <td>0.057005</td>\n",
       "      <td>-0.040138</td>\n",
       "      <td>0.139075</td>\n",
       "      <td>-0.240067</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>0.634772</td>\n",
       "      <td>1.273974</td>\n",
       "      <td>0.130543</td>\n",
       "      <td>-0.112971</td>\n",
       "      <td>0.880515</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>-0.185321</td>\n",
       "      <td>0.114886</td>\n",
       "      <td>0.150478</td>\n",
       "      <td>-0.078345</td>\n",
       "      <td>-0.457893</td>\n",
       "      <td>-0.306409</td>\n",
       "      <td>-0.084836</td>\n",
       "      <td>0.023258</td>\n",
       "      <td>-0.020834</td>\n",
       "      <td>0.082531</td>\n",
       "      <td>-0.264680</td>\n",
       "      <td>-0.318537</td>\n",
       "      <td>0.106770</td>\n",
       "      <td>-0.155994</td>\n",
       "      <td>0.047493</td>\n",
       "      <td>-0.025387</td>\n",
       "      <td>0.396370</td>\n",
       "      <td>-0.493968</td>\n",
       "      <td>-0.070045</td>\n",
       "      <td>0.330228</td>\n",
       "      <td>-0.301858</td>\n",
       "      <td>0.096318</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>-0.274402</td>\n",
       "      <td>-0.408135</td>\n",
       "      <td>0.118408</td>\n",
       "      <td>0.323034</td>\n",
       "      <td>-0.007343</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.268068</td>\n",
       "      <td>-0.341359</td>\n",
       "      <td>-0.223546</td>\n",
       "      <td>-0.089394</td>\n",
       "      <td>0.133622</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>-0.034420</td>\n",
       "      <td>-0.199397</td>\n",
       "      <td>-0.385047</td>\n",
       "      <td>0.116580</td>\n",
       "      <td>0.583352</td>\n",
       "      <td>0.106876</td>\n",
       "      <td>-0.450092</td>\n",
       "      <td>-0.066457</td>\n",
       "      <td>0.593961</td>\n",
       "      <td>-0.060532</td>\n",
       "      <td>-0.146741</td>\n",
       "      <td>0.894652</td>\n",
       "      <td>-0.079527</td>\n",
       "      <td>0.247370</td>\n",
       "      <td>-0.085136</td>\n",
       "      <td>0.182634</td>\n",
       "      <td>0.035095</td>\n",
       "      <td>0.083640</td>\n",
       "      <td>-1.741395</td>\n",
       "      <td>-0.311545</td>\n",
       "      <td>0.188462</td>\n",
       "      <td>0.093584</td>\n",
       "      <td>0.099274</td>\n",
       "      <td>-0.278172</td>\n",
       "      <td>0.229764</td>\n",
       "      <td>-0.033252</td>\n",
       "      <td>0.197171</td>\n",
       "      <td>0.289365</td>\n",
       "      <td>0.091514</td>\n",
       "      <td>0.213897</td>\n",
       "      <td>-0.529606</td>\n",
       "      <td>0.146549</td>\n",
       "      <td>-1.955689</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.095796</td>\n",
       "      <td>0.217502</td>\n",
       "      <td>-0.142379</td>\n",
       "      <td>0.105184</td>\n",
       "      <td>-0.063172</td>\n",
       "      <td>0.530033</td>\n",
       "      <td>0.308147</td>\n",
       "      <td>-0.083365</td>\n",
       "      <td>-0.033799</td>\n",
       "      <td>0.183865</td>\n",
       "      <td>-0.223949</td>\n",
       "      <td>0.799385</td>\n",
       "      <td>-0.474524</td>\n",
       "      <td>-0.460576</td>\n",
       "      <td>-0.044182</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>0.165013</td>\n",
       "      <td>-0.111722</td>\n",
       "      <td>-0.081998</td>\n",
       "      <td>-0.041621</td>\n",
       "      <td>-0.108558</td>\n",
       "      <td>-0.066339</td>\n",
       "      <td>0.317524</td>\n",
       "      <td>-0.246073</td>\n",
       "      <td>-0.091003</td>\n",
       "      <td>0.121568</td>\n",
       "      <td>-0.406422</td>\n",
       "      <td>-0.161607</td>\n",
       "      <td>-0.119543</td>\n",
       "      <td>-0.176654</td>\n",
       "      <td>0.280035</td>\n",
       "      <td>0.184037</td>\n",
       "      <td>-0.061895</td>\n",
       "      <td>-0.314775</td>\n",
       "      <td>0.222850</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.159540</td>\n",
       "      <td>0.030843</td>\n",
       "      <td>0.099132</td>\n",
       "      <td>-0.054322</td>\n",
       "      <td>-0.081392</td>\n",
       "      <td>0.141576</td>\n",
       "      <td>-0.072004</td>\n",
       "      <td>0.210885</td>\n",
       "      <td>0.036518</td>\n",
       "      <td>-0.140025</td>\n",
       "      <td>-0.128226</td>\n",
       "      <td>0.552561</td>\n",
       "      <td>-0.010165</td>\n",
       "      <td>-0.011358</td>\n",
       "      <td>0.064538</td>\n",
       "      <td>-0.065134</td>\n",
       "      <td>-0.181452</td>\n",
       "      <td>-0.825406</td>\n",
       "      <td>-0.065834</td>\n",
       "      <td>0.041021</td>\n",
       "      <td>0.121893</td>\n",
       "      <td>0.098652</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>-0.240898</td>\n",
       "      <td>-0.244179</td>\n",
       "      <td>-0.061744</td>\n",
       "      <td>0.032404</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.221089</td>\n",
       "      <td>-0.200351</td>\n",
       "      <td>-0.144002</td>\n",
       "      <td>0.239725</td>\n",
       "      <td>-1.297311</td>\n",
       "      <td>0.223271</td>\n",
       "      <td>0.056987</td>\n",
       "      <td>-0.247194</td>\n",
       "      <td>0.069370</td>\n",
       "      <td>0.114280</td>\n",
       "      <td>-0.231376</td>\n",
       "      <td>-0.022354</td>\n",
       "      <td>-0.242212</td>\n",
       "      <td>-0.198054</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.037717</td>\n",
       "      <td>0.052453</td>\n",
       "      <td>0.397130</td>\n",
       "      <td>-0.272806</td>\n",
       "      <td>0.030363</td>\n",
       "      <td>-0.268373</td>\n",
       "      <td>-1.167314</td>\n",
       "      <td>-0.001862</td>\n",
       "      <td>0.123383</td>\n",
       "      <td>-0.158216</td>\n",
       "      <td>-0.117367</td>\n",
       "      <td>-0.106028</td>\n",
       "      <td>-0.103492</td>\n",
       "      <td>0.276079</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>0.405745</td>\n",
       "      <td>-0.206124</td>\n",
       "      <td>-0.201381</td>\n",
       "      <td>-0.074994</td>\n",
       "      <td>0.138113</td>\n",
       "      <td>0.260481</td>\n",
       "      <td>-0.213984</td>\n",
       "      <td>-0.065476</td>\n",
       "      <td>-0.056164</td>\n",
       "      <td>-0.069920</td>\n",
       "      <td>-0.279557</td>\n",
       "      <td>-0.433864</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>-0.016963</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.374225</td>\n",
       "      <td>0.086910</td>\n",
       "      <td>-0.205872</td>\n",
       "      <td>0.600580</td>\n",
       "      <td>0.530726</td>\n",
       "      <td>0.013050</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>-0.286193</td>\n",
       "      <td>-0.201827</td>\n",
       "      <td>-0.007286</td>\n",
       "      <td>0.199121</td>\n",
       "      <td>-0.062046</td>\n",
       "      <td>0.122459</td>\n",
       "      <td>-0.410956</td>\n",
       "      <td>0.151578</td>\n",
       "      <td>-0.079604</td>\n",
       "      <td>-0.232702</td>\n",
       "      <td>-0.070239</td>\n",
       "      <td>0.372755</td>\n",
       "      <td>0.180651</td>\n",
       "      <td>-0.177912</td>\n",
       "      <td>0.058427</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.144243</td>\n",
       "      <td>0.815164</td>\n",
       "      <td>0.402201</td>\n",
       "      <td>0.494515</td>\n",
       "      <td>-0.190647</td>\n",
       "      <td>-0.484440</td>\n",
       "      <td>0.103818</td>\n",
       "      <td>0.052889</td>\n",
       "      <td>-0.270021</td>\n",
       "      <td>-0.292426</td>\n",
       "      <td>0.511552</td>\n",
       "      <td>-0.176861</td>\n",
       "      <td>-0.193064</td>\n",
       "      <td>-0.136917</td>\n",
       "      <td>-1.171707</td>\n",
       "      <td>0.065674</td>\n",
       "      <td>0.243738</td>\n",
       "      <td>-1.193957</td>\n",
       "      <td>-0.367779</td>\n",
       "      <td>-0.069849</td>\n",
       "      <td>-0.296484</td>\n",
       "      <td>-0.070715</td>\n",
       "      <td>0.070555</td>\n",
       "      <td>0.114988</td>\n",
       "      <td>-0.240360</td>\n",
       "      <td>0.080535</td>\n",
       "      <td>-0.130899</td>\n",
       "      <td>-0.147260</td>\n",
       "      <td>-0.184686</td>\n",
       "      <td>-0.068393</td>\n",
       "      <td>-0.130287</td>\n",
       "      <td>0.195133</td>\n",
       "      <td>-0.127566</td>\n",
       "      <td>-0.258508</td>\n",
       "      <td>0.226293</td>\n",
       "      <td>0.205090</td>\n",
       "      <td>-0.045750</td>\n",
       "      <td>-1.411300</td>\n",
       "      <td>-0.276614</td>\n",
       "      <td>0.645107</td>\n",
       "      <td>-0.078114</td>\n",
       "      <td>-0.113617</td>\n",
       "      <td>-0.114822</td>\n",
       "      <td>-0.164031</td>\n",
       "      <td>0.512193</td>\n",
       "      <td>-0.076346</td>\n",
       "      <td>-0.004242</td>\n",
       "      <td>0.162491</td>\n",
       "      <td>-0.335828</td>\n",
       "      <td>-0.061126</td>\n",
       "      <td>-0.096352</td>\n",
       "      <td>-0.617940</td>\n",
       "      <td>0.419832</td>\n",
       "      <td>0.518378</td>\n",
       "      <td>-0.039191</td>\n",
       "      <td>-0.483218</td>\n",
       "      <td>0.064265</td>\n",
       "      <td>-0.036647</td>\n",
       "      <td>-0.110322</td>\n",
       "      <td>-0.156566</td>\n",
       "      <td>0.008641</td>\n",
       "      <td>0.043329</td>\n",
       "      <td>-0.363142</td>\n",
       "      <td>0.092097</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>-1.108861</td>\n",
       "      <td>-1.579018</td>\n",
       "      <td>0.711617</td>\n",
       "      <td>-0.332547</td>\n",
       "      <td>0.187043</td>\n",
       "      <td>0.398282</td>\n",
       "      <td>1.573167</td>\n",
       "      <td>0.086047</td>\n",
       "      <td>0.259198</td>\n",
       "      <td>-0.014788</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>-0.018086</td>\n",
       "      <td>2.691875</td>\n",
       "      <td>0.424553</td>\n",
       "      <td>-0.285286</td>\n",
       "      <td>1.073353</td>\n",
       "      <td>0.205668</td>\n",
       "      <td>0.843496</td>\n",
       "      <td>-0.249535</td>\n",
       "      <td>-1.224445</td>\n",
       "      <td>-1.822779</td>\n",
       "      <td>-0.041405</td>\n",
       "      <td>-0.244548</td>\n",
       "      <td>-0.309810</td>\n",
       "      <td>0.044677</td>\n",
       "      <td>-0.175156</td>\n",
       "      <td>-0.225520</td>\n",
       "      <td>0.340738</td>\n",
       "      <td>-0.115321</td>\n",
       "      <td>-0.127952</td>\n",
       "      <td>0.143786</td>\n",
       "      <td>0.139659</td>\n",
       "      <td>-0.197647</td>\n",
       "      <td>-0.219472</td>\n",
       "      <td>-0.315886</td>\n",
       "      <td>0.042161</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>-0.005217</td>\n",
       "      <td>0.138704</td>\n",
       "      <td>-0.182541</td>\n",
       "      <td>0.039850</td>\n",
       "      <td>-0.334530</td>\n",
       "      <td>-1.765925</td>\n",
       "      <td>0.397904</td>\n",
       "      <td>-0.150699</td>\n",
       "      <td>0.172539</td>\n",
       "      <td>-0.140366</td>\n",
       "      <td>0.139684</td>\n",
       "      <td>-0.124165</td>\n",
       "      <td>-0.006417</td>\n",
       "      <td>-0.304291</td>\n",
       "      <td>0.260391</td>\n",
       "      <td>-0.036458</td>\n",
       "      <td>-0.016719</td>\n",
       "      <td>0.158128</td>\n",
       "      <td>-0.059998</td>\n",
       "      <td>-0.090386</td>\n",
       "      <td>0.286222</td>\n",
       "      <td>0.546196</td>\n",
       "      <td>-0.221657</td>\n",
       "      <td>1.450733</td>\n",
       "      <td>-0.241211</td>\n",
       "      <td>-0.110588</td>\n",
       "      <td>0.106254</td>\n",
       "      <td>0.213943</td>\n",
       "      <td>0.393615</td>\n",
       "      <td>-0.191916</td>\n",
       "      <td>-0.103155</td>\n",
       "      <td>-0.245200</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>-0.053682</td>\n",
       "      <td>-0.056607</td>\n",
       "      <td>-0.050480</td>\n",
       "      <td>-0.529721</td>\n",
       "      <td>0.206441</td>\n",
       "      <td>-0.173974</td>\n",
       "      <td>0.289319</td>\n",
       "      <td>0.364565</td>\n",
       "      <td>0.101875</td>\n",
       "      <td>0.491731</td>\n",
       "      <td>-0.868367</td>\n",
       "      <td>0.245347</td>\n",
       "      <td>-0.044390</td>\n",
       "      <td>-0.046577</td>\n",
       "      <td>-0.309987</td>\n",
       "      <td>-0.362868</td>\n",
       "      <td>-0.036630</td>\n",
       "      <td>0.331506</td>\n",
       "      <td>-0.020480</td>\n",
       "      <td>0.172062</td>\n",
       "      <td>-0.118994</td>\n",
       "      <td>-0.065471</td>\n",
       "      <td>-0.268840</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>-0.403372</td>\n",
       "      <td>-0.054016</td>\n",
       "      <td>0.106732</td>\n",
       "      <td>-0.108456</td>\n",
       "      <td>-0.322719</td>\n",
       "      <td>0.107584</td>\n",
       "      <td>1.112973</td>\n",
       "      <td>0.116413</td>\n",
       "      <td>0.068554</td>\n",
       "      <td>0.326928</td>\n",
       "      <td>0.072175</td>\n",
       "      <td>-0.067838</td>\n",
       "      <td>0.222880</td>\n",
       "      <td>0.370944</td>\n",
       "      <td>-0.051924</td>\n",
       "      <td>-0.155630</td>\n",
       "      <td>-0.059293</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>-0.181028</td>\n",
       "      <td>0.202403</td>\n",
       "      <td>-0.106007</td>\n",
       "      <td>-0.070647</td>\n",
       "      <td>-0.150441</td>\n",
       "      <td>0.189153</td>\n",
       "      <td>0.368452</td>\n",
       "      <td>0.173785</td>\n",
       "      <td>0.212450</td>\n",
       "      <td>-0.111362</td>\n",
       "      <td>0.119032</td>\n",
       "      <td>-0.173829</td>\n",
       "      <td>-0.008164</td>\n",
       "      <td>0.186362</td>\n",
       "      <td>0.149499</td>\n",
       "      <td>0.070120</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.248999</td>\n",
       "      <td>0.076153</td>\n",
       "      <td>-0.161569</td>\n",
       "      <td>-0.172550</td>\n",
       "      <td>-0.246436</td>\n",
       "      <td>0.362859</td>\n",
       "      <td>-0.167990</td>\n",
       "      <td>0.157734</td>\n",
       "      <td>0.148326</td>\n",
       "      <td>0.149125</td>\n",
       "      <td>-0.234781</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>-0.141593</td>\n",
       "      <td>-0.356290</td>\n",
       "      <td>-0.267505</td>\n",
       "      <td>0.196785</td>\n",
       "      <td>0.308629</td>\n",
       "      <td>-0.020930</td>\n",
       "      <td>0.292640</td>\n",
       "      <td>-0.231156</td>\n",
       "      <td>-0.032565</td>\n",
       "      <td>-0.210210</td>\n",
       "      <td>-0.163715</td>\n",
       "      <td>0.307969</td>\n",
       "      <td>-0.005841</td>\n",
       "      <td>-0.344377</td>\n",
       "      <td>-0.239263</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>0.267427</td>\n",
       "      <td>0.293433</td>\n",
       "      <td>-0.172587</td>\n",
       "      <td>0.042097</td>\n",
       "      <td>-0.185313</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>0.066347</td>\n",
       "      <td>-0.068619</td>\n",
       "      <td>0.033096</td>\n",
       "      <td>0.076562</td>\n",
       "      <td>0.384875</td>\n",
       "      <td>-0.055000</td>\n",
       "      <td>0.255780</td>\n",
       "      <td>-0.181817</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>0.151714</td>\n",
       "      <td>0.221216</td>\n",
       "      <td>-0.662532</td>\n",
       "      <td>0.304952</td>\n",
       "      <td>0.109765</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>-0.024596</td>\n",
       "      <td>0.188517</td>\n",
       "      <td>0.271330</td>\n",
       "      <td>-1.862894</td>\n",
       "      <td>0.215602</td>\n",
       "      <td>-0.254758</td>\n",
       "      <td>-0.077060</td>\n",
       "      <td>-0.153325</td>\n",
       "      <td>0.319688</td>\n",
       "      <td>0.880024</td>\n",
       "      <td>-0.153273</td>\n",
       "      <td>0.016548</td>\n",
       "      <td>-0.119517</td>\n",
       "      <td>0.037971</td>\n",
       "      <td>-0.178994</td>\n",
       "      <td>-0.146242</td>\n",
       "      <td>-0.012068</td>\n",
       "      <td>-0.522181</td>\n",
       "      <td>-0.205068</td>\n",
       "      <td>0.133247</td>\n",
       "      <td>0.100120</td>\n",
       "      <td>-0.156359</td>\n",
       "      <td>0.187195</td>\n",
       "      <td>-1.311999</td>\n",
       "      <td>-0.348073</td>\n",
       "      <td>0.045656</td>\n",
       "      <td>0.184526</td>\n",
       "      <td>-0.422370</td>\n",
       "      <td>0.141621</td>\n",
       "      <td>0.038451</td>\n",
       "      <td>-0.151638</td>\n",
       "      <td>-0.414896</td>\n",
       "      <td>0.235821</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>-0.072472</td>\n",
       "      <td>0.404454</td>\n",
       "      <td>-0.770902</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>0.185872</td>\n",
       "      <td>-0.447320</td>\n",
       "      <td>0.174181</td>\n",
       "      <td>-0.056535</td>\n",
       "      <td>0.186204</td>\n",
       "      <td>-0.369701</td>\n",
       "      <td>-0.289970</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>-0.112147</td>\n",
       "      <td>0.138150</td>\n",
       "      <td>0.231469</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.259129</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.047027</td>\n",
       "      <td>-1.368948</td>\n",
       "      <td>0.032279</td>\n",
       "      <td>-0.173145</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>-0.239498</td>\n",
       "      <td>-0.049627</td>\n",
       "      <td>0.105536</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.041485</td>\n",
       "      <td>0.133470</td>\n",
       "      <td>-0.408488</td>\n",
       "      <td>-0.442951</td>\n",
       "      <td>0.508196</td>\n",
       "      <td>0.066863</td>\n",
       "      <td>-0.092350</td>\n",
       "      <td>-0.116701</td>\n",
       "      <td>0.354941</td>\n",
       "      <td>0.053406</td>\n",
       "      <td>-0.021454</td>\n",
       "      <td>0.059903</td>\n",
       "      <td>0.178708</td>\n",
       "      <td>0.090702</td>\n",
       "      <td>0.420581</td>\n",
       "      <td>-0.192488</td>\n",
       "      <td>0.328129</td>\n",
       "      <td>0.514543</td>\n",
       "      <td>0.030183</td>\n",
       "      <td>0.288722</td>\n",
       "      <td>-0.018220</td>\n",
       "      <td>0.350887</td>\n",
       "      <td>0.449416</td>\n",
       "      <td>-0.645665</td>\n",
       "      <td>0.190850</td>\n",
       "      <td>-0.021356</td>\n",
       "      <td>0.035417</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>-0.039108</td>\n",
       "      <td>0.280523</td>\n",
       "      <td>0.059826</td>\n",
       "      <td>0.297179</td>\n",
       "      <td>-0.191275</td>\n",
       "      <td>0.553749</td>\n",
       "      <td>1.451620</td>\n",
       "      <td>-0.077626</td>\n",
       "      <td>0.159301</td>\n",
       "      <td>0.465388</td>\n",
       "      <td>-0.026384</td>\n",
       "      <td>-0.013242</td>\n",
       "      <td>-0.275869</td>\n",
       "      <td>-0.571373</td>\n",
       "      <td>1.513658</td>\n",
       "      <td>-0.235480</td>\n",
       "      <td>-0.240460</td>\n",
       "      <td>-0.165990</td>\n",
       "      <td>0.670881</td>\n",
       "      <td>0.133347</td>\n",
       "      <td>1.429063</td>\n",
       "      <td>-0.025801</td>\n",
       "      <td>0.221670</td>\n",
       "      <td>-0.127355</td>\n",
       "      <td>0.117763</td>\n",
       "      <td>0.378035</td>\n",
       "      <td>0.345593</td>\n",
       "      <td>-0.082650</td>\n",
       "      <td>-0.196601</td>\n",
       "      <td>0.065567</td>\n",
       "      <td>-1.515087</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.186859</td>\n",
       "      <td>0.153084</td>\n",
       "      <td>-0.084930</td>\n",
       "      <td>-0.197776</td>\n",
       "      <td>0.441878</td>\n",
       "      <td>-0.428810</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>-0.056936</td>\n",
       "      <td>-0.217087</td>\n",
       "      <td>-0.544300</td>\n",
       "      <td>0.316671</td>\n",
       "      <td>0.039623</td>\n",
       "      <td>0.168404</td>\n",
       "      <td>-0.297815</td>\n",
       "      <td>-0.111745</td>\n",
       "      <td>-0.261690</td>\n",
       "      <td>-0.002555</td>\n",
       "      <td>0.132198</td>\n",
       "      <td>0.130110</td>\n",
       "      <td>0.299142</td>\n",
       "      <td>-0.296408</td>\n",
       "      <td>0.137298</td>\n",
       "      <td>0.827499</td>\n",
       "      <td>-0.239577</td>\n",
       "      <td>0.291484</td>\n",
       "      <td>0.062325</td>\n",
       "      <td>-0.218971</td>\n",
       "      <td>-0.567088</td>\n",
       "      <td>-0.617004</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.163738</td>\n",
       "      <td>0.165489</td>\n",
       "      <td>0.253347</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>-0.174792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304795e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>58746.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.214187</td>\n",
       "      <td>-0.059479</td>\n",
       "      <td>-0.007084</td>\n",
       "      <td>0.156702</td>\n",
       "      <td>0.115002</td>\n",
       "      <td>-0.039318</td>\n",
       "      <td>0.072332</td>\n",
       "      <td>-0.113792</td>\n",
       "      <td>-0.055804</td>\n",
       "      <td>0.303812</td>\n",
       "      <td>0.233381</td>\n",
       "      <td>-0.173573</td>\n",
       "      <td>0.187994</td>\n",
       "      <td>0.064655</td>\n",
       "      <td>-0.705361</td>\n",
       "      <td>-0.066105</td>\n",
       "      <td>-0.065898</td>\n",
       "      <td>0.296897</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.156202</td>\n",
       "      <td>0.302877</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>-0.355658</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.135165</td>\n",
       "      <td>-0.862504</td>\n",
       "      <td>0.141235</td>\n",
       "      <td>0.377050</td>\n",
       "      <td>0.364685</td>\n",
       "      <td>0.203312</td>\n",
       "      <td>0.171481</td>\n",
       "      <td>0.050292</td>\n",
       "      <td>0.130933</td>\n",
       "      <td>0.172614</td>\n",
       "      <td>0.049645</td>\n",
       "      <td>0.036237</td>\n",
       "      <td>-2.015000</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>-0.291435</td>\n",
       "      <td>-0.239225</td>\n",
       "      <td>-0.206998</td>\n",
       "      <td>0.170334</td>\n",
       "      <td>0.081537</td>\n",
       "      <td>0.068786</td>\n",
       "      <td>0.120738</td>\n",
       "      <td>0.820182</td>\n",
       "      <td>0.099613</td>\n",
       "      <td>0.079094</td>\n",
       "      <td>1.232060</td>\n",
       "      <td>-0.346116</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>-0.756126</td>\n",
       "      <td>0.255950</td>\n",
       "      <td>-1.328524</td>\n",
       "      <td>-0.029311</td>\n",
       "      <td>0.234481</td>\n",
       "      <td>0.172572</td>\n",
       "      <td>-0.344263</td>\n",
       "      <td>-0.016448</td>\n",
       "      <td>0.227704</td>\n",
       "      <td>0.264607</td>\n",
       "      <td>-0.096200</td>\n",
       "      <td>-0.017468</td>\n",
       "      <td>0.385079</td>\n",
       "      <td>-0.298228</td>\n",
       "      <td>-0.081520</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.062978</td>\n",
       "      <td>0.097310</td>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.164003</td>\n",
       "      <td>0.022362</td>\n",
       "      <td>0.275796</td>\n",
       "      <td>0.147933</td>\n",
       "      <td>-0.152266</td>\n",
       "      <td>-0.098864</td>\n",
       "      <td>0.304742</td>\n",
       "      <td>0.381578</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>-0.331045</td>\n",
       "      <td>0.008643</td>\n",
       "      <td>-0.302817</td>\n",
       "      <td>0.153991</td>\n",
       "      <td>0.105696</td>\n",
       "      <td>-0.173794</td>\n",
       "      <td>-0.152133</td>\n",
       "      <td>-0.155555</td>\n",
       "      <td>0.223095</td>\n",
       "      <td>-0.129571</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>-0.238156</td>\n",
       "      <td>-0.491402</td>\n",
       "      <td>-0.063609</td>\n",
       "      <td>-0.260211</td>\n",
       "      <td>-1.019694</td>\n",
       "      <td>0.257401</td>\n",
       "      <td>0.186786</td>\n",
       "      <td>-0.098493</td>\n",
       "      <td>-0.133445</td>\n",
       "      <td>0.071476</td>\n",
       "      <td>1.586970</td>\n",
       "      <td>0.055472</td>\n",
       "      <td>-0.203007</td>\n",
       "      <td>-2.714346</td>\n",
       "      <td>0.397003</td>\n",
       "      <td>0.149403</td>\n",
       "      <td>-0.072598</td>\n",
       "      <td>0.540367</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.254004</td>\n",
       "      <td>-0.016248</td>\n",
       "      <td>0.316370</td>\n",
       "      <td>-0.043439</td>\n",
       "      <td>-0.147107</td>\n",
       "      <td>0.330427</td>\n",
       "      <td>-0.180701</td>\n",
       "      <td>-0.151760</td>\n",
       "      <td>-0.617787</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.134987</td>\n",
       "      <td>0.053385</td>\n",
       "      <td>0.351290</td>\n",
       "      <td>-0.511990</td>\n",
       "      <td>0.031337</td>\n",
       "      <td>-0.121426</td>\n",
       "      <td>0.255728</td>\n",
       "      <td>-0.380908</td>\n",
       "      <td>0.421364</td>\n",
       "      <td>1.234295</td>\n",
       "      <td>-0.259464</td>\n",
       "      <td>1.811132</td>\n",
       "      <td>-0.036996</td>\n",
       "      <td>-0.478139</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>1.468619</td>\n",
       "      <td>-0.062780</td>\n",
       "      <td>0.305993</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.054765</td>\n",
       "      <td>-0.337659</td>\n",
       "      <td>0.025861</td>\n",
       "      <td>-0.514213</td>\n",
       "      <td>-0.290076</td>\n",
       "      <td>0.110464</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>-0.181488</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>0.083280</td>\n",
       "      <td>-0.079189</td>\n",
       "      <td>-0.082426</td>\n",
       "      <td>-0.091485</td>\n",
       "      <td>0.030964</td>\n",
       "      <td>-0.308282</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>-0.040573</td>\n",
       "      <td>-0.154920</td>\n",
       "      <td>-0.157907</td>\n",
       "      <td>-0.211003</td>\n",
       "      <td>0.770553</td>\n",
       "      <td>1.654910</td>\n",
       "      <td>0.400755</td>\n",
       "      <td>0.154540</td>\n",
       "      <td>0.593819</td>\n",
       "      <td>0.139085</td>\n",
       "      <td>-0.146766</td>\n",
       "      <td>-0.154728</td>\n",
       "      <td>0.083942</td>\n",
       "      <td>-0.269372</td>\n",
       "      <td>-0.347635</td>\n",
       "      <td>-0.365141</td>\n",
       "      <td>-0.027031</td>\n",
       "      <td>0.200434</td>\n",
       "      <td>-0.114942</td>\n",
       "      <td>0.103986</td>\n",
       "      <td>-0.302938</td>\n",
       "      <td>0.030013</td>\n",
       "      <td>-0.085181</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>-0.286653</td>\n",
       "      <td>-0.088080</td>\n",
       "      <td>0.280806</td>\n",
       "      <td>-0.376989</td>\n",
       "      <td>-0.136734</td>\n",
       "      <td>0.308672</td>\n",
       "      <td>0.075215</td>\n",
       "      <td>-0.015112</td>\n",
       "      <td>-0.136485</td>\n",
       "      <td>-0.181825</td>\n",
       "      <td>-0.282070</td>\n",
       "      <td>0.250367</td>\n",
       "      <td>0.332802</td>\n",
       "      <td>0.209194</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>-0.210849</td>\n",
       "      <td>0.172185</td>\n",
       "      <td>-0.298031</td>\n",
       "      <td>0.192958</td>\n",
       "      <td>-0.340685</td>\n",
       "      <td>-0.035528</td>\n",
       "      <td>0.181731</td>\n",
       "      <td>0.222832</td>\n",
       "      <td>-0.029590</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.042110</td>\n",
       "      <td>0.605381</td>\n",
       "      <td>-0.018986</td>\n",
       "      <td>-0.333173</td>\n",
       "      <td>0.286789</td>\n",
       "      <td>0.317704</td>\n",
       "      <td>0.078848</td>\n",
       "      <td>-0.338971</td>\n",
       "      <td>1.144300</td>\n",
       "      <td>0.369384</td>\n",
       "      <td>0.138040</td>\n",
       "      <td>-0.113330</td>\n",
       "      <td>0.245751</td>\n",
       "      <td>-0.057461</td>\n",
       "      <td>-0.022409</td>\n",
       "      <td>-2.190071</td>\n",
       "      <td>-0.049383</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.150893</td>\n",
       "      <td>0.126102</td>\n",
       "      <td>-0.434847</td>\n",
       "      <td>0.061381</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.393739</td>\n",
       "      <td>0.395514</td>\n",
       "      <td>0.099296</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>-0.502763</td>\n",
       "      <td>0.206816</td>\n",
       "      <td>-2.172371</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.173373</td>\n",
       "      <td>-0.177359</td>\n",
       "      <td>-0.451319</td>\n",
       "      <td>-0.263518</td>\n",
       "      <td>0.385291</td>\n",
       "      <td>0.171382</td>\n",
       "      <td>0.210566</td>\n",
       "      <td>-0.061831</td>\n",
       "      <td>0.206926</td>\n",
       "      <td>0.073623</td>\n",
       "      <td>0.853944</td>\n",
       "      <td>-0.539201</td>\n",
       "      <td>-0.004527</td>\n",
       "      <td>-0.271299</td>\n",
       "      <td>0.224307</td>\n",
       "      <td>-0.079206</td>\n",
       "      <td>0.089325</td>\n",
       "      <td>0.013823</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>-0.150260</td>\n",
       "      <td>-0.087359</td>\n",
       "      <td>0.130078</td>\n",
       "      <td>-0.315774</td>\n",
       "      <td>-0.059658</td>\n",
       "      <td>0.041780</td>\n",
       "      <td>-0.340865</td>\n",
       "      <td>-0.109432</td>\n",
       "      <td>-0.086527</td>\n",
       "      <td>-0.210897</td>\n",
       "      <td>0.211090</td>\n",
       "      <td>0.387554</td>\n",
       "      <td>-0.283232</td>\n",
       "      <td>-0.101025</td>\n",
       "      <td>-0.014107</td>\n",
       "      <td>0.160044</td>\n",
       "      <td>0.345910</td>\n",
       "      <td>-0.113012</td>\n",
       "      <td>-0.127737</td>\n",
       "      <td>-0.130198</td>\n",
       "      <td>0.292036</td>\n",
       "      <td>0.035848</td>\n",
       "      <td>-0.034419</td>\n",
       "      <td>0.181228</td>\n",
       "      <td>0.147207</td>\n",
       "      <td>-0.213254</td>\n",
       "      <td>-0.032855</td>\n",
       "      <td>0.292569</td>\n",
       "      <td>0.080702</td>\n",
       "      <td>0.135831</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>0.160318</td>\n",
       "      <td>-0.509982</td>\n",
       "      <td>-1.067933</td>\n",
       "      <td>0.129897</td>\n",
       "      <td>0.099443</td>\n",
       "      <td>-0.038021</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>-0.056278</td>\n",
       "      <td>-0.280135</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>-0.169074</td>\n",
       "      <td>-0.045351</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>0.078974</td>\n",
       "      <td>-0.277739</td>\n",
       "      <td>-0.221999</td>\n",
       "      <td>-0.147781</td>\n",
       "      <td>-1.526245</td>\n",
       "      <td>0.312276</td>\n",
       "      <td>-0.169789</td>\n",
       "      <td>-0.189879</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.200151</td>\n",
       "      <td>-0.115968</td>\n",
       "      <td>-0.203248</td>\n",
       "      <td>-0.270180</td>\n",
       "      <td>-0.311013</td>\n",
       "      <td>-0.412499</td>\n",
       "      <td>0.096283</td>\n",
       "      <td>-0.127346</td>\n",
       "      <td>0.416598</td>\n",
       "      <td>-0.116475</td>\n",
       "      <td>-0.051191</td>\n",
       "      <td>0.120350</td>\n",
       "      <td>-1.671294</td>\n",
       "      <td>0.036233</td>\n",
       "      <td>-0.142249</td>\n",
       "      <td>-0.089807</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>-0.138569</td>\n",
       "      <td>0.169761</td>\n",
       "      <td>-0.073503</td>\n",
       "      <td>0.378245</td>\n",
       "      <td>0.177246</td>\n",
       "      <td>-0.298494</td>\n",
       "      <td>-0.242366</td>\n",
       "      <td>-0.096903</td>\n",
       "      <td>0.027831</td>\n",
       "      <td>0.310804</td>\n",
       "      <td>-0.412920</td>\n",
       "      <td>-0.067518</td>\n",
       "      <td>-0.333639</td>\n",
       "      <td>-0.180675</td>\n",
       "      <td>-0.279922</td>\n",
       "      <td>-0.050898</td>\n",
       "      <td>-0.012616</td>\n",
       "      <td>-0.119002</td>\n",
       "      <td>0.268140</td>\n",
       "      <td>0.381062</td>\n",
       "      <td>-0.089646</td>\n",
       "      <td>-0.068158</td>\n",
       "      <td>0.166571</td>\n",
       "      <td>0.504804</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>0.491953</td>\n",
       "      <td>-0.429315</td>\n",
       "      <td>-0.196230</td>\n",
       "      <td>0.203333</td>\n",
       "      <td>-0.014058</td>\n",
       "      <td>-0.229958</td>\n",
       "      <td>0.112453</td>\n",
       "      <td>-0.315014</td>\n",
       "      <td>0.347439</td>\n",
       "      <td>-0.408025</td>\n",
       "      <td>-0.287301</td>\n",
       "      <td>0.082728</td>\n",
       "      <td>-0.002378</td>\n",
       "      <td>0.314346</td>\n",
       "      <td>-0.026424</td>\n",
       "      <td>0.083048</td>\n",
       "      <td>-0.164114</td>\n",
       "      <td>-0.099241</td>\n",
       "      <td>0.433942</td>\n",
       "      <td>0.312877</td>\n",
       "      <td>0.454024</td>\n",
       "      <td>-0.196333</td>\n",
       "      <td>-0.688390</td>\n",
       "      <td>-0.200323</td>\n",
       "      <td>-0.058434</td>\n",
       "      <td>-0.109639</td>\n",
       "      <td>-0.280405</td>\n",
       "      <td>0.487777</td>\n",
       "      <td>-0.132122</td>\n",
       "      <td>0.043661</td>\n",
       "      <td>-0.076397</td>\n",
       "      <td>-1.149711</td>\n",
       "      <td>0.526460</td>\n",
       "      <td>0.343722</td>\n",
       "      <td>-1.818758</td>\n",
       "      <td>-0.143835</td>\n",
       "      <td>0.096751</td>\n",
       "      <td>-0.412193</td>\n",
       "      <td>-0.056452</td>\n",
       "      <td>-0.085726</td>\n",
       "      <td>0.049318</td>\n",
       "      <td>0.047936</td>\n",
       "      <td>0.252898</td>\n",
       "      <td>-0.139994</td>\n",
       "      <td>0.046172</td>\n",
       "      <td>-0.232740</td>\n",
       "      <td>-0.090420</td>\n",
       "      <td>0.123246</td>\n",
       "      <td>0.018235</td>\n",
       "      <td>0.065993</td>\n",
       "      <td>0.041021</td>\n",
       "      <td>0.071642</td>\n",
       "      <td>0.154944</td>\n",
       "      <td>-0.145971</td>\n",
       "      <td>-1.589564</td>\n",
       "      <td>0.032751</td>\n",
       "      <td>0.420787</td>\n",
       "      <td>-0.105569</td>\n",
       "      <td>0.107190</td>\n",
       "      <td>-0.085670</td>\n",
       "      <td>-0.116576</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>-0.219717</td>\n",
       "      <td>-0.041125</td>\n",
       "      <td>0.119072</td>\n",
       "      <td>-0.539758</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.627740</td>\n",
       "      <td>0.041640</td>\n",
       "      <td>0.679750</td>\n",
       "      <td>-0.330866</td>\n",
       "      <td>-0.303605</td>\n",
       "      <td>0.164967</td>\n",
       "      <td>-0.082978</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>-0.290759</td>\n",
       "      <td>0.186624</td>\n",
       "      <td>0.193874</td>\n",
       "      <td>-0.217141</td>\n",
       "      <td>0.517328</td>\n",
       "      <td>-0.162559</td>\n",
       "      <td>-1.340829</td>\n",
       "      <td>-1.992660</td>\n",
       "      <td>1.062324</td>\n",
       "      <td>-0.206364</td>\n",
       "      <td>-0.166329</td>\n",
       "      <td>0.403310</td>\n",
       "      <td>1.990638</td>\n",
       "      <td>0.146052</td>\n",
       "      <td>0.068864</td>\n",
       "      <td>0.179619</td>\n",
       "      <td>0.200503</td>\n",
       "      <td>0.149777</td>\n",
       "      <td>3.317294</td>\n",
       "      <td>0.361158</td>\n",
       "      <td>-0.146686</td>\n",
       "      <td>1.285694</td>\n",
       "      <td>0.151069</td>\n",
       "      <td>0.448719</td>\n",
       "      <td>0.035179</td>\n",
       "      <td>-1.420188</td>\n",
       "      <td>-2.108225</td>\n",
       "      <td>-0.060243</td>\n",
       "      <td>-0.113239</td>\n",
       "      <td>0.062463</td>\n",
       "      <td>0.039317</td>\n",
       "      <td>-0.192538</td>\n",
       "      <td>-0.126141</td>\n",
       "      <td>0.322284</td>\n",
       "      <td>0.013035</td>\n",
       "      <td>0.045056</td>\n",
       "      <td>0.345787</td>\n",
       "      <td>-0.001926</td>\n",
       "      <td>0.129155</td>\n",
       "      <td>-0.331641</td>\n",
       "      <td>-0.260734</td>\n",
       "      <td>0.036931</td>\n",
       "      <td>0.335010</td>\n",
       "      <td>0.285514</td>\n",
       "      <td>-0.139525</td>\n",
       "      <td>-0.141440</td>\n",
       "      <td>-0.053293</td>\n",
       "      <td>0.042684</td>\n",
       "      <td>-1.738688</td>\n",
       "      <td>-0.008258</td>\n",
       "      <td>-0.119134</td>\n",
       "      <td>0.193801</td>\n",
       "      <td>0.171490</td>\n",
       "      <td>0.015136</td>\n",
       "      <td>-0.108404</td>\n",
       "      <td>0.037943</td>\n",
       "      <td>-0.341501</td>\n",
       "      <td>0.152690</td>\n",
       "      <td>0.078154</td>\n",
       "      <td>-0.129326</td>\n",
       "      <td>0.351366</td>\n",
       "      <td>-0.150211</td>\n",
       "      <td>-0.029313</td>\n",
       "      <td>-0.023901</td>\n",
       "      <td>0.395660</td>\n",
       "      <td>-0.068190</td>\n",
       "      <td>2.189972</td>\n",
       "      <td>-0.343455</td>\n",
       "      <td>-0.223054</td>\n",
       "      <td>0.142225</td>\n",
       "      <td>0.112101</td>\n",
       "      <td>0.639055</td>\n",
       "      <td>-0.119655</td>\n",
       "      <td>-0.039461</td>\n",
       "      <td>-0.200234</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.182408</td>\n",
       "      <td>0.077666</td>\n",
       "      <td>-0.123274</td>\n",
       "      <td>-0.560783</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.046167</td>\n",
       "      <td>0.082704</td>\n",
       "      <td>0.527687</td>\n",
       "      <td>0.066244</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>-0.940851</td>\n",
       "      <td>0.176149</td>\n",
       "      <td>-0.073755</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>-0.389462</td>\n",
       "      <td>-0.271160</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>-0.236872</td>\n",
       "      <td>0.039546</td>\n",
       "      <td>0.279473</td>\n",
       "      <td>-0.174726</td>\n",
       "      <td>-0.192419</td>\n",
       "      <td>-0.370484</td>\n",
       "      <td>-0.145340</td>\n",
       "      <td>-0.363148</td>\n",
       "      <td>-0.153669</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>-0.100332</td>\n",
       "      <td>-0.411319</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>1.377432</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>-0.108455</td>\n",
       "      <td>0.831785</td>\n",
       "      <td>0.314014</td>\n",
       "      <td>-0.263285</td>\n",
       "      <td>0.481155</td>\n",
       "      <td>0.017858</td>\n",
       "      <td>-0.090655</td>\n",
       "      <td>-0.293449</td>\n",
       "      <td>-0.133164</td>\n",
       "      <td>-0.056068</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.432246</td>\n",
       "      <td>-0.057671</td>\n",
       "      <td>0.133675</td>\n",
       "      <td>0.281067</td>\n",
       "      <td>0.110005</td>\n",
       "      <td>0.270878</td>\n",
       "      <td>0.236417</td>\n",
       "      <td>-0.081640</td>\n",
       "      <td>-0.153988</td>\n",
       "      <td>-0.360544</td>\n",
       "      <td>-0.014357</td>\n",
       "      <td>0.054174</td>\n",
       "      <td>0.232930</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.234501</td>\n",
       "      <td>0.043375</td>\n",
       "      <td>0.092088</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>-0.016296</td>\n",
       "      <td>-0.178823</td>\n",
       "      <td>-0.168576</td>\n",
       "      <td>0.440691</td>\n",
       "      <td>0.261947</td>\n",
       "      <td>0.094987</td>\n",
       "      <td>0.100061</td>\n",
       "      <td>0.295376</td>\n",
       "      <td>-0.027401</td>\n",
       "      <td>-0.145291</td>\n",
       "      <td>-0.203151</td>\n",
       "      <td>-0.397400</td>\n",
       "      <td>-0.364736</td>\n",
       "      <td>0.358719</td>\n",
       "      <td>0.294185</td>\n",
       "      <td>-0.078618</td>\n",
       "      <td>0.301834</td>\n",
       "      <td>-0.358973</td>\n",
       "      <td>-0.003805</td>\n",
       "      <td>-0.099479</td>\n",
       "      <td>-0.415659</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.144251</td>\n",
       "      <td>-0.282019</td>\n",
       "      <td>-0.171681</td>\n",
       "      <td>-0.050986</td>\n",
       "      <td>-0.038448</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>-0.412630</td>\n",
       "      <td>-0.085567</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.352698</td>\n",
       "      <td>0.323416</td>\n",
       "      <td>-0.052376</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>-0.251748</td>\n",
       "      <td>0.696412</td>\n",
       "      <td>0.115743</td>\n",
       "      <td>0.291411</td>\n",
       "      <td>-0.171567</td>\n",
       "      <td>0.122354</td>\n",
       "      <td>-0.002959</td>\n",
       "      <td>0.080790</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>-0.503203</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>-0.199504</td>\n",
       "      <td>0.117559</td>\n",
       "      <td>-0.108211</td>\n",
       "      <td>0.475217</td>\n",
       "      <td>0.164117</td>\n",
       "      <td>-2.055612</td>\n",
       "      <td>-0.021256</td>\n",
       "      <td>-0.218793</td>\n",
       "      <td>-0.032524</td>\n",
       "      <td>-0.242874</td>\n",
       "      <td>0.127543</td>\n",
       "      <td>0.775446</td>\n",
       "      <td>-0.093631</td>\n",
       "      <td>-0.125298</td>\n",
       "      <td>-0.050492</td>\n",
       "      <td>0.036180</td>\n",
       "      <td>-0.066783</td>\n",
       "      <td>-0.062866</td>\n",
       "      <td>-0.018110</td>\n",
       "      <td>-0.731210</td>\n",
       "      <td>0.102162</td>\n",
       "      <td>0.135387</td>\n",
       "      <td>0.188749</td>\n",
       "      <td>-0.159923</td>\n",
       "      <td>-0.025772</td>\n",
       "      <td>-1.764593</td>\n",
       "      <td>-0.488105</td>\n",
       "      <td>-0.078763</td>\n",
       "      <td>-0.199315</td>\n",
       "      <td>-0.439530</td>\n",
       "      <td>0.041473</td>\n",
       "      <td>-0.126680</td>\n",
       "      <td>-0.145141</td>\n",
       "      <td>-0.341191</td>\n",
       "      <td>0.401582</td>\n",
       "      <td>0.100638</td>\n",
       "      <td>-0.227045</td>\n",
       "      <td>0.183787</td>\n",
       "      <td>-0.330814</td>\n",
       "      <td>0.247868</td>\n",
       "      <td>-0.079385</td>\n",
       "      <td>-0.120809</td>\n",
       "      <td>0.059129</td>\n",
       "      <td>-0.020011</td>\n",
       "      <td>0.309639</td>\n",
       "      <td>-0.034716</td>\n",
       "      <td>-0.144897</td>\n",
       "      <td>-0.094473</td>\n",
       "      <td>0.058416</td>\n",
       "      <td>-0.020123</td>\n",
       "      <td>0.252113</td>\n",
       "      <td>0.183506</td>\n",
       "      <td>0.047585</td>\n",
       "      <td>-0.025251</td>\n",
       "      <td>-0.002996</td>\n",
       "      <td>-1.495259</td>\n",
       "      <td>0.169124</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.485362</td>\n",
       "      <td>-0.048588</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>-0.012018</td>\n",
       "      <td>-0.043289</td>\n",
       "      <td>-0.300735</td>\n",
       "      <td>0.224916</td>\n",
       "      <td>-0.472905</td>\n",
       "      <td>0.105496</td>\n",
       "      <td>0.355393</td>\n",
       "      <td>0.239578</td>\n",
       "      <td>0.123667</td>\n",
       "      <td>-0.243022</td>\n",
       "      <td>0.318586</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>-0.175605</td>\n",
       "      <td>-0.406291</td>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.103295</td>\n",
       "      <td>0.720933</td>\n",
       "      <td>-0.547663</td>\n",
       "      <td>0.106099</td>\n",
       "      <td>0.472680</td>\n",
       "      <td>0.160865</td>\n",
       "      <td>0.049538</td>\n",
       "      <td>-0.133090</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.201352</td>\n",
       "      <td>-0.889775</td>\n",
       "      <td>0.327003</td>\n",
       "      <td>0.033826</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>0.173388</td>\n",
       "      <td>-0.118219</td>\n",
       "      <td>0.342384</td>\n",
       "      <td>0.127484</td>\n",
       "      <td>0.484244</td>\n",
       "      <td>-0.219989</td>\n",
       "      <td>0.624424</td>\n",
       "      <td>1.821531</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>-0.006789</td>\n",
       "      <td>0.227202</td>\n",
       "      <td>-0.120915</td>\n",
       "      <td>-0.423679</td>\n",
       "      <td>-0.626222</td>\n",
       "      <td>1.776622</td>\n",
       "      <td>-0.138687</td>\n",
       "      <td>-0.143847</td>\n",
       "      <td>-0.128546</td>\n",
       "      <td>0.542090</td>\n",
       "      <td>0.064392</td>\n",
       "      <td>1.814694</td>\n",
       "      <td>-0.222461</td>\n",
       "      <td>0.185527</td>\n",
       "      <td>0.147876</td>\n",
       "      <td>-0.015208</td>\n",
       "      <td>0.267573</td>\n",
       "      <td>0.289887</td>\n",
       "      <td>0.062728</td>\n",
       "      <td>-0.283036</td>\n",
       "      <td>-0.168815</td>\n",
       "      <td>-0.939124</td>\n",
       "      <td>-0.136199</td>\n",
       "      <td>0.272463</td>\n",
       "      <td>0.414165</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>-0.085339</td>\n",
       "      <td>0.256143</td>\n",
       "      <td>-0.101246</td>\n",
       "      <td>-0.149911</td>\n",
       "      <td>-0.089261</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.333065</td>\n",
       "      <td>0.329606</td>\n",
       "      <td>0.159369</td>\n",
       "      <td>0.105106</td>\n",
       "      <td>0.136789</td>\n",
       "      <td>-0.185530</td>\n",
       "      <td>-0.109052</td>\n",
       "      <td>0.068948</td>\n",
       "      <td>0.058279</td>\n",
       "      <td>-0.078391</td>\n",
       "      <td>0.644802</td>\n",
       "      <td>-0.419815</td>\n",
       "      <td>-0.091957</td>\n",
       "      <td>0.794983</td>\n",
       "      <td>-0.016139</td>\n",
       "      <td>-0.033586</td>\n",
       "      <td>0.333985</td>\n",
       "      <td>-0.193174</td>\n",
       "      <td>-0.613174</td>\n",
       "      <td>-0.672665</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.253517</td>\n",
       "      <td>0.135646</td>\n",
       "      <td>0.110566</td>\n",
       "      <td>0.269420</td>\n",
       "      <td>-0.121226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304794e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>58746.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.078277</td>\n",
       "      <td>-0.174203</td>\n",
       "      <td>0.049625</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>-0.127791</td>\n",
       "      <td>0.065504</td>\n",
       "      <td>-0.210719</td>\n",
       "      <td>-0.321700</td>\n",
       "      <td>0.204327</td>\n",
       "      <td>0.263463</td>\n",
       "      <td>0.196186</td>\n",
       "      <td>-0.032833</td>\n",
       "      <td>0.318963</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>-0.564071</td>\n",
       "      <td>-0.042986</td>\n",
       "      <td>-0.122809</td>\n",
       "      <td>-0.031171</td>\n",
       "      <td>-0.148130</td>\n",
       "      <td>0.041004</td>\n",
       "      <td>0.186105</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>-0.457684</td>\n",
       "      <td>-0.047066</td>\n",
       "      <td>-0.370980</td>\n",
       "      <td>-0.614162</td>\n",
       "      <td>0.025137</td>\n",
       "      <td>0.274583</td>\n",
       "      <td>0.308254</td>\n",
       "      <td>-0.036962</td>\n",
       "      <td>0.128284</td>\n",
       "      <td>0.137059</td>\n",
       "      <td>-0.012715</td>\n",
       "      <td>0.381060</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>-0.054038</td>\n",
       "      <td>-2.011146</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>-0.234859</td>\n",
       "      <td>-0.290969</td>\n",
       "      <td>-0.254029</td>\n",
       "      <td>0.296830</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.040856</td>\n",
       "      <td>0.178252</td>\n",
       "      <td>0.917320</td>\n",
       "      <td>0.318727</td>\n",
       "      <td>0.032389</td>\n",
       "      <td>0.915136</td>\n",
       "      <td>-0.732133</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>-0.653140</td>\n",
       "      <td>0.105605</td>\n",
       "      <td>-1.290869</td>\n",
       "      <td>-0.091505</td>\n",
       "      <td>0.343258</td>\n",
       "      <td>0.421036</td>\n",
       "      <td>-0.372141</td>\n",
       "      <td>0.148036</td>\n",
       "      <td>0.089838</td>\n",
       "      <td>0.069913</td>\n",
       "      <td>0.023110</td>\n",
       "      <td>-0.002952</td>\n",
       "      <td>0.285805</td>\n",
       "      <td>-0.148635</td>\n",
       "      <td>-0.107743</td>\n",
       "      <td>-0.060025</td>\n",
       "      <td>0.204169</td>\n",
       "      <td>0.198132</td>\n",
       "      <td>0.041243</td>\n",
       "      <td>0.157347</td>\n",
       "      <td>-0.040833</td>\n",
       "      <td>0.411137</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>-0.010356</td>\n",
       "      <td>-0.230977</td>\n",
       "      <td>0.175235</td>\n",
       "      <td>0.166121</td>\n",
       "      <td>-0.196147</td>\n",
       "      <td>-0.133367</td>\n",
       "      <td>-0.125821</td>\n",
       "      <td>0.258623</td>\n",
       "      <td>-0.216341</td>\n",
       "      <td>0.198041</td>\n",
       "      <td>0.280206</td>\n",
       "      <td>-0.185696</td>\n",
       "      <td>-0.228802</td>\n",
       "      <td>-0.097395</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>-0.275382</td>\n",
       "      <td>0.026012</td>\n",
       "      <td>-0.146869</td>\n",
       "      <td>-0.312870</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>-0.291111</td>\n",
       "      <td>-0.629528</td>\n",
       "      <td>0.250742</td>\n",
       "      <td>0.102231</td>\n",
       "      <td>0.132436</td>\n",
       "      <td>-0.023161</td>\n",
       "      <td>-0.187149</td>\n",
       "      <td>1.473456</td>\n",
       "      <td>0.146264</td>\n",
       "      <td>-0.237220</td>\n",
       "      <td>-2.635231</td>\n",
       "      <td>0.429763</td>\n",
       "      <td>0.069974</td>\n",
       "      <td>0.068536</td>\n",
       "      <td>0.433039</td>\n",
       "      <td>-0.025383</td>\n",
       "      <td>0.415848</td>\n",
       "      <td>0.077679</td>\n",
       "      <td>0.292029</td>\n",
       "      <td>0.195455</td>\n",
       "      <td>-0.194162</td>\n",
       "      <td>0.187316</td>\n",
       "      <td>-0.138606</td>\n",
       "      <td>-0.214609</td>\n",
       "      <td>-0.618403</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>-0.119306</td>\n",
       "      <td>0.201350</td>\n",
       "      <td>0.337874</td>\n",
       "      <td>-0.436353</td>\n",
       "      <td>0.100118</td>\n",
       "      <td>-0.090218</td>\n",
       "      <td>0.098789</td>\n",
       "      <td>-0.335698</td>\n",
       "      <td>0.298973</td>\n",
       "      <td>1.348853</td>\n",
       "      <td>0.151304</td>\n",
       "      <td>1.808684</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>-0.473900</td>\n",
       "      <td>0.047641</td>\n",
       "      <td>1.355007</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.082156</td>\n",
       "      <td>0.339395</td>\n",
       "      <td>-0.223712</td>\n",
       "      <td>-0.065106</td>\n",
       "      <td>-0.320821</td>\n",
       "      <td>-0.012630</td>\n",
       "      <td>-0.040721</td>\n",
       "      <td>0.178549</td>\n",
       "      <td>-0.358616</td>\n",
       "      <td>-0.205188</td>\n",
       "      <td>0.299488</td>\n",
       "      <td>0.138494</td>\n",
       "      <td>-0.214512</td>\n",
       "      <td>-0.211112</td>\n",
       "      <td>0.261672</td>\n",
       "      <td>-0.285922</td>\n",
       "      <td>-0.156016</td>\n",
       "      <td>-0.159515</td>\n",
       "      <td>-0.179229</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>-0.184307</td>\n",
       "      <td>0.786049</td>\n",
       "      <td>1.625420</td>\n",
       "      <td>0.219464</td>\n",
       "      <td>-0.166830</td>\n",
       "      <td>0.725683</td>\n",
       "      <td>0.241789</td>\n",
       "      <td>-0.205717</td>\n",
       "      <td>-0.072434</td>\n",
       "      <td>-0.044138</td>\n",
       "      <td>-0.185978</td>\n",
       "      <td>-0.420110</td>\n",
       "      <td>-0.256598</td>\n",
       "      <td>0.009339</td>\n",
       "      <td>-0.057659</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>0.198539</td>\n",
       "      <td>-0.135660</td>\n",
       "      <td>-0.225502</td>\n",
       "      <td>0.020885</td>\n",
       "      <td>-0.012674</td>\n",
       "      <td>-0.310646</td>\n",
       "      <td>0.010176</td>\n",
       "      <td>0.406861</td>\n",
       "      <td>-0.277630</td>\n",
       "      <td>-0.202905</td>\n",
       "      <td>0.314422</td>\n",
       "      <td>-0.009326</td>\n",
       "      <td>-0.090112</td>\n",
       "      <td>-0.227132</td>\n",
       "      <td>-0.126610</td>\n",
       "      <td>-0.190479</td>\n",
       "      <td>0.254401</td>\n",
       "      <td>0.264294</td>\n",
       "      <td>-0.126774</td>\n",
       "      <td>0.060016</td>\n",
       "      <td>-0.097042</td>\n",
       "      <td>0.145901</td>\n",
       "      <td>-0.048483</td>\n",
       "      <td>0.078163</td>\n",
       "      <td>-0.449819</td>\n",
       "      <td>0.084382</td>\n",
       "      <td>-0.132889</td>\n",
       "      <td>0.171121</td>\n",
       "      <td>-0.189504</td>\n",
       "      <td>-0.342030</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>0.778550</td>\n",
       "      <td>-0.143175</td>\n",
       "      <td>-0.417559</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.288917</td>\n",
       "      <td>0.084393</td>\n",
       "      <td>-0.375040</td>\n",
       "      <td>1.064945</td>\n",
       "      <td>0.281548</td>\n",
       "      <td>0.037414</td>\n",
       "      <td>-0.001120</td>\n",
       "      <td>0.269933</td>\n",
       "      <td>-0.172180</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>-1.919863</td>\n",
       "      <td>-0.093487</td>\n",
       "      <td>0.317770</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>0.195610</td>\n",
       "      <td>-0.383640</td>\n",
       "      <td>0.163156</td>\n",
       "      <td>-0.090594</td>\n",
       "      <td>0.325676</td>\n",
       "      <td>0.299610</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.350221</td>\n",
       "      <td>-0.312806</td>\n",
       "      <td>-0.068715</td>\n",
       "      <td>-2.059254</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.073927</td>\n",
       "      <td>0.282447</td>\n",
       "      <td>-0.125010</td>\n",
       "      <td>-0.514405</td>\n",
       "      <td>-0.141305</td>\n",
       "      <td>0.439810</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.206061</td>\n",
       "      <td>-0.072718</td>\n",
       "      <td>0.526341</td>\n",
       "      <td>0.186767</td>\n",
       "      <td>0.814527</td>\n",
       "      <td>-0.464755</td>\n",
       "      <td>-0.052261</td>\n",
       "      <td>-0.460785</td>\n",
       "      <td>0.233475</td>\n",
       "      <td>-0.151032</td>\n",
       "      <td>0.309687</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>-0.167510</td>\n",
       "      <td>0.474102</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>-0.236642</td>\n",
       "      <td>0.277459</td>\n",
       "      <td>-0.172376</td>\n",
       "      <td>-0.276700</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>-0.437688</td>\n",
       "      <td>-0.172888</td>\n",
       "      <td>-0.124094</td>\n",
       "      <td>-0.282119</td>\n",
       "      <td>0.371277</td>\n",
       "      <td>0.472138</td>\n",
       "      <td>-0.106011</td>\n",
       "      <td>-0.176401</td>\n",
       "      <td>0.054863</td>\n",
       "      <td>0.197259</td>\n",
       "      <td>0.284405</td>\n",
       "      <td>0.087026</td>\n",
       "      <td>-0.368444</td>\n",
       "      <td>-0.119897</td>\n",
       "      <td>0.160622</td>\n",
       "      <td>0.292768</td>\n",
       "      <td>-0.287437</td>\n",
       "      <td>0.155879</td>\n",
       "      <td>0.039946</td>\n",
       "      <td>-0.177790</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.033765</td>\n",
       "      <td>0.160491</td>\n",
       "      <td>0.292537</td>\n",
       "      <td>0.099510</td>\n",
       "      <td>-0.262996</td>\n",
       "      <td>-0.936150</td>\n",
       "      <td>0.222801</td>\n",
       "      <td>0.085874</td>\n",
       "      <td>0.104765</td>\n",
       "      <td>0.151843</td>\n",
       "      <td>-0.250792</td>\n",
       "      <td>-0.209840</td>\n",
       "      <td>0.215738</td>\n",
       "      <td>-0.144433</td>\n",
       "      <td>-0.177102</td>\n",
       "      <td>0.232946</td>\n",
       "      <td>-0.150948</td>\n",
       "      <td>-0.259506</td>\n",
       "      <td>-0.276193</td>\n",
       "      <td>-0.196652</td>\n",
       "      <td>-1.397798</td>\n",
       "      <td>0.262344</td>\n",
       "      <td>-0.093686</td>\n",
       "      <td>-0.204031</td>\n",
       "      <td>0.146204</td>\n",
       "      <td>0.173420</td>\n",
       "      <td>-0.188087</td>\n",
       "      <td>-0.165837</td>\n",
       "      <td>-0.244575</td>\n",
       "      <td>-0.320595</td>\n",
       "      <td>-0.490913</td>\n",
       "      <td>-0.033065</td>\n",
       "      <td>-0.049807</td>\n",
       "      <td>0.383614</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>-0.110179</td>\n",
       "      <td>0.132222</td>\n",
       "      <td>-1.425190</td>\n",
       "      <td>-0.043583</td>\n",
       "      <td>-0.298038</td>\n",
       "      <td>0.031573</td>\n",
       "      <td>-0.009196</td>\n",
       "      <td>0.082097</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.150740</td>\n",
       "      <td>0.254168</td>\n",
       "      <td>-0.015646</td>\n",
       "      <td>0.156347</td>\n",
       "      <td>-0.078445</td>\n",
       "      <td>-0.230638</td>\n",
       "      <td>0.139303</td>\n",
       "      <td>0.458750</td>\n",
       "      <td>-0.332444</td>\n",
       "      <td>0.031446</td>\n",
       "      <td>-0.080375</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>-0.176316</td>\n",
       "      <td>-0.120925</td>\n",
       "      <td>0.091290</td>\n",
       "      <td>-0.162544</td>\n",
       "      <td>0.021520</td>\n",
       "      <td>0.134609</td>\n",
       "      <td>-0.076166</td>\n",
       "      <td>-0.051236</td>\n",
       "      <td>0.158702</td>\n",
       "      <td>0.418449</td>\n",
       "      <td>0.058305</td>\n",
       "      <td>0.236439</td>\n",
       "      <td>-0.463145</td>\n",
       "      <td>-0.248602</td>\n",
       "      <td>0.093736</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>-0.204310</td>\n",
       "      <td>0.042113</td>\n",
       "      <td>-0.187854</td>\n",
       "      <td>0.091225</td>\n",
       "      <td>-0.364573</td>\n",
       "      <td>-0.459845</td>\n",
       "      <td>0.120351</td>\n",
       "      <td>0.104499</td>\n",
       "      <td>0.091956</td>\n",
       "      <td>-0.089123</td>\n",
       "      <td>-0.066872</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.270656</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.333185</td>\n",
       "      <td>0.231751</td>\n",
       "      <td>-0.093407</td>\n",
       "      <td>-0.567328</td>\n",
       "      <td>-0.046153</td>\n",
       "      <td>-0.138583</td>\n",
       "      <td>-0.133085</td>\n",
       "      <td>-0.341136</td>\n",
       "      <td>0.473257</td>\n",
       "      <td>-0.113112</td>\n",
       "      <td>0.045474</td>\n",
       "      <td>-0.182890</td>\n",
       "      <td>-1.253330</td>\n",
       "      <td>0.406586</td>\n",
       "      <td>0.340712</td>\n",
       "      <td>-1.498406</td>\n",
       "      <td>-0.271695</td>\n",
       "      <td>0.072576</td>\n",
       "      <td>-0.352481</td>\n",
       "      <td>0.054213</td>\n",
       "      <td>-0.204740</td>\n",
       "      <td>-0.210762</td>\n",
       "      <td>-0.061104</td>\n",
       "      <td>0.216606</td>\n",
       "      <td>-0.089067</td>\n",
       "      <td>-0.298108</td>\n",
       "      <td>-0.130303</td>\n",
       "      <td>-0.227067</td>\n",
       "      <td>0.075511</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.118384</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>0.274825</td>\n",
       "      <td>-0.225737</td>\n",
       "      <td>-1.348910</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.669332</td>\n",
       "      <td>-0.095988</td>\n",
       "      <td>0.097986</td>\n",
       "      <td>-0.134075</td>\n",
       "      <td>-0.064345</td>\n",
       "      <td>0.462825</td>\n",
       "      <td>-0.275614</td>\n",
       "      <td>-0.037747</td>\n",
       "      <td>0.049038</td>\n",
       "      <td>-0.783268</td>\n",
       "      <td>-0.095683</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.712106</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.475336</td>\n",
       "      <td>0.037678</td>\n",
       "      <td>-0.488290</td>\n",
       "      <td>-0.159992</td>\n",
       "      <td>0.043705</td>\n",
       "      <td>-0.117826</td>\n",
       "      <td>-0.111004</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>0.014828</td>\n",
       "      <td>-0.314476</td>\n",
       "      <td>0.625367</td>\n",
       "      <td>-0.064194</td>\n",
       "      <td>-1.142129</td>\n",
       "      <td>-1.891173</td>\n",
       "      <td>0.882021</td>\n",
       "      <td>-0.113276</td>\n",
       "      <td>-0.073272</td>\n",
       "      <td>0.396857</td>\n",
       "      <td>2.025402</td>\n",
       "      <td>0.216449</td>\n",
       "      <td>0.210084</td>\n",
       "      <td>0.147967</td>\n",
       "      <td>0.271568</td>\n",
       "      <td>0.094674</td>\n",
       "      <td>3.215306</td>\n",
       "      <td>0.295688</td>\n",
       "      <td>-0.189353</td>\n",
       "      <td>1.526142</td>\n",
       "      <td>0.183819</td>\n",
       "      <td>0.328140</td>\n",
       "      <td>-0.113133</td>\n",
       "      <td>-1.552332</td>\n",
       "      <td>-2.073508</td>\n",
       "      <td>-0.212581</td>\n",
       "      <td>-0.011197</td>\n",
       "      <td>0.176810</td>\n",
       "      <td>0.247520</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>-0.122759</td>\n",
       "      <td>0.092361</td>\n",
       "      <td>-0.013593</td>\n",
       "      <td>0.067065</td>\n",
       "      <td>0.211937</td>\n",
       "      <td>0.259217</td>\n",
       "      <td>0.237663</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>-0.224228</td>\n",
       "      <td>-0.189737</td>\n",
       "      <td>0.240411</td>\n",
       "      <td>0.385132</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.082523</td>\n",
       "      <td>-0.288838</td>\n",
       "      <td>-0.224598</td>\n",
       "      <td>-1.728942</td>\n",
       "      <td>0.162733</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>0.266663</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>0.229339</td>\n",
       "      <td>-0.128911</td>\n",
       "      <td>0.066067</td>\n",
       "      <td>-0.271898</td>\n",
       "      <td>0.254672</td>\n",
       "      <td>0.181222</td>\n",
       "      <td>-0.193093</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.046412</td>\n",
       "      <td>-0.074836</td>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>-0.048620</td>\n",
       "      <td>1.910270</td>\n",
       "      <td>-0.208834</td>\n",
       "      <td>-0.071059</td>\n",
       "      <td>0.149921</td>\n",
       "      <td>0.261883</td>\n",
       "      <td>0.449044</td>\n",
       "      <td>-0.039009</td>\n",
       "      <td>-0.083040</td>\n",
       "      <td>-0.277306</td>\n",
       "      <td>-0.093306</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>-0.126179</td>\n",
       "      <td>-0.112660</td>\n",
       "      <td>-0.441329</td>\n",
       "      <td>0.073218</td>\n",
       "      <td>-0.094642</td>\n",
       "      <td>-0.088253</td>\n",
       "      <td>0.452180</td>\n",
       "      <td>-0.191223</td>\n",
       "      <td>0.108963</td>\n",
       "      <td>-0.859930</td>\n",
       "      <td>-0.036754</td>\n",
       "      <td>0.115587</td>\n",
       "      <td>0.165090</td>\n",
       "      <td>-0.235534</td>\n",
       "      <td>-0.419043</td>\n",
       "      <td>0.132750</td>\n",
       "      <td>0.113156</td>\n",
       "      <td>0.178455</td>\n",
       "      <td>0.177366</td>\n",
       "      <td>-0.074935</td>\n",
       "      <td>0.021362</td>\n",
       "      <td>-0.304195</td>\n",
       "      <td>-0.210215</td>\n",
       "      <td>-0.237170</td>\n",
       "      <td>-0.139138</td>\n",
       "      <td>-0.031268</td>\n",
       "      <td>-0.055137</td>\n",
       "      <td>-0.625930</td>\n",
       "      <td>-0.048458</td>\n",
       "      <td>1.297941</td>\n",
       "      <td>0.081248</td>\n",
       "      <td>-0.091083</td>\n",
       "      <td>0.661218</td>\n",
       "      <td>0.298312</td>\n",
       "      <td>-0.103515</td>\n",
       "      <td>0.274794</td>\n",
       "      <td>0.036803</td>\n",
       "      <td>-0.017087</td>\n",
       "      <td>-0.204624</td>\n",
       "      <td>-0.087942</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.189177</td>\n",
       "      <td>0.303763</td>\n",
       "      <td>-0.067535</td>\n",
       "      <td>0.120095</td>\n",
       "      <td>0.182622</td>\n",
       "      <td>0.364601</td>\n",
       "      <td>0.387490</td>\n",
       "      <td>0.372465</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>-0.280789</td>\n",
       "      <td>-0.220945</td>\n",
       "      <td>-0.240931</td>\n",
       "      <td>0.146988</td>\n",
       "      <td>0.156036</td>\n",
       "      <td>0.097520</td>\n",
       "      <td>0.051460</td>\n",
       "      <td>-0.031399</td>\n",
       "      <td>0.277281</td>\n",
       "      <td>0.201957</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>-0.188521</td>\n",
       "      <td>-0.324693</td>\n",
       "      <td>0.512566</td>\n",
       "      <td>0.226718</td>\n",
       "      <td>0.129673</td>\n",
       "      <td>0.202575</td>\n",
       "      <td>0.236635</td>\n",
       "      <td>0.139811</td>\n",
       "      <td>-0.085589</td>\n",
       "      <td>-0.218155</td>\n",
       "      <td>-0.334953</td>\n",
       "      <td>-0.232384</td>\n",
       "      <td>0.227386</td>\n",
       "      <td>0.185030</td>\n",
       "      <td>-0.085552</td>\n",
       "      <td>0.177533</td>\n",
       "      <td>-0.264404</td>\n",
       "      <td>-0.122173</td>\n",
       "      <td>-0.184156</td>\n",
       "      <td>-0.263314</td>\n",
       "      <td>0.251593</td>\n",
       "      <td>0.169985</td>\n",
       "      <td>-0.544212</td>\n",
       "      <td>-0.139173</td>\n",
       "      <td>0.070388</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.378906</td>\n",
       "      <td>-0.446852</td>\n",
       "      <td>0.099065</td>\n",
       "      <td>-0.002701</td>\n",
       "      <td>0.122351</td>\n",
       "      <td>0.299040</td>\n",
       "      <td>-0.092147</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>-0.231184</td>\n",
       "      <td>0.647902</td>\n",
       "      <td>0.125160</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>-0.119875</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>-0.073823</td>\n",
       "      <td>0.136381</td>\n",
       "      <td>0.214499</td>\n",
       "      <td>-0.551401</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.006809</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>-0.106861</td>\n",
       "      <td>0.515798</td>\n",
       "      <td>0.136783</td>\n",
       "      <td>-1.922994</td>\n",
       "      <td>-0.031434</td>\n",
       "      <td>-0.381685</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.304362</td>\n",
       "      <td>0.897940</td>\n",
       "      <td>-0.194853</td>\n",
       "      <td>-0.167243</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.170018</td>\n",
       "      <td>-0.008709</td>\n",
       "      <td>-0.019636</td>\n",
       "      <td>-0.240010</td>\n",
       "      <td>-0.566145</td>\n",
       "      <td>-0.077604</td>\n",
       "      <td>0.110287</td>\n",
       "      <td>0.081361</td>\n",
       "      <td>-0.184272</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-0.435302</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>-0.138828</td>\n",
       "      <td>-0.387774</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>-0.105501</td>\n",
       "      <td>-0.375121</td>\n",
       "      <td>-0.403055</td>\n",
       "      <td>0.223763</td>\n",
       "      <td>-0.186897</td>\n",
       "      <td>-0.337892</td>\n",
       "      <td>0.330360</td>\n",
       "      <td>-0.319272</td>\n",
       "      <td>0.347890</td>\n",
       "      <td>-0.231664</td>\n",
       "      <td>0.022066</td>\n",
       "      <td>-0.104849</td>\n",
       "      <td>0.125927</td>\n",
       "      <td>0.353665</td>\n",
       "      <td>-0.343490</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>-0.074666</td>\n",
       "      <td>-0.005430</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.230290</td>\n",
       "      <td>0.296979</td>\n",
       "      <td>-0.239275</td>\n",
       "      <td>-0.387272</td>\n",
       "      <td>-0.087447</td>\n",
       "      <td>-1.385870</td>\n",
       "      <td>0.164165</td>\n",
       "      <td>0.086118</td>\n",
       "      <td>0.477693</td>\n",
       "      <td>-0.154805</td>\n",
       "      <td>0.204899</td>\n",
       "      <td>-0.017503</td>\n",
       "      <td>0.207635</td>\n",
       "      <td>-0.267387</td>\n",
       "      <td>0.163694</td>\n",
       "      <td>-0.361353</td>\n",
       "      <td>0.104494</td>\n",
       "      <td>0.257115</td>\n",
       "      <td>0.249953</td>\n",
       "      <td>0.208703</td>\n",
       "      <td>-0.311522</td>\n",
       "      <td>0.287231</td>\n",
       "      <td>0.194591</td>\n",
       "      <td>-0.075470</td>\n",
       "      <td>-0.236217</td>\n",
       "      <td>0.242925</td>\n",
       "      <td>-0.163116</td>\n",
       "      <td>0.624239</td>\n",
       "      <td>-0.419350</td>\n",
       "      <td>0.202040</td>\n",
       "      <td>0.466183</td>\n",
       "      <td>0.207372</td>\n",
       "      <td>-0.090815</td>\n",
       "      <td>-0.076450</td>\n",
       "      <td>-0.013476</td>\n",
       "      <td>0.323429</td>\n",
       "      <td>-0.867128</td>\n",
       "      <td>0.113595</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>-0.095396</td>\n",
       "      <td>0.317765</td>\n",
       "      <td>-0.278106</td>\n",
       "      <td>0.396642</td>\n",
       "      <td>0.261178</td>\n",
       "      <td>0.354749</td>\n",
       "      <td>-0.337213</td>\n",
       "      <td>0.736657</td>\n",
       "      <td>1.748621</td>\n",
       "      <td>0.192067</td>\n",
       "      <td>-0.009619</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.283791</td>\n",
       "      <td>-0.343024</td>\n",
       "      <td>-0.040708</td>\n",
       "      <td>-0.716915</td>\n",
       "      <td>1.739474</td>\n",
       "      <td>-0.121069</td>\n",
       "      <td>-0.197947</td>\n",
       "      <td>-0.331439</td>\n",
       "      <td>0.326331</td>\n",
       "      <td>0.093071</td>\n",
       "      <td>1.605037</td>\n",
       "      <td>-0.059501</td>\n",
       "      <td>0.090526</td>\n",
       "      <td>-0.034893</td>\n",
       "      <td>0.041936</td>\n",
       "      <td>-0.044364</td>\n",
       "      <td>0.358782</td>\n",
       "      <td>-0.168617</td>\n",
       "      <td>-0.243071</td>\n",
       "      <td>-0.021190</td>\n",
       "      <td>-0.913893</td>\n",
       "      <td>-0.228516</td>\n",
       "      <td>0.380395</td>\n",
       "      <td>0.543660</td>\n",
       "      <td>0.173594</td>\n",
       "      <td>-0.251597</td>\n",
       "      <td>0.144921</td>\n",
       "      <td>-0.185885</td>\n",
       "      <td>-0.031317</td>\n",
       "      <td>-0.159127</td>\n",
       "      <td>-0.084391</td>\n",
       "      <td>-0.335865</td>\n",
       "      <td>0.321858</td>\n",
       "      <td>0.203519</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>-0.087100</td>\n",
       "      <td>-0.111201</td>\n",
       "      <td>-0.227971</td>\n",
       "      <td>0.101217</td>\n",
       "      <td>-0.015857</td>\n",
       "      <td>0.060778</td>\n",
       "      <td>0.529199</td>\n",
       "      <td>-0.160367</td>\n",
       "      <td>-0.153159</td>\n",
       "      <td>0.777417</td>\n",
       "      <td>-0.200691</td>\n",
       "      <td>0.036016</td>\n",
       "      <td>0.215525</td>\n",
       "      <td>-0.281756</td>\n",
       "      <td>-0.346513</td>\n",
       "      <td>-0.911152</td>\n",
       "      <td>-0.005457</td>\n",
       "      <td>0.121276</td>\n",
       "      <td>0.145280</td>\n",
       "      <td>0.193568</td>\n",
       "      <td>0.342988</td>\n",
       "      <td>0.069493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  retweet_count  favorite_count  \\\n",
       "0  1.304799e+18  1.599923e+09            0.0             1.0   \n",
       "1  1.304796e+18  1.599923e+09            1.0             0.0   \n",
       "2  1.304796e+18  1.599923e+09          802.0             0.0   \n",
       "3  1.304795e+18  1.599922e+09            0.0            15.0   \n",
       "4  1.304794e+18  1.599922e+09            0.0            10.0   \n",
       "\n",
       "   quoted_status_id       user.id  user.created_at  user.favourites_count  \\\n",
       "0               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "1               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "2               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "3               1.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "4               1.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "\n",
       "   user.followers_count  user.friends_count  user.listed_count  \\\n",
       "0                 377.0               774.0                1.0   \n",
       "1                 377.0               774.0                1.0   \n",
       "2                 377.0               774.0                1.0   \n",
       "3                 377.0               774.0                1.0   \n",
       "4                 377.0               774.0                1.0   \n",
       "\n",
       "   user.statuses_count  quoted_status.user.followers_count  \\\n",
       "0               1447.0                                 0.0   \n",
       "1               1447.0                                 0.0   \n",
       "2               1447.0                                 0.0   \n",
       "3               1447.0                             58746.0   \n",
       "4               1447.0                             58746.0   \n",
       "\n",
       "   quoted_status.user.friends_count  retweeted_status.user.followers_count  \\\n",
       "0                               0.0                                    0.0   \n",
       "1                               0.0                                   15.0   \n",
       "2                               0.0                                58746.0   \n",
       "3                            1102.0                                    0.0   \n",
       "4                            1102.0                                    0.0   \n",
       "\n",
       "   retweeted_status.user.friends_count  user_age  tweets_per_day  \\\n",
       "0                                  0.0      73.0       19.821918   \n",
       "1                                 35.0      73.0       19.821918   \n",
       "2                               1102.0      73.0       19.821918   \n",
       "3                                  0.0      73.0       19.821918   \n",
       "4                                  0.0      73.0       19.821918   \n",
       "\n",
       "   since_last_tweet_mins  since_last_tweet_mins_min  \\\n",
       "0                    0.0                        0.0   \n",
       "1                    0.0                        0.0   \n",
       "2                    0.0                        0.0   \n",
       "3                    0.0                        0.0   \n",
       "4                    0.0                        0.0   \n",
       "\n",
       "   since_last_tweet_mins_max  since_last_tweet_mins_mean  avg_tweets_per_hr  \\\n",
       "0                1380.216667                   38.853925           5.060942   \n",
       "1                1380.216667                   38.853925           5.060942   \n",
       "2                1380.216667                   38.853925           5.060942   \n",
       "3                1380.216667                   38.853925           5.060942   \n",
       "4                1380.216667                   38.853925           5.060942   \n",
       "\n",
       "   avg_tweets_per_day  no_hashtags  no_mentions  no_urls  tw_len  \\\n",
       "0           35.823529          0.0          1.0      0.0    66.0   \n",
       "1           35.823529          0.0          3.0      0.0   123.0   \n",
       "2           35.823529          0.0          1.0      1.0    60.0   \n",
       "3           35.823529          0.0          0.0      2.0    71.0   \n",
       "4           35.823529          0.0          0.0      1.0    37.0   \n",
       "\n",
       "   followers_per_followees  containsURL  user.urls_per_tweet  \\\n",
       "0                  0.48708          0.0             0.137384   \n",
       "1                  0.48708          0.0             0.137384   \n",
       "2                  0.48708          1.0             0.137384   \n",
       "3                  0.48708          1.0             0.137384   \n",
       "4                  0.48708          1.0             0.137384   \n",
       "\n",
       "   no_hashtags_per_tweet  no_mentions_per_tweet  no_urls_per_tweet  \\\n",
       "0               0.019704               1.860427           0.145594   \n",
       "1               0.019704               1.860427           0.145594   \n",
       "2               0.019704               1.860427           0.145594   \n",
       "3               0.019704               1.860427           0.145594   \n",
       "4               0.019704               1.860427           0.145594   \n",
       "\n",
       "   user.followers_countdailychange  user.friends_countdailychange  \\\n",
       "0                             7.08                          12.82   \n",
       "1                             7.08                          12.82   \n",
       "2                             7.08                          12.82   \n",
       "3                             7.08                          12.82   \n",
       "4                             7.08                          12.82   \n",
       "\n",
       "   user.friend_rate  user.followers_rate  user.has_url  user.has_location  \\\n",
       "0          10.60274             5.164384           0.0                1.0   \n",
       "1          10.60274             5.164384           0.0                1.0   \n",
       "2          10.60274             5.164384           0.0                1.0   \n",
       "3          10.60274             5.164384           0.0                1.0   \n",
       "4          10.60274             5.164384           0.0                1.0   \n",
       "\n",
       "   user.screen_name.digit_length  user.screen_name.length  is_reply  \\\n",
       "0                            0.0                     11.0       1.0   \n",
       "1                            0.0                     11.0       0.0   \n",
       "2                            0.0                     11.0       0.0   \n",
       "3                            0.0                     11.0       0.0   \n",
       "4                            0.0                     11.0       0.0   \n",
       "\n",
       "   suspended  source_        Round   Year Fun!       \\\n",
       "0          1                                      0   \n",
       "1          1                                      0   \n",
       "2          1                                      0   \n",
       "3          1                                      0   \n",
       "4          1                                      0   \n",
       "\n",
       "   source_      Round      Year   Fum    source_     Round    Year Fum         \\\n",
       "0                                     0                                     0   \n",
       "1                                     0                                     0   \n",
       "2                                     0                                     0   \n",
       "3                                     0                                     0   \n",
       "4                                     0                                     0   \n",
       "\n",
       "   source_  Round     Year Fum           source_ Round      Year   Fum         \\\n",
       "0                                     0                                     0   \n",
       "1                                     0                                     0   \n",
       "2                                     0                                     0   \n",
       "3                                     0                                     0   \n",
       "4                                     0                                     0   \n",
       "\n",
       "   source_ 「モンストからツイート」  source_Affinitweet.com  source_BIGO LIVE  \\\n",
       "0                     0                       0                 0   \n",
       "1                     0                       0                 0   \n",
       "2                     0                       0                 0   \n",
       "3                     0                       0                 0   \n",
       "4                     0                       0                 0   \n",
       "\n",
       "   source_Blog2Social APP  source_CallApp  source_Etsy  source_Instagram  \\\n",
       "0                       0               0            0                 0   \n",
       "1                       0               0            0                 0   \n",
       "2                       0               0            0                 0   \n",
       "3                       0               0            0                 0   \n",
       "4                       0               0            0                 0   \n",
       "\n",
       "   source_Joinfsocial  source_Mobile Web (M2)  source_Nintendo Switch Share  \\\n",
       "0                   0                       0                             0   \n",
       "1                   0                       0                             0   \n",
       "2                   0                       0                             0   \n",
       "3                   0                       0                             0   \n",
       "4                   0                       0                             0   \n",
       "\n",
       "   source_Paper.li  source_Peing  source_SocialPilot.co  \\\n",
       "0                0             0                      0   \n",
       "1                0             0                      0   \n",
       "2                0             0                      0   \n",
       "3                0             0                      0   \n",
       "4                0             0                      0   \n",
       "\n",
       "   source_TeamSight Publisher  source_TweetDeck  source_Twibbon  \\\n",
       "0                           0                 0               0   \n",
       "1                           0                 0               0   \n",
       "2                           0                 0               0   \n",
       "3                           0                 0               0   \n",
       "4                           0                 0               0   \n",
       "\n",
       "   source_Twitter Web App  source_Twitter Web Client  \\\n",
       "0                       1                          0   \n",
       "1                       1                          0   \n",
       "2                       1                          0   \n",
       "3                       1                          0   \n",
       "4                       1                          0   \n",
       "\n",
       "   source_Twitter for Android  source_Twitter for Mac  \\\n",
       "0                           0                       0   \n",
       "1                           0                       0   \n",
       "2                           0                       0   \n",
       "3                           0                       0   \n",
       "4                           0                       0   \n",
       "\n",
       "   source_Twitter for iPad  source_Twitter for iPhone  source_WShare  \\\n",
       "0                        0                          0              0   \n",
       "1                        0                          0              0   \n",
       "2                        0                          0              0   \n",
       "3                        0                          0              0   \n",
       "4                        0                          0              0   \n",
       "\n",
       "   source_WordPress.com  source_漫威超級戰爭（MARVEL Super War）  lang_False  lang_am  \\\n",
       "0                     0                                0           0        0   \n",
       "1                     0                                0           0        0   \n",
       "2                     0                                0           0        0   \n",
       "3                     0                                0           0        0   \n",
       "4                     0                                0           0        0   \n",
       "\n",
       "   lang_ar  lang_bg  lang_bn  lang_bo  lang_ca  lang_ckb  lang_cs  lang_cy  \\\n",
       "0        0        0        0        0        0         0        0        0   \n",
       "1        0        0        0        0        0         0        0        0   \n",
       "2        0        0        0        0        0         0        0        0   \n",
       "3        0        0        0        0        0         0        0        0   \n",
       "4        0        0        0        0        0         0        0        0   \n",
       "\n",
       "   lang_da  lang_de  lang_el  lang_en  lang_es  lang_et  lang_eu  lang_fa  \\\n",
       "0        0        0        0        1        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_fi  lang_fr  lang_gu  lang_hi  lang_ht  lang_hu  lang_hy  lang_in  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_is  lang_it  lang_iw  lang_ja  lang_km  lang_ko  lang_lt  lang_lv  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        1        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_ml  lang_mr  lang_ne  lang_nl  lang_no  lang_pl  lang_ps  lang_pt  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_ro  lang_ru  lang_sd  lang_si  lang_sl  lang_sr  lang_sv  lang_ta  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_th  lang_tl  lang_tr  lang_uk  lang_und  lang_ur  lang_vi  lang_zh  \\\n",
       "0        0        0        0        0         0        0        0        0   \n",
       "1        0        0        0        0         0        0        0        1   \n",
       "2        0        0        0        0         0        0        0        0   \n",
       "3        0        0        0        0         0        0        0        1   \n",
       "4        0        0        0        0         0        0        0        1   \n",
       "\n",
       "   possibly_sensitive_False  possibly_sensitive_True  \\\n",
       "0                         0                        0   \n",
       "1                         0                        0   \n",
       "2                         0                        0   \n",
       "3                         0                        0   \n",
       "4                         0                        0   \n",
       "\n",
       "   withheld_in_countries_['IN']  withheld_in_countries_['TR']  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   place.country_Finland  place.country_Hong Kong  place.country_India  \\\n",
       "0                      0                        0                    0   \n",
       "1                      0                        0                    0   \n",
       "2                      0                        0                    0   \n",
       "3                      0                        0                    0   \n",
       "4                      0                        0                    0   \n",
       "\n",
       "   place.country_Indonesia  place.country_Italy  place.country_Mongolia  \\\n",
       "0                        0                    0                       0   \n",
       "1                        0                    0                       0   \n",
       "2                        0                    0                       0   \n",
       "3                        0                    0                       0   \n",
       "4                        0                    0                       0   \n",
       "\n",
       "   place.country_Pakistan  place.country_People's Republic of China  \\\n",
       "0                       0                                         0   \n",
       "1                       0                                         0   \n",
       "2                       0                                         0   \n",
       "3                       0                                         0   \n",
       "4                       0                                         0   \n",
       "\n",
       "   place.country_Portugal  place.country_Republic of Korea  \\\n",
       "0                       0                                0   \n",
       "1                       0                                0   \n",
       "2                       0                                0   \n",
       "3                       0                                0   \n",
       "4                       0                                0   \n",
       "\n",
       "   place.country_Russia  place.country_Singapore  place.country_Sri Lanka  \\\n",
       "0                     0                        0                        0   \n",
       "1                     0                        0                        0   \n",
       "2                     0                        0                        0   \n",
       "3                     0                        0                        0   \n",
       "4                     0                        0                        0   \n",
       "\n",
       "   place.country_United Arab Emirates  place.country_United States  \\\n",
       "0                                   0                            0   \n",
       "1                                   0                            0   \n",
       "2                                   0                            0   \n",
       "3                                   0                            0   \n",
       "4                                   0                            0   \n",
       "\n",
       "   user.geo_enabled_False  user.geo_enabled_True  user.verified_False  \\\n",
       "0                       1                      0                    1   \n",
       "1                       1                      0                    1   \n",
       "2                       1                      0                    1   \n",
       "3                       1                      0                    1   \n",
       "4                       1                      0                    1   \n",
       "\n",
       "   user.has_extended_profile_False  user.has_extended_profile_True  \\\n",
       "0                                0                               1   \n",
       "1                                0                               1   \n",
       "2                                0                               1   \n",
       "3                                0                               1   \n",
       "4                                0                               1   \n",
       "\n",
       "   user.protected_False  user.verified_False.1  user.default_profile_True  \\\n",
       "0                     1                      1                          1   \n",
       "1                     1                      1                          1   \n",
       "2                     1                      1                          1   \n",
       "3                     1                      1                          1   \n",
       "4                     1                      1                          1   \n",
       "\n",
       "   is_quote_status_False  is_quote_status_True         0         1         2  \\\n",
       "0                      1                     0 -0.114942 -0.134353  0.173331   \n",
       "1                      1                     0  0.052672 -0.058918  0.191353   \n",
       "2                      1                     0  0.080426 -0.134674  0.112707   \n",
       "3                      0                     1  0.214187 -0.059479 -0.007084   \n",
       "4                      0                     1 -0.078277 -0.174203  0.049625   \n",
       "\n",
       "          3         4         5         6         7         8         9  \\\n",
       "0  0.136512  0.229718  0.017566  0.039284 -0.023467  0.081272  0.364327   \n",
       "1  0.277223  0.259454  0.100490 -0.096110  0.028599  0.088158 -0.022418   \n",
       "2  0.176269  0.488535 -0.147479  0.186054 -0.220972  0.044768  0.135161   \n",
       "3  0.156702  0.115002 -0.039318  0.072332 -0.113792 -0.055804  0.303812   \n",
       "4  0.021162 -0.127791  0.065504 -0.210719 -0.321700  0.204327  0.263463   \n",
       "\n",
       "         10        11        12        13        14        15        16  \\\n",
       "0  0.242406 -0.285628  0.187173 -0.057560 -0.566414 -0.101790 -0.213441   \n",
       "1  0.131286  0.000678  0.163917  0.129935 -0.749790 -0.128104 -0.170851   \n",
       "2  0.122971 -0.212626  0.327361  0.303039 -0.856692 -0.036403 -0.016536   \n",
       "3  0.233381 -0.173573  0.187994  0.064655 -0.705361 -0.066105 -0.065898   \n",
       "4  0.196186 -0.032833  0.318963  0.092261 -0.564071 -0.042986 -0.122809   \n",
       "\n",
       "         17        18        19        20        21        22        23  \\\n",
       "0  0.038836 -0.056996  0.019255  0.119526  0.101794  0.047508  0.254573   \n",
       "1  0.145990  0.114734  0.028382 -0.283772 -0.137989 -0.118955 -0.095648   \n",
       "2  0.346242  0.021573  0.203156 -0.120113 -0.064584 -0.072145 -0.281763   \n",
       "3  0.296897  0.089600  0.156202  0.302877  0.005592 -0.355658  0.006931   \n",
       "4 -0.031171 -0.148130  0.041004  0.186105  0.002320 -0.457684 -0.047066   \n",
       "\n",
       "         24        25        26        27        28        29        30  \\\n",
       "0  0.219492 -0.140189  0.046078 -0.057930  0.317335  0.130507  0.283876   \n",
       "1 -0.122314 -0.228784  0.025816  0.096372  0.057047  0.349612  0.018087   \n",
       "2 -0.095024 -0.598254 -0.139197  0.418660  0.406367  0.134682  0.320400   \n",
       "3  0.135165 -0.862504  0.141235  0.377050  0.364685  0.203312  0.171481   \n",
       "4 -0.370980 -0.614162  0.025137  0.274583  0.308254 -0.036962  0.128284   \n",
       "\n",
       "         31        32        33        34        35        36        37  \\\n",
       "0  0.014324  0.070263  0.260213  0.203483  0.198592 -1.972591 -0.319086   \n",
       "1  0.196444 -0.050039  0.327058  0.184806  0.012651 -1.862826 -0.201311   \n",
       "2  0.143624 -0.023052  0.107601  0.081439 -0.014636 -1.778479 -0.099808   \n",
       "3  0.050292  0.130933  0.172614  0.049645  0.036237 -2.015000  0.095337   \n",
       "4  0.137059 -0.012715  0.381060  0.012437 -0.054038 -2.011146  0.007331   \n",
       "\n",
       "         38        39        40        41        42        43        44  \\\n",
       "0  0.026047 -0.155292 -0.136676  0.215126  0.038997 -0.000104  0.230340   \n",
       "1 -0.131837 -0.297519  0.315354  0.075185 -0.315775 -0.008864  0.124587   \n",
       "2 -0.007258 -0.227994 -0.162111  0.179612 -0.119347 -0.176773  0.274527   \n",
       "3 -0.291435 -0.239225 -0.206998  0.170334  0.081537  0.068786  0.120738   \n",
       "4 -0.234859 -0.290969 -0.254029  0.296830  0.019744  0.040856  0.178252   \n",
       "\n",
       "         45        46        47        48        49        50        51  \\\n",
       "0  1.312637  0.217007  0.021399  1.049731 -0.427167  0.133239 -0.618760   \n",
       "1  0.907268  0.184001 -0.308740  1.200685 -0.390387  0.443256 -0.695511   \n",
       "2  0.815549  0.259377 -0.013182  0.902782 -0.291883 -0.044108 -0.737716   \n",
       "3  0.820182  0.099613  0.079094  1.232060 -0.346116  0.019636 -0.756126   \n",
       "4  0.917320  0.318727  0.032389  0.915136 -0.732133  0.020397 -0.653140   \n",
       "\n",
       "         52        53        54        55        56        57        58  \\\n",
       "0  0.321044 -1.421141  0.516384  0.091369  0.289944 -0.139161 -0.093265   \n",
       "1  0.064472 -1.346388  0.017364  0.126622  0.291861 -0.188345 -0.143834   \n",
       "2 -0.184261 -1.206096  0.026269 -0.086320  0.294976 -0.311108 -0.130351   \n",
       "3  0.255950 -1.328524 -0.029311  0.234481  0.172572 -0.344263 -0.016448   \n",
       "4  0.105605 -1.290869 -0.091505  0.343258  0.421036 -0.372141  0.148036   \n",
       "\n",
       "         59        60        61        62        63        64        65  \\\n",
       "0  0.153029  0.050548  0.228823  0.085134  0.013365 -0.350873 -0.073694   \n",
       "1  0.013241  0.190193  0.128031  0.121911 -0.067801 -0.373287 -0.057563   \n",
       "2 -0.131200  0.221982  0.262959  0.214355  0.097085 -0.189611 -0.189078   \n",
       "3  0.227704  0.264607 -0.096200 -0.017468  0.385079 -0.298228 -0.081520   \n",
       "4  0.089838  0.069913  0.023110 -0.002952  0.285805 -0.148635 -0.107743   \n",
       "\n",
       "         66        67        68        69        70        71        72  \\\n",
       "0  0.102264 -0.034420  0.115044  0.182258  0.293416 -0.052503  0.231046   \n",
       "1  0.357599  0.221048 -0.121907  0.192224 -0.014816  0.303250  0.206961   \n",
       "2  0.397775  0.184169 -0.027201  0.279851  0.032806  0.244333  0.224702   \n",
       "3  0.049895  0.062978  0.097310  0.156088  0.164003  0.022362  0.275796   \n",
       "4 -0.060025  0.204169  0.198132  0.041243  0.157347 -0.040833  0.411137   \n",
       "\n",
       "         73        74        75        76        77        78        79  \\\n",
       "0 -0.170732 -0.038697 -0.282374  0.060193 -0.242756 -0.020662 -0.004166   \n",
       "1 -0.121902  0.172318  0.275762  0.074032  0.073718  0.082032 -0.076715   \n",
       "2 -0.027513  0.132306  0.013470  0.132776  0.276600 -0.378284  0.277806   \n",
       "3  0.147933 -0.152266 -0.098864  0.304742  0.381578  0.012273  0.013237   \n",
       "4  0.033220 -0.010356 -0.230977  0.175235  0.166121 -0.196147 -0.133367   \n",
       "\n",
       "         80        81        82        83        84        85        86  \\\n",
       "0  0.102102  0.383095 -0.341194 -0.073726  0.008700 -0.022729  0.039218   \n",
       "1 -0.127579  0.311564 -0.495520  0.135617 -0.105971 -0.114640 -0.105681   \n",
       "2 -0.015235 -0.011454  0.042170  0.388840  0.007314 -0.275472  0.020594   \n",
       "3 -0.331045  0.008643 -0.302817  0.153991  0.105696 -0.173794 -0.152133   \n",
       "4 -0.125821  0.258623 -0.216341  0.198041  0.280206 -0.185696 -0.228802   \n",
       "\n",
       "         87        88        89        90        91        92        93  \\\n",
       "0 -0.025650 -0.293690 -0.182859 -0.288709 -0.271488 -0.614931 -0.333310   \n",
       "1 -0.026936  0.065985  0.142857 -0.273669 -0.065600 -0.768612  0.052605   \n",
       "2 -0.024743  0.110728  0.018980 -0.165665 -0.255867 -0.486603 -0.042945   \n",
       "3 -0.155555  0.223095 -0.129571  0.030113 -0.238156 -0.491402 -0.063609   \n",
       "4 -0.097395 -0.015763 -0.275382  0.026012 -0.146869 -0.312870  0.004885   \n",
       "\n",
       "         94        95        96        97        98        99       100  \\\n",
       "0 -0.298701 -0.683803 -0.058847 -0.198511 -0.017378 -0.005585  0.134216   \n",
       "1 -0.348544 -0.940609 -0.053573  0.093908  0.200630 -0.055771  0.015091   \n",
       "2 -0.137272 -0.663345  0.206857  0.038714  0.249513 -0.051702 -0.126937   \n",
       "3 -0.260211 -1.019694  0.257401  0.186786 -0.098493 -0.133445  0.071476   \n",
       "4 -0.291111 -0.629528  0.250742  0.102231  0.132436 -0.023161 -0.187149   \n",
       "\n",
       "        101       102       103       104       105       106       107  \\\n",
       "0  1.623948  0.309160  0.016629 -2.841270  0.057761 -0.121874 -0.301343   \n",
       "1  1.204730  0.665214  0.238307 -2.531948  0.051051  0.189210 -0.537894   \n",
       "2  1.299432  0.290933 -0.113339 -2.107814  0.302757  0.242091 -0.312805   \n",
       "3  1.586970  0.055472 -0.203007 -2.714346  0.397003  0.149403 -0.072598   \n",
       "4  1.473456  0.146264 -0.237220 -2.635231  0.429763  0.069974  0.068536   \n",
       "\n",
       "        108       109       110       111       112       113       114  \\\n",
       "0  0.591229  0.122781  0.157077 -0.075809  0.289568  0.025465  0.224462   \n",
       "1  0.488715 -0.026767  0.235405 -0.035871  0.320023  0.032610  0.025327   \n",
       "2  0.515373  0.225225  0.194329  0.051338  0.381716 -0.136601 -0.098521   \n",
       "3  0.540367  0.011368  0.254004 -0.016248  0.316370 -0.043439 -0.147107   \n",
       "4  0.433039 -0.025383  0.415848  0.077679  0.292029  0.195455 -0.194162   \n",
       "\n",
       "        115       116       117       118       119       120       121  \\\n",
       "0  0.050155 -0.220560 -0.423566 -0.298882 -0.003763  0.089865  0.116149   \n",
       "1  0.045891  0.200612 -0.094909 -0.383914  0.002838 -0.278634  0.095051   \n",
       "2 -0.048946  0.035652 -0.289012 -0.104529 -0.076477  0.016833  0.250727   \n",
       "3  0.330427 -0.180701 -0.151760 -0.617787 -0.029431 -0.134987  0.053385   \n",
       "4  0.187316 -0.138606 -0.214609 -0.618403  0.036800 -0.119306  0.201350   \n",
       "\n",
       "        122       123       124       125       126       127       128  \\\n",
       "0 -0.066239 -0.116553 -0.211578 -0.026433  0.066474 -0.172347  0.114439   \n",
       "1  0.084782 -0.127206 -0.124167 -0.214576 -0.098237 -0.354313  0.066203   \n",
       "2  0.488066 -0.163111  0.074161 -0.492105  0.141187 -0.429315 -0.015018   \n",
       "3  0.351290 -0.511990  0.031337 -0.121426  0.255728 -0.380908  0.421364   \n",
       "4  0.337874 -0.436353  0.100118 -0.090218  0.098789 -0.335698  0.298973   \n",
       "\n",
       "        129       130       131       132       133       134       135  \\\n",
       "0  1.008800  0.048970  2.113558  0.183062 -0.089003  0.328721  1.391015   \n",
       "1  0.631516  0.098180  1.734454  0.078826 -0.433104  0.204309  1.436106   \n",
       "2  0.875712 -0.125413  1.471851 -0.132518 -0.057724  0.034620  1.556313   \n",
       "3  1.234295 -0.259464  1.811132 -0.036996 -0.478139  0.015984  1.468619   \n",
       "4  1.348853  0.151304  1.808684  0.008026 -0.473900  0.047641  1.355007   \n",
       "\n",
       "        136       137       138       139       140       141       142  \\\n",
       "0 -0.000484  0.246612  0.259096  0.428238 -0.113854 -0.072313  0.136662   \n",
       "1  0.191420  0.421982  0.505524  0.497787 -0.072330 -0.060478  0.230735   \n",
       "2  0.174178  0.398473  0.318080  0.213674 -0.270714  0.020475  0.138001   \n",
       "3 -0.062780  0.305993  0.109800  0.054765 -0.337659  0.025861 -0.514213   \n",
       "4  0.000264  0.205882  0.082156  0.339395 -0.223712 -0.065106 -0.320821   \n",
       "\n",
       "        143       144       145       146       147       148       149  \\\n",
       "0 -0.262657 -0.196306 -0.065022 -0.073605  0.112380  0.153970 -0.234338   \n",
       "1 -0.030728 -0.236866  0.057978  0.192613  0.188570  0.201878 -0.129033   \n",
       "2 -0.262408 -0.151041  0.176879 -0.110966  0.306894  0.031218 -0.280083   \n",
       "3 -0.290076  0.110464  0.058185 -0.181488 -0.005890  0.083280 -0.079189   \n",
       "4 -0.012630 -0.040721  0.178549 -0.358616 -0.205188  0.299488  0.138494   \n",
       "\n",
       "        150       151       152       153       154       155       156  \\\n",
       "0 -0.253047 -0.042749 -0.030998 -0.203711  0.182217 -0.255710  0.009442   \n",
       "1 -0.254614 -0.172734  0.153413 -0.106675  0.317641 -0.008802  0.110751   \n",
       "2  0.090888 -0.005017  0.234349 -0.301008  0.057005 -0.040138  0.139075   \n",
       "3 -0.082426 -0.091485  0.030964 -0.308282 -0.033312 -0.040573 -0.154920   \n",
       "4 -0.214512 -0.211112  0.261672 -0.285922 -0.156016 -0.159515 -0.179229   \n",
       "\n",
       "        157       158       159       160       161       162       163  \\\n",
       "0 -0.058575  0.026881  0.192080  1.438943  0.128215 -0.054786  0.577039   \n",
       "1 -0.294944 -0.223167  0.704600  1.681405  0.013180 -0.202991  0.510207   \n",
       "2 -0.240067  0.016235  0.634772  1.273974  0.130543 -0.112971  0.880515   \n",
       "3 -0.157907 -0.211003  0.770553  1.654910  0.400755  0.154540  0.593819   \n",
       "4  0.013896 -0.184307  0.786049  1.625420  0.219464 -0.166830  0.725683   \n",
       "\n",
       "        164       165       166       167       168       169       170  \\\n",
       "0  0.131675  0.065016  0.033296  0.418629 -0.098543 -0.443419 -0.315794   \n",
       "1  0.068040 -0.075538  0.008880  0.091109 -0.222204 -0.408947 -0.335609   \n",
       "2  0.006160 -0.185321  0.114886  0.150478 -0.078345 -0.457893 -0.306409   \n",
       "3  0.139085 -0.146766 -0.154728  0.083942 -0.269372 -0.347635 -0.365141   \n",
       "4  0.241789 -0.205717 -0.072434 -0.044138 -0.185978 -0.420110 -0.256598   \n",
       "\n",
       "        171       172       173       174       175       176       177  \\\n",
       "0 -0.119009  0.308256 -0.164729  0.024635  0.097390 -0.153226  0.066648   \n",
       "1 -0.332711 -0.113538 -0.086438 -0.219503 -0.354867 -0.105105  0.173235   \n",
       "2 -0.084836  0.023258 -0.020834  0.082531 -0.264680 -0.318537  0.106770   \n",
       "3 -0.027031  0.200434 -0.114942  0.103986 -0.302938  0.030013 -0.085181   \n",
       "4  0.009339 -0.057659 -0.012772  0.198539 -0.135660 -0.225502  0.020885   \n",
       "\n",
       "        178       179       180       181       182       183       184  \\\n",
       "0 -0.089602 -0.012601 -0.169174  0.033604 -0.017636  0.225650 -0.003460   \n",
       "1 -0.037553 -0.089969  0.027282  0.029195 -0.048773  0.003316  0.241397   \n",
       "2 -0.155994  0.047493 -0.025387  0.396370 -0.493968 -0.070045  0.330228   \n",
       "3  0.033061 -0.286653 -0.088080  0.280806 -0.376989 -0.136734  0.308672   \n",
       "4 -0.012674 -0.310646  0.010176  0.406861 -0.277630 -0.202905  0.314422   \n",
       "\n",
       "        185       186       187       188       189       190       191  \\\n",
       "0  0.091388 -0.043985 -0.090680 -0.060891  0.044943  0.080476  0.194946   \n",
       "1  0.057202 -0.001857  0.010730 -0.132713 -0.009519  0.188325  0.013419   \n",
       "2 -0.301858  0.096318  0.005961 -0.274402 -0.408135  0.118408  0.323034   \n",
       "3  0.075215 -0.015112 -0.136485 -0.181825 -0.282070  0.250367  0.332802   \n",
       "4 -0.009326 -0.090112 -0.227132 -0.126610 -0.190479  0.254401  0.264294   \n",
       "\n",
       "        192       193       194       195       196       197       198  \\\n",
       "0  0.252762  0.111750 -0.045780  0.061371 -0.100169  0.108905 -0.034522   \n",
       "1  0.135744  0.133174  0.131778  0.254248 -0.340015  0.174643  0.053142   \n",
       "2 -0.007343  0.240692  0.012226  0.268068 -0.341359 -0.223546 -0.089394   \n",
       "3  0.209194  0.090597 -0.210849  0.172185 -0.298031  0.192958 -0.340685   \n",
       "4 -0.126774  0.060016 -0.097042  0.145901 -0.048483  0.078163 -0.449819   \n",
       "\n",
       "        199       200       201       202       203       204       205  \\\n",
       "0  0.117903  0.236024  0.185837 -0.002428 -0.124239  0.026417  0.456760   \n",
       "1  0.103338  0.149088  0.293034 -0.181952 -0.105761 -0.033977  0.536524   \n",
       "2  0.133622  0.193548 -0.034420 -0.199397 -0.385047  0.116580  0.583352   \n",
       "3 -0.035528  0.181731  0.222832 -0.029590 -0.281797 -0.042110  0.605381   \n",
       "4  0.084382 -0.132889  0.171121 -0.189504 -0.342030  0.090388  0.778550   \n",
       "\n",
       "        206       207       208       209       210       211       212  \\\n",
       "0  0.130609  0.084755  0.107448  0.221918 -0.272671 -0.175078  1.045756   \n",
       "1  0.067477 -0.321624  0.067708  0.358331 -0.217633 -0.080630  0.992161   \n",
       "2  0.106876 -0.450092 -0.066457  0.593961 -0.060532 -0.146741  0.894652   \n",
       "3 -0.018986 -0.333173  0.286789  0.317704  0.078848 -0.338971  1.144300   \n",
       "4 -0.143175 -0.417559  0.307019  0.288917  0.084393 -0.375040  1.064945   \n",
       "\n",
       "        213       214       215       216       217       218       219  \\\n",
       "0  0.088317  0.269874  0.043521 -0.258605  0.022063  0.277678 -2.092627   \n",
       "1  0.162348 -0.004624  0.005774  0.146003 -0.078902  0.326103 -2.175950   \n",
       "2 -0.079527  0.247370 -0.085136  0.182634  0.035095  0.083640 -1.741395   \n",
       "3  0.369384  0.138040 -0.113330  0.245751 -0.057461 -0.022409 -2.190071   \n",
       "4  0.281548  0.037414 -0.001120  0.269933 -0.172180 -0.005728 -1.919863   \n",
       "\n",
       "        220       221       222       223       224       225       226  \\\n",
       "0 -0.238210  0.068849 -0.393595  0.038905 -0.137781 -0.100083 -0.076923   \n",
       "1  0.074236  0.042553  0.088233  0.003111 -0.213741  0.015710  0.087525   \n",
       "2 -0.311545  0.188462  0.093584  0.099274 -0.278172  0.229764 -0.033252   \n",
       "3 -0.049383  0.248353  0.150893  0.126102 -0.434847  0.061381  0.010033   \n",
       "4 -0.093487  0.317770  0.054238  0.195610 -0.383640  0.163156 -0.090594   \n",
       "\n",
       "        227       228       229       230       231       232       233  \\\n",
       "0  0.408899 -0.204873  0.167127  0.469248  0.156101  0.150499 -1.912055   \n",
       "1  0.259136 -0.068323  0.082989  0.368531 -0.357657  0.271537 -1.579786   \n",
       "2  0.197171  0.289365  0.091514  0.213897 -0.529606  0.146549 -1.955689   \n",
       "3  0.393739  0.395514  0.099296  0.055092 -0.502763  0.206816 -2.172371   \n",
       "4  0.325676  0.299610  0.135714  0.350221 -0.312806 -0.068715 -2.059254   \n",
       "\n",
       "        234       235       236       237       238       239       240  \\\n",
       "0  0.029725  0.057159  0.352324 -0.022346 -0.168845 -0.216576 -0.005189   \n",
       "1  0.034223  0.075506  0.161659 -0.129009 -0.005902  0.190349  0.467461   \n",
       "2  0.030564  0.095796  0.217502 -0.142379  0.105184 -0.063172  0.530033   \n",
       "3  0.006114  0.002107  0.173373 -0.177359 -0.451319 -0.263518  0.385291   \n",
       "4  0.011472  0.073927  0.282447 -0.125010 -0.514405 -0.141305  0.439810   \n",
       "\n",
       "        241       242       243       244       245       246       247  \\\n",
       "0  0.256714  0.271091 -0.324324  0.253692 -0.162768  1.048868 -0.015548   \n",
       "1 -0.093247 -0.032525 -0.231185  0.262532 -0.227358  1.034526 -0.204923   \n",
       "2  0.308147 -0.083365 -0.033799  0.183865 -0.223949  0.799385 -0.474524   \n",
       "3  0.171382  0.210566 -0.061831  0.206926  0.073623  0.853944 -0.539201   \n",
       "4  0.013498  0.206061 -0.072718  0.526341  0.186767  0.814527 -0.464755   \n",
       "\n",
       "        248       249       250       251       252       253       254  \\\n",
       "0  0.101220 -0.398941  0.156095  0.285644  0.588194 -0.006593  0.381711   \n",
       "1 -0.084587 -0.122245  0.009187  0.071010  0.479335  0.000184 -0.138671   \n",
       "2 -0.460576 -0.044182  0.216623  0.009320  0.165013 -0.111722 -0.081998   \n",
       "3 -0.004527 -0.271299  0.224307 -0.079206  0.089325  0.013823 -0.130193   \n",
       "4 -0.052261 -0.460785  0.233475 -0.151032  0.309687  0.044735 -0.167510   \n",
       "\n",
       "        255       256       257       258       259       260       261  \\\n",
       "0  0.217428 -0.123698 -0.237995  0.217746  0.134358 -0.252432  0.151119   \n",
       "1  0.275043 -0.053618 -0.344126  0.070405 -0.138441  0.040745  0.141255   \n",
       "2 -0.041621 -0.108558 -0.066339  0.317524 -0.246073 -0.091003  0.121568   \n",
       "3  0.119565 -0.150260 -0.087359  0.130078 -0.315774 -0.059658  0.041780   \n",
       "4  0.474102  0.011034 -0.236642  0.277459 -0.172376 -0.276700  0.079124   \n",
       "\n",
       "        262       263       264       265       266       267       268  \\\n",
       "0 -0.462669 -0.318928 -0.183064 -0.243318  0.218916  0.216158  0.016429   \n",
       "1 -0.336352 -0.371722 -0.066489 -0.310397  0.045778  0.026634  0.146001   \n",
       "2 -0.406422 -0.161607 -0.119543 -0.176654  0.280035  0.184037 -0.061895   \n",
       "3 -0.340865 -0.109432 -0.086527 -0.210897  0.211090  0.387554 -0.283232   \n",
       "4 -0.437688 -0.172888 -0.124094 -0.282119  0.371277  0.472138 -0.106011   \n",
       "\n",
       "        269       270       271       272       273       274       275  \\\n",
       "0  0.127511  0.165406 -0.006228  0.055674 -0.137860 -0.120376  0.075913   \n",
       "1 -0.142373 -0.140524 -0.097830 -0.011692  0.127692 -0.196047  0.147516   \n",
       "2 -0.314775  0.222850  0.006709  0.159540  0.030843  0.099132 -0.054322   \n",
       "3 -0.101025 -0.014107  0.160044  0.345910 -0.113012 -0.127737 -0.130198   \n",
       "4 -0.176401  0.054863  0.197259  0.284405  0.087026 -0.368444 -0.119897   \n",
       "\n",
       "        276       277       278       279       280       281       282  \\\n",
       "0 -0.085324  0.215300 -0.205171 -0.048181 -0.212470  0.058060  0.034922   \n",
       "1 -0.194761  0.066085  0.051742  0.010566  0.114777 -0.071259 -0.082067   \n",
       "2 -0.081392  0.141576 -0.072004  0.210885  0.036518 -0.140025 -0.128226   \n",
       "3  0.292036  0.035848 -0.034419  0.181228  0.147207 -0.213254 -0.032855   \n",
       "4  0.160622  0.292768 -0.287437  0.155879  0.039946 -0.177790  0.026550   \n",
       "\n",
       "        283       284       285       286       287       288       289  \\\n",
       "0  0.342037 -0.033949  0.027961 -0.168917 -0.254982 -0.527160 -1.113713   \n",
       "1  0.221838  0.036886  0.193118  0.069551 -0.088746 -0.201686 -0.903562   \n",
       "2  0.552561 -0.010165 -0.011358  0.064538 -0.065134 -0.181452 -0.825406   \n",
       "3  0.292569  0.080702  0.135831  0.427715  0.160318 -0.509982 -1.067933   \n",
       "4  0.589625  0.033765  0.160491  0.292537  0.099510 -0.262996 -0.936150   \n",
       "\n",
       "        290       291       292       293       294       295       296  \\\n",
       "0 -0.229061 -0.034499 -0.110246 -0.217912 -0.049210 -0.355850 -0.580077   \n",
       "1 -0.063673  0.091515  0.005868 -0.218143  0.026177  0.047984 -0.249656   \n",
       "2 -0.065834  0.041021  0.121893  0.098652  0.044519 -0.240898 -0.244179   \n",
       "3  0.129897  0.099443 -0.038021  0.017503 -0.056278 -0.280135  0.008329   \n",
       "4  0.222801  0.085874  0.104765  0.151843 -0.250792 -0.209840  0.215738   \n",
       "\n",
       "        297       298       299       300       301       302       303  \\\n",
       "0 -0.110057 -0.213163  0.030453  0.203308 -0.270568 -0.007047  0.004234   \n",
       "1 -0.097899 -0.105714  0.259082  0.325662 -0.098979  0.079754  0.274477   \n",
       "2 -0.061744  0.032404  0.001118  0.221089 -0.200351 -0.144002  0.239725   \n",
       "3 -0.169074 -0.045351  0.167777  0.078974 -0.277739 -0.221999 -0.147781   \n",
       "4 -0.144433 -0.177102  0.232946 -0.150948 -0.259506 -0.276193 -0.196652   \n",
       "\n",
       "        304       305       306       307       308       309       310  \\\n",
       "0 -1.640332  0.327920  0.058750  0.041248  0.390960  0.216196 -0.291855   \n",
       "1 -1.577111  0.079100  0.445884 -0.122460  0.168900  0.045718 -0.253436   \n",
       "2 -1.297311  0.223271  0.056987 -0.247194  0.069370  0.114280 -0.231376   \n",
       "3 -1.526245  0.312276 -0.169789 -0.189879  0.036879  0.200151 -0.115968   \n",
       "4 -1.397798  0.262344 -0.093686 -0.204031  0.146204  0.173420 -0.188087   \n",
       "\n",
       "        311       312       313       314       315       316       317  \\\n",
       "0  0.139326  0.134371 -0.194382 -0.130841 -0.088784  0.056905  0.100911   \n",
       "1  0.006776 -0.145037 -0.244011 -0.048429  0.001072  0.295289  0.201259   \n",
       "2 -0.022354 -0.242212 -0.198054  0.008500  0.037717  0.052453  0.397130   \n",
       "3 -0.203248 -0.270180 -0.311013 -0.412499  0.096283 -0.127346  0.416598   \n",
       "4 -0.165837 -0.244575 -0.320595 -0.490913 -0.033065 -0.049807  0.383614   \n",
       "\n",
       "        318       319       320       321       322       323       324  \\\n",
       "0 -0.171897 -0.006919  0.043518 -1.802850  0.140893  0.054359  0.064364   \n",
       "1 -0.096990 -0.035488 -0.201499 -1.669044  0.178489  0.094926  0.038488   \n",
       "2 -0.272806  0.030363 -0.268373 -1.167314 -0.001862  0.123383 -0.158216   \n",
       "3 -0.116475 -0.051191  0.120350 -1.671294  0.036233 -0.142249 -0.089807   \n",
       "4 -0.010223 -0.110179  0.132222 -1.425190 -0.043583 -0.298038  0.031573   \n",
       "\n",
       "        325       326       327       328       329       330       331  \\\n",
       "0  0.045542 -0.069615  0.170746  0.135491  0.113001  0.106351  0.503941   \n",
       "1 -0.138048  0.035850  0.125822  0.153879  0.104066  0.075819 -0.107096   \n",
       "2 -0.117367 -0.106028 -0.103492  0.276079  0.014971  0.405745 -0.206124   \n",
       "3  0.011266 -0.138569  0.169761 -0.073503  0.378245  0.177246 -0.298494   \n",
       "4 -0.009196  0.082097  0.134483  0.150740  0.254168 -0.015646  0.156347   \n",
       "\n",
       "        332       333       334       335       336       337       338  \\\n",
       "0  0.023236 -0.149475 -0.038604 -0.028201  0.180176 -0.152957  0.044299   \n",
       "1 -0.275737 -0.271589 -0.227140  0.033027 -0.106995 -0.156322 -0.096130   \n",
       "2 -0.201381 -0.074994  0.138113  0.260481 -0.213984 -0.065476 -0.056164   \n",
       "3 -0.242366 -0.096903  0.027831  0.310804 -0.412920 -0.067518 -0.333639   \n",
       "4 -0.078445 -0.230638  0.139303  0.458750 -0.332444  0.031446 -0.080375   \n",
       "\n",
       "        339       340       341       342       343       344       345  \\\n",
       "0  0.061128 -0.151400 -0.176841  0.180401  0.139347  0.093173  0.067539   \n",
       "1 -0.297114 -0.370665 -0.090220  0.389313  0.246668 -0.106050  0.273808   \n",
       "2 -0.069920 -0.279557 -0.433864  0.043803 -0.016963  0.008879  0.374225   \n",
       "3 -0.180675 -0.279922 -0.050898 -0.012616 -0.119002  0.268140  0.381062   \n",
       "4  0.002856 -0.176316 -0.120925  0.091290 -0.162544  0.021520  0.134609   \n",
       "\n",
       "        346       347       348       349       350       351       352  \\\n",
       "0 -0.130477 -0.445037  0.109330  0.417492  0.085073  0.250069 -0.570612   \n",
       "1  0.218396 -0.227423  0.292198  0.360163  0.128740  0.179784 -0.361277   \n",
       "2  0.086910 -0.205872  0.600580  0.530726  0.013050  0.167333 -0.286193   \n",
       "3 -0.089646 -0.068158  0.166571  0.504804  0.011533  0.491953 -0.429315   \n",
       "4 -0.076166 -0.051236  0.158702  0.418449  0.058305  0.236439 -0.463145   \n",
       "\n",
       "        353       354       355       356       357       358       359  \\\n",
       "0 -0.318602 -0.086646  0.042726 -0.010509  0.057579 -0.375788 -0.136379   \n",
       "1 -0.108207 -0.017846  0.124008 -0.091146  0.145459 -0.043438  0.153791   \n",
       "2 -0.201827 -0.007286  0.199121 -0.062046  0.122459 -0.410956  0.151578   \n",
       "3 -0.196230  0.203333 -0.014058 -0.229958  0.112453 -0.315014  0.347439   \n",
       "4 -0.248602  0.093736  0.017708 -0.204310  0.042113 -0.187854  0.091225   \n",
       "\n",
       "        360       361       362       363       364       365       366  \\\n",
       "0 -0.087900 -0.180236  0.029235  0.038217  0.164449 -0.085967 -0.072250   \n",
       "1 -0.177207 -0.155849  0.046694  0.126901  0.327502  0.008151 -0.177967   \n",
       "2 -0.079604 -0.232702 -0.070239  0.372755  0.180651 -0.177912  0.058427   \n",
       "3 -0.408025 -0.287301  0.082728 -0.002378  0.314346 -0.026424  0.083048   \n",
       "4 -0.364573 -0.459845  0.120351  0.104499  0.091956 -0.089123 -0.066872   \n",
       "\n",
       "        367       368       369       370       371       372       373  \\\n",
       "0 -0.199525  0.235542 -0.436713  0.120687  0.205707 -0.210220 -0.710811   \n",
       "1 -0.251835 -0.165355  0.395987  0.420862  0.351807 -0.331257 -0.673759   \n",
       "2  0.031830  0.144243  0.815164  0.402201  0.494515 -0.190647 -0.484440   \n",
       "3 -0.164114 -0.099241  0.433942  0.312877  0.454024 -0.196333 -0.688390   \n",
       "4 -0.028471  0.270656  0.001729  0.333185  0.231751 -0.093407 -0.567328   \n",
       "\n",
       "        374       375       376       377       378       379       380  \\\n",
       "0  0.212766 -0.010203  0.080827  0.053471 -0.311711 -0.168204  0.067390   \n",
       "1  0.336020 -0.122305  0.011936 -0.293467  0.009400 -0.158813  0.100868   \n",
       "2  0.103818  0.052889 -0.270021 -0.292426  0.511552 -0.176861 -0.193064   \n",
       "3 -0.200323 -0.058434 -0.109639 -0.280405  0.487777 -0.132122  0.043661   \n",
       "4 -0.046153 -0.138583 -0.133085 -0.341136  0.473257 -0.113112  0.045474   \n",
       "\n",
       "        381       382       383       384       385       386       387  \\\n",
       "0 -0.205203 -1.537679  0.305504  0.329089 -1.564162  0.075029  0.076229   \n",
       "1 -0.089790 -1.169701  0.179054  0.248582 -1.394230 -0.149468 -0.023500   \n",
       "2 -0.136917 -1.171707  0.065674  0.243738 -1.193957 -0.367779 -0.069849   \n",
       "3 -0.076397 -1.149711  0.526460  0.343722 -1.818758 -0.143835  0.096751   \n",
       "4 -0.182890 -1.253330  0.406586  0.340712 -1.498406 -0.271695  0.072576   \n",
       "\n",
       "        388       389       390       391       392       393       394  \\\n",
       "0 -0.200819 -0.131148  0.026027 -0.256942  0.252053  0.298461 -0.086356   \n",
       "1 -0.021593 -0.020985  0.017698  0.082202  0.122629 -0.391835  0.009212   \n",
       "2 -0.296484 -0.070715  0.070555  0.114988 -0.240360  0.080535 -0.130899   \n",
       "3 -0.412193 -0.056452 -0.085726  0.049318  0.047936  0.252898 -0.139994   \n",
       "4 -0.352481  0.054213 -0.204740 -0.210762 -0.061104  0.216606 -0.089067   \n",
       "\n",
       "        395       396       397       398       399       400       401  \\\n",
       "0 -0.193547  0.055482 -0.099840 -0.198960 -0.060246 -0.017196 -0.160717   \n",
       "1 -0.009983 -0.022500 -0.196205 -0.122086  0.161794 -0.006796 -0.256333   \n",
       "2 -0.147260 -0.184686 -0.068393 -0.130287  0.195133 -0.127566 -0.258508   \n",
       "3  0.046172 -0.232740 -0.090420  0.123246  0.018235  0.065993  0.041021   \n",
       "4 -0.298108 -0.130303 -0.227067  0.075511  0.022013  0.118384  0.027540   \n",
       "\n",
       "        402       403       404       405       406       407       408  \\\n",
       "0  0.112565 -0.132643 -0.005622 -1.633456 -0.062534  0.014561 -0.104622   \n",
       "1  0.271447  0.161287 -0.187338 -1.482814 -0.154344  0.286548 -0.042049   \n",
       "2  0.226293  0.205090 -0.045750 -1.411300 -0.276614  0.645107 -0.078114   \n",
       "3  0.071642  0.154944 -0.145971 -1.589564  0.032751  0.420787 -0.105569   \n",
       "4  0.238228  0.274825 -0.225737 -1.348910  0.237879  0.669332 -0.095988   \n",
       "\n",
       "        409       410       411       412       413       414       415  \\\n",
       "0 -0.117503  0.257080 -0.060662  0.244352 -0.127829  0.251282  0.158270   \n",
       "1 -0.071985  0.049052 -0.204481  0.259334  0.021722 -0.145066  0.033043   \n",
       "2 -0.113617 -0.114822 -0.164031  0.512193 -0.076346 -0.004242  0.162491   \n",
       "3  0.107190 -0.085670 -0.116576  0.506426 -0.219717 -0.041125  0.119072   \n",
       "4  0.097986 -0.134075 -0.064345  0.462825 -0.275614 -0.037747  0.049038   \n",
       "\n",
       "        416       417       418       419       420       421       422  \\\n",
       "0 -0.318931  0.009510  0.168723 -0.508360  0.119342  0.455592  0.114864   \n",
       "1 -0.414365  0.147750  0.182834 -0.569998  0.227454  0.411193  0.059790   \n",
       "2 -0.335828 -0.061126 -0.096352 -0.617940  0.419832  0.518378 -0.039191   \n",
       "3 -0.539758  0.022791  0.000199 -0.627740  0.041640  0.679750 -0.330866   \n",
       "4 -0.783268 -0.095683 -0.000269 -0.712106  0.219600  0.475336  0.037678   \n",
       "\n",
       "        423       424       425       426       427       428       429  \\\n",
       "0 -0.104670 -0.052394  0.173794 -0.036887 -0.176584  0.258948  0.020489   \n",
       "1 -0.568324 -0.017131  0.123660  0.232415  0.094009  0.175487  0.028380   \n",
       "2 -0.483218  0.064265 -0.036647 -0.110322 -0.156566  0.008641  0.043329   \n",
       "3 -0.303605  0.164967 -0.082978  0.000791 -0.290759  0.186624  0.193874   \n",
       "4 -0.488290 -0.159992  0.043705 -0.117826 -0.111004  0.059409  0.014828   \n",
       "\n",
       "        430       431       432       433       434       435       436  \\\n",
       "0 -0.154116 -0.135886  0.157079 -1.483996 -1.929655  1.082304 -0.263823   \n",
       "1 -0.485617  0.106544  0.090441 -1.039344 -1.771620  0.636875 -0.175646   \n",
       "2 -0.363142  0.092097  0.000996 -1.108861 -1.579018  0.711617 -0.332547   \n",
       "3 -0.217141  0.517328 -0.162559 -1.340829 -1.992660  1.062324 -0.206364   \n",
       "4 -0.314476  0.625367 -0.064194 -1.142129 -1.891173  0.882021 -0.113276   \n",
       "\n",
       "        437       438       439       440       441       442       443  \\\n",
       "0  0.088481 -0.150787  2.084621  0.149600  0.093464 -0.220641 -0.145591   \n",
       "1 -0.013233  0.218205  1.987606  0.102627  0.197109  0.071595  0.005534   \n",
       "2  0.187043  0.398282  1.573167  0.086047  0.259198 -0.014788  0.012401   \n",
       "3 -0.166329  0.403310  1.990638  0.146052  0.068864  0.179619  0.200503   \n",
       "4 -0.073272  0.396857  2.025402  0.216449  0.210084  0.147967  0.271568   \n",
       "\n",
       "        444       445       446       447       448       449       450  \\\n",
       "0  0.160669  3.216524  0.394509  0.042119  1.465430 -0.133346  0.297071   \n",
       "1 -0.156831  2.751392  0.377630 -0.027311  1.131103 -0.120357  0.394758   \n",
       "2 -0.018086  2.691875  0.424553 -0.285286  1.073353  0.205668  0.843496   \n",
       "3  0.149777  3.317294  0.361158 -0.146686  1.285694  0.151069  0.448719   \n",
       "4  0.094674  3.215306  0.295688 -0.189353  1.526142  0.183819  0.328140   \n",
       "\n",
       "        451       452       453       454       455       456       457  \\\n",
       "0 -0.410105 -1.875664 -1.967450  0.234505 -0.215000 -0.006932  0.034541   \n",
       "1 -0.361440 -1.504752 -2.058141 -0.205147 -0.599994 -0.011699  0.016134   \n",
       "2 -0.249535 -1.224445 -1.822779 -0.041405 -0.244548 -0.309810  0.044677   \n",
       "3  0.035179 -1.420188 -2.108225 -0.060243 -0.113239  0.062463  0.039317   \n",
       "4 -0.113133 -1.552332 -2.073508 -0.212581 -0.011197  0.176810  0.247520   \n",
       "\n",
       "        458       459       460       461       462       463       464  \\\n",
       "0  0.131632  0.053364  0.147339  0.183002 -0.291611 -0.386891 -0.199754   \n",
       "1  0.049833 -0.184557  0.057525 -0.205613 -0.124114 -0.085659  0.163440   \n",
       "2 -0.175156 -0.225520  0.340738 -0.115321 -0.127952  0.143786  0.139659   \n",
       "3 -0.192538 -0.126141  0.322284  0.013035  0.045056  0.345787 -0.001926   \n",
       "4  0.011344 -0.122759  0.092361 -0.013593  0.067065  0.211937  0.259217   \n",
       "\n",
       "        465       466       467       468       469       470       471  \\\n",
       "0 -0.120723 -0.042630 -0.073798 -0.144431 -0.067715  0.656683 -0.059617   \n",
       "1 -0.101729 -0.256462 -0.034334 -0.174804  0.395555  0.413684  0.021974   \n",
       "2 -0.197647 -0.219472 -0.315886  0.042161  0.389100 -0.005217  0.138704   \n",
       "3  0.129155 -0.331641 -0.260734  0.036931  0.335010  0.285514 -0.139525   \n",
       "4  0.237663 -0.064437 -0.224228 -0.189737  0.240411  0.385132 -0.028429   \n",
       "\n",
       "        472       473       474       475       476       477       478  \\\n",
       "0  0.201287  0.015651 -0.000267 -2.168596  0.243330  0.085551 -0.002216   \n",
       "1 -0.141392  0.359166  0.051719 -1.513369  0.264356  0.192586  0.110194   \n",
       "2 -0.182541  0.039850 -0.334530 -1.765925  0.397904 -0.150699  0.172539   \n",
       "3 -0.141440 -0.053293  0.042684 -1.738688 -0.008258 -0.119134  0.193801   \n",
       "4 -0.082523 -0.288838 -0.224598 -1.728942  0.162733 -0.017179  0.266663   \n",
       "\n",
       "        479       480       481       482       483       484       485  \\\n",
       "0  0.098574  0.123953 -0.171547 -0.316257 -0.210408  0.078679 -0.031711   \n",
       "1  0.202126  0.204930 -0.076260  0.140300 -0.265937  0.031038  0.034266   \n",
       "2 -0.140366  0.139684 -0.124165 -0.006417 -0.304291  0.260391 -0.036458   \n",
       "3  0.171490  0.015136 -0.108404  0.037943 -0.341501  0.152690  0.078154   \n",
       "4  0.036672  0.229339 -0.128911  0.066067 -0.271898  0.254672  0.181222   \n",
       "\n",
       "        486       487       488       489       490       491       492  \\\n",
       "0  0.191861 -0.265081 -0.073679 -0.140103  0.252700  0.231685 -0.068803   \n",
       "1 -0.001414  0.067865 -0.109329 -0.018298  0.438693 -0.028649 -0.264353   \n",
       "2 -0.016719  0.158128 -0.059998 -0.090386  0.286222  0.546196 -0.221657   \n",
       "3 -0.129326  0.351366 -0.150211 -0.029313 -0.023901  0.395660 -0.068190   \n",
       "4 -0.193093  0.603896  0.046412 -0.074836  0.041520  0.191176 -0.048620   \n",
       "\n",
       "        493       494       495       496       497       498       499  \\\n",
       "0  2.217354 -0.132934 -0.264120  0.372043  0.232893  0.056702 -0.252251   \n",
       "1  1.436473 -0.416519 -0.035952  0.159192 -0.059729  0.706937 -0.230198   \n",
       "2  1.450733 -0.241211 -0.110588  0.106254  0.213943  0.393615 -0.191916   \n",
       "3  2.189972 -0.343455 -0.223054  0.142225  0.112101  0.639055 -0.119655   \n",
       "4  1.910270 -0.208834 -0.071059  0.149921  0.261883  0.449044 -0.039009   \n",
       "\n",
       "        500       501       502       503       504       505       506  \\\n",
       "0 -0.175052 -0.270504 -0.036600 -0.178605 -0.002319  0.087022 -0.117753   \n",
       "1  0.058927 -0.134497 -0.001441 -0.006109 -0.131867 -0.114722 -0.357298   \n",
       "2 -0.103155 -0.245200  0.100946 -0.053682 -0.056607 -0.050480 -0.529721   \n",
       "3 -0.039461 -0.200234  0.011526  0.182408  0.077666 -0.123274 -0.560783   \n",
       "4 -0.083040 -0.277306 -0.093306 -0.056826 -0.126179 -0.112660 -0.441329   \n",
       "\n",
       "        507       508       509       510       511       512       513  \\\n",
       "0  0.218738 -0.096903  0.275393  0.604561  0.331357  0.152610 -0.763375   \n",
       "1  0.408232 -0.017827  0.090089  0.632461  0.565319  0.191118 -0.867875   \n",
       "2  0.206441 -0.173974  0.289319  0.364565  0.101875  0.491731 -0.868367   \n",
       "3  0.062814  0.046167  0.082704  0.527687  0.066244  0.094802 -0.940851   \n",
       "4  0.073218 -0.094642 -0.088253  0.452180 -0.191223  0.108963 -0.859930   \n",
       "\n",
       "        514       515       516       517       518       519       520  \\\n",
       "0  0.110329 -0.116213 -0.015277 -0.148767 -0.252354 -0.096613  0.401045   \n",
       "1  0.207628  0.123648 -0.037126 -0.260453 -0.321050  0.164743  0.344055   \n",
       "2  0.245347 -0.044390 -0.046577 -0.309987 -0.362868 -0.036630  0.331506   \n",
       "3  0.176149 -0.073755  0.078400 -0.389462 -0.271160  0.176440 -0.236872   \n",
       "4 -0.036754  0.115587  0.165090 -0.235534 -0.419043  0.132750  0.113156   \n",
       "\n",
       "        521       522       523       524       525       526       527  \\\n",
       "0 -0.020862 -0.197651 -0.107114 -0.308751 -0.054740 -0.230146  0.045499   \n",
       "1  0.087359  0.005859 -0.024404  0.024432 -0.204412 -0.026046 -0.240043   \n",
       "2 -0.020480  0.172062 -0.118994 -0.065471 -0.268840  0.018713 -0.403372   \n",
       "3  0.039546  0.279473 -0.174726 -0.192419 -0.370484 -0.145340 -0.363148   \n",
       "4  0.178455  0.177366 -0.074935  0.021362 -0.304195 -0.210215 -0.237170   \n",
       "\n",
       "        528       529       530       531       532       533       534  \\\n",
       "0 -0.159947  0.225794 -0.156760 -0.579758 -0.016450  1.404797  0.111808   \n",
       "1 -0.064821  0.183053 -0.224395 -0.376258  0.081533  1.017830  0.100157   \n",
       "2 -0.054016  0.106732 -0.108456 -0.322719  0.107584  1.112973  0.116413   \n",
       "3 -0.153669  0.072945 -0.100332 -0.411319  0.030645  1.377432  0.100426   \n",
       "4 -0.139138 -0.031268 -0.055137 -0.625930 -0.048458  1.297941  0.081248   \n",
       "\n",
       "        535       536       537       538       539       540       541  \\\n",
       "0  0.040975  1.208381  0.022359 -0.256045 -0.036632  0.121336  0.029794   \n",
       "1  0.178653  0.943344  0.143569 -0.113825  0.040710  0.314431  0.024283   \n",
       "2  0.068554  0.326928  0.072175 -0.067838  0.222880  0.370944 -0.051924   \n",
       "3 -0.108455  0.831785  0.314014 -0.263285  0.481155  0.017858 -0.090655   \n",
       "4 -0.091083  0.661218  0.298312 -0.103515  0.274794  0.036803 -0.017087   \n",
       "\n",
       "        542       543       544       545       546       547       548  \\\n",
       "0  0.034173 -0.368353  0.110052  0.257424  0.194618 -0.150148 -0.149690   \n",
       "1 -0.320023 -0.094417  0.095315  0.196241  0.189659  0.047881 -0.127979   \n",
       "2 -0.155630 -0.059293  0.114583 -0.181028  0.202403 -0.106007 -0.070647   \n",
       "3 -0.293449 -0.133164 -0.056068  0.010699  0.432246 -0.057671  0.133675   \n",
       "4 -0.204624 -0.087942  0.007562  0.189177  0.303763 -0.067535  0.120095   \n",
       "\n",
       "        549       550       551       552       553       554       555  \\\n",
       "0  0.092321  0.076165  0.303099  0.231132  0.345945 -0.165437 -0.085704   \n",
       "1 -0.229229  0.044677  0.266998  0.295038  0.322375 -0.101064  0.114283   \n",
       "2 -0.150441  0.189153  0.368452  0.173785  0.212450 -0.111362  0.119032   \n",
       "3  0.281067  0.110005  0.270878  0.236417 -0.081640 -0.153988 -0.360544   \n",
       "4  0.182622  0.364601  0.387490  0.372465 -0.138916 -0.280789 -0.220945   \n",
       "\n",
       "        556       557       558       559       560       561       562  \\\n",
       "0 -0.069272  0.213369 -0.076859  0.020564  0.252402  0.226426  0.024572   \n",
       "1 -0.107116  0.213579  0.163793  0.054643  0.082962  0.080243  0.342241   \n",
       "2 -0.173829 -0.008164  0.186362  0.149499  0.070120  0.108571  0.248999   \n",
       "3 -0.014357  0.054174  0.232930  0.011875  0.234501  0.043375  0.092088   \n",
       "4 -0.240931  0.146988  0.156036  0.097520  0.051460 -0.031399  0.277281   \n",
       "\n",
       "        563       564       565       566       567       568       569  \\\n",
       "0 -0.391727 -0.023108 -0.128336 -0.245829  0.394729  0.139937  0.088682   \n",
       "1 -0.103329 -0.087903 -0.164677 -0.130176  0.100276 -0.073228  0.117260   \n",
       "2  0.076153 -0.161569 -0.172550 -0.246436  0.362859 -0.167990  0.157734   \n",
       "3  0.003643 -0.016296 -0.178823 -0.168576  0.440691  0.261947  0.094987   \n",
       "4  0.201957  0.108890 -0.188521 -0.324693  0.512566  0.226718  0.129673   \n",
       "\n",
       "        570       571       572       573       574       575       576  \\\n",
       "0 -0.077467  0.054343 -0.262566 -0.132284  0.137676 -0.164990 -0.137680   \n",
       "1  0.080257 -0.145715  0.087322 -0.323063  0.143101 -0.176359 -0.398334   \n",
       "2  0.148326  0.149125 -0.234781  0.024343 -0.141593 -0.356290 -0.267505   \n",
       "3  0.100061  0.295376 -0.027401 -0.145291 -0.203151 -0.397400 -0.364736   \n",
       "4  0.202575  0.236635  0.139811 -0.085589 -0.218155 -0.334953 -0.232384   \n",
       "\n",
       "        577       578       579       580       581       582       583  \\\n",
       "0  0.301046  0.086288 -0.124058  0.171924 -0.223275  0.035698 -0.124004   \n",
       "1  0.261797  0.372226  0.115816  0.046890 -0.281192 -0.257780 -0.030262   \n",
       "2  0.196785  0.308629 -0.020930  0.292640 -0.231156 -0.032565 -0.210210   \n",
       "3  0.358719  0.294185 -0.078618  0.301834 -0.358973 -0.003805 -0.099479   \n",
       "4  0.227386  0.185030 -0.085552  0.177533 -0.264404 -0.122173 -0.184156   \n",
       "\n",
       "        584       585       586       587       588       589       590  \\\n",
       "0 -0.681465 -0.288272 -0.068686 -0.182465 -0.211806 -0.017527  0.143061   \n",
       "1 -0.368841 -0.087002 -0.158140 -0.133311 -0.131062 -0.172555 -0.217656   \n",
       "2 -0.163715  0.307969 -0.005841 -0.344377 -0.239263  0.033553  0.267427   \n",
       "3 -0.415659  0.295500  0.144251 -0.282019 -0.171681 -0.050986 -0.038448   \n",
       "4 -0.263314  0.251593  0.169985 -0.544212 -0.139173  0.070388 -0.002895   \n",
       "\n",
       "        591       592       593       594       595       596       597  \\\n",
       "0  0.360638  0.385750  0.120876  0.146029  0.455177  0.031480 -0.157944   \n",
       "1  0.278188  0.030150  0.142261  0.224161  0.316851  0.135279  0.030255   \n",
       "2  0.293433 -0.172587  0.042097 -0.185313  0.436783  0.066347 -0.068619   \n",
       "3  0.097281 -0.412630 -0.085567  0.012304  0.352698  0.323416 -0.052376   \n",
       "4  0.378906 -0.446852  0.099065 -0.002701  0.122351  0.299040 -0.092147   \n",
       "\n",
       "        598       599       600       601       602       603       604  \\\n",
       "0  0.014069  0.117251 -0.098581  0.153611  0.066878 -0.095238 -0.047519   \n",
       "1  0.156185  0.227636  0.292328  0.004565  0.195537  0.036159 -0.276614   \n",
       "2  0.033096  0.076562  0.384875 -0.055000  0.255780 -0.181817  0.004807   \n",
       "3  0.005428 -0.251748  0.696412  0.115743  0.291411 -0.171567  0.122354   \n",
       "4  0.008964 -0.231184  0.647902  0.125160  0.009350 -0.119875  0.001248   \n",
       "\n",
       "        605       606       607       608       609       610       611  \\\n",
       "0 -0.058652  0.204454  0.212656 -0.146449 -0.052718 -0.051662  0.239247   \n",
       "1 -0.207048  0.166374  0.228283 -0.568953 -0.030577  0.187095  0.398820   \n",
       "2 -0.008539  0.151714  0.221216 -0.662532  0.304952  0.109765  0.005974   \n",
       "3 -0.002959  0.080790  0.001512 -0.503203  0.256098 -0.199504  0.117559   \n",
       "4 -0.073823  0.136381  0.214499 -0.551401  0.061540  0.006809  0.003467   \n",
       "\n",
       "        612       613       614       615       616       617       618  \\\n",
       "0  0.109455  0.389533  0.144415 -2.333261 -0.317807  0.222099  0.255289   \n",
       "1 -0.276514  0.068632  0.116219 -2.113282  0.119141 -0.041259  0.180413   \n",
       "2 -0.024596  0.188517  0.271330 -1.862894  0.215602 -0.254758 -0.077060   \n",
       "3 -0.108211  0.475217  0.164117 -2.055612 -0.021256 -0.218793 -0.032524   \n",
       "4 -0.106861  0.515798  0.136783 -1.922994 -0.031434 -0.381685 -0.001128   \n",
       "\n",
       "        619       620       621       622       623       624       625  \\\n",
       "0 -0.029469  0.230554  0.694520 -0.193432 -0.095616 -0.238705  0.110119   \n",
       "1 -0.061698  0.361656  0.714611  0.002112 -0.058756 -0.228537  0.046178   \n",
       "2 -0.153325  0.319688  0.880024 -0.153273  0.016548 -0.119517  0.037971   \n",
       "3 -0.242874  0.127543  0.775446 -0.093631 -0.125298 -0.050492  0.036180   \n",
       "4  0.012976  0.304362  0.897940 -0.194853 -0.167243  0.030169  0.170018   \n",
       "\n",
       "        626       627       628       629       630       631       632  \\\n",
       "0 -0.050350  0.008782  0.101708 -0.483683 -0.019196  0.287094 -0.011222   \n",
       "1 -0.108297  0.047795 -0.139194 -0.462275 -0.130494 -0.243649  0.220655   \n",
       "2 -0.178994 -0.146242 -0.012068 -0.522181 -0.205068  0.133247  0.100120   \n",
       "3 -0.066783 -0.062866 -0.018110 -0.731210  0.102162  0.135387  0.188749   \n",
       "4 -0.008709 -0.019636 -0.240010 -0.566145 -0.077604  0.110287  0.081361   \n",
       "\n",
       "        633       634       635       636       637       638       639  \\\n",
       "0 -0.136490 -0.056817 -1.835776 -0.228524 -0.072501  0.066729 -0.063621   \n",
       "1 -0.339361  0.073952 -1.444949 -0.447369 -0.151190  0.221576  0.178028   \n",
       "2 -0.156359  0.187195 -1.311999 -0.348073  0.045656  0.184526 -0.422370   \n",
       "3 -0.159923 -0.025772 -1.764593 -0.488105 -0.078763 -0.199315 -0.439530   \n",
       "4 -0.184272  0.011819 -1.533955 -0.435302  0.005969 -0.138828 -0.387774   \n",
       "\n",
       "        640       641       642       643       644       645       646  \\\n",
       "0  0.012999 -0.096271 -0.187056 -0.086765  0.066112 -0.191554 -0.213342   \n",
       "1 -0.024075 -0.097401 -0.086004 -0.504357  0.056902 -0.103230 -0.195016   \n",
       "2  0.141621  0.038451 -0.151638 -0.414896  0.235821  0.025449 -0.072472   \n",
       "3  0.041473 -0.126680 -0.145141 -0.341191  0.401582  0.100638 -0.227045   \n",
       "4  0.017681 -0.105501 -0.375121 -0.403055  0.223763 -0.186897 -0.337892   \n",
       "\n",
       "        647       648       649       650       651       652       653  \\\n",
       "0 -0.010719 -0.192471  0.234207  0.094020 -0.042235 -0.058626 -0.110245   \n",
       "1  0.216620 -0.432201  0.073675 -0.058388 -0.300429  0.004103 -0.107911   \n",
       "2  0.404454 -0.770902  0.025708  0.185872 -0.447320  0.174181 -0.056535   \n",
       "3  0.183787 -0.330814  0.247868 -0.079385 -0.120809  0.059129 -0.020011   \n",
       "4  0.330360 -0.319272  0.347890 -0.231664  0.022066 -0.104849  0.125927   \n",
       "\n",
       "        654       655       656       657       658       659       660  \\\n",
       "0 -0.129554  0.059742  0.274113 -0.104256  0.148161  0.265457  0.165809   \n",
       "1  0.018969 -0.120935 -0.223063 -0.339355 -0.006361  0.262837  0.363263   \n",
       "2  0.186204 -0.369701 -0.289970 -0.019157 -0.112147  0.138150  0.231469   \n",
       "3  0.309639 -0.034716 -0.144897 -0.094473  0.058416 -0.020123  0.252113   \n",
       "4  0.353665 -0.343490  0.007636 -0.074666 -0.005430  0.050625  0.230290   \n",
       "\n",
       "        661       662       663       664       665       666       667  \\\n",
       "0  0.107775  0.040481 -0.212443  0.078095 -1.782096 -0.026157 -0.226626   \n",
       "1  0.011260  0.178856 -0.174584  0.103692 -1.399578 -0.114160 -0.117055   \n",
       "2  0.009302  0.259129 -0.213399 -0.047027 -1.368948  0.032279 -0.173145   \n",
       "3  0.183506  0.047585 -0.025251 -0.002996 -1.495259  0.169124  0.022396   \n",
       "4  0.296979 -0.239275 -0.387272 -0.087447 -1.385870  0.164165  0.086118   \n",
       "\n",
       "        668       669       670       671       672       673       674  \\\n",
       "0  0.102626 -0.115495  0.192710  0.002623 -0.160434  0.253892  0.322352   \n",
       "1  0.166249 -0.172455 -0.004891 -0.016554 -0.024926  0.182688  0.155108   \n",
       "2  0.466664 -0.239498 -0.049627  0.105536  0.017686  0.041485  0.133470   \n",
       "3  0.485362 -0.048588  0.171053 -0.012018 -0.043289 -0.300735  0.224916   \n",
       "4  0.477693 -0.154805  0.204899 -0.017503  0.207635 -0.267387  0.163694   \n",
       "\n",
       "        675       676       677       678       679       680       681  \\\n",
       "0 -0.386042  0.015917  0.194733  0.058368 -0.090867 -0.101546  0.386789   \n",
       "1 -0.163391 -0.100343  0.173562 -0.032489 -0.094124 -0.132808  0.472829   \n",
       "2 -0.408488 -0.442951  0.508196  0.066863 -0.092350 -0.116701  0.354941   \n",
       "3 -0.472905  0.105496  0.355393  0.239578  0.123667 -0.243022  0.318586   \n",
       "4 -0.361353  0.104494  0.257115  0.249953  0.208703 -0.311522  0.287231   \n",
       "\n",
       "        682       683       684       685       686       687       688  \\\n",
       "0  0.093716 -0.267528  0.145146  0.049296 -0.096692  0.502369 -0.192930   \n",
       "1  0.166019 -0.081053  0.213492  0.196840  0.109788  0.223372 -0.336534   \n",
       "2  0.053406 -0.021454  0.059903  0.178708  0.090702  0.420581 -0.192488   \n",
       "3 -0.003279 -0.175605 -0.406291  0.064134  0.103295  0.720933 -0.547663   \n",
       "4  0.194591 -0.075470 -0.236217  0.242925 -0.163116  0.624239 -0.419350   \n",
       "\n",
       "        689       690       691       692       693       694       695  \\\n",
       "0  0.034642  0.422873  0.265714  0.146159 -0.319094  0.222661  0.161907   \n",
       "1  0.187136  0.284269 -0.018717  0.026683 -0.181287  0.421302  0.170856   \n",
       "2  0.328129  0.514543  0.030183  0.288722 -0.018220  0.350887  0.449416   \n",
       "3  0.106099  0.472680  0.160865  0.049538 -0.133090  0.154398  0.201352   \n",
       "4  0.202040  0.466183  0.207372 -0.090815 -0.076450 -0.013476  0.323429   \n",
       "\n",
       "        696       697       698       699       700       701       702  \\\n",
       "0 -1.055999  0.007846  0.101913 -0.295660 -0.045099 -0.244384  0.333828   \n",
       "1 -0.858100  0.060576 -0.146746 -0.012470  0.064857 -0.095487  0.155275   \n",
       "2 -0.645665  0.190850 -0.021356  0.035417  0.097439 -0.039108  0.280523   \n",
       "3 -0.889775  0.327003  0.033826  0.009326  0.173388 -0.118219  0.342384   \n",
       "4 -0.867128  0.113595  0.141242 -0.095396  0.317765 -0.278106  0.396642   \n",
       "\n",
       "        703       704       705       706       707       708       709  \\\n",
       "0  0.459999  0.292852 -0.083219  0.515633  1.913599 -0.029188 -0.211715   \n",
       "1  0.096208  0.568612 -0.117829  0.487051  1.664372  0.268089  0.060689   \n",
       "2  0.059826  0.297179 -0.191275  0.553749  1.451620 -0.077626  0.159301   \n",
       "3  0.127484  0.484244 -0.219989  0.624424  1.821531  0.083916  0.074862   \n",
       "4  0.261178  0.354749 -0.337213  0.736657  1.748621  0.192067 -0.009619   \n",
       "\n",
       "        710       711       712       713       714       715       716  \\\n",
       "0 -0.010307  0.258732  0.023835  0.310256 -0.192884  1.750768 -0.040066   \n",
       "1  0.253742  0.154707  0.001059 -0.268022 -0.328705  1.512110 -0.157075   \n",
       "2  0.465388 -0.026384 -0.013242 -0.275869 -0.571373  1.513658 -0.235480   \n",
       "3 -0.006789  0.227202 -0.120915 -0.423679 -0.626222  1.776622 -0.138687   \n",
       "4  0.279778  0.283791 -0.343024 -0.040708 -0.716915  1.739474 -0.121069   \n",
       "\n",
       "        717       718       719       720       721       722       723  \\\n",
       "0 -0.166193 -0.322148  0.147801  0.167517  1.832676  0.259535  0.001316   \n",
       "1 -0.032671 -0.164188  0.480546  0.047024  1.439537  0.091194  0.193451   \n",
       "2 -0.240460 -0.165990  0.670881  0.133347  1.429063 -0.025801  0.221670   \n",
       "3 -0.143847 -0.128546  0.542090  0.064392  1.814694 -0.222461  0.185527   \n",
       "4 -0.197947 -0.331439  0.326331  0.093071  1.605037 -0.059501  0.090526   \n",
       "\n",
       "        724       725       726       727       728       729       730  \\\n",
       "0  0.121710  0.106218 -0.111246  0.175421  0.027898 -0.058007  0.013651   \n",
       "1 -0.224989  0.241381 -0.017364  0.445531  0.124944 -0.017264  0.024864   \n",
       "2 -0.127355  0.117763  0.378035  0.345593 -0.082650 -0.196601  0.065567   \n",
       "3  0.147876 -0.015208  0.267573  0.289887  0.062728 -0.283036 -0.168815   \n",
       "4 -0.034893  0.041936 -0.044364  0.358782 -0.168617 -0.243071 -0.021190   \n",
       "\n",
       "        731       732       733       734       735       736       737  \\\n",
       "0 -1.100867  0.038646 -0.059722  0.010678 -0.131453 -0.741544  0.613290   \n",
       "1 -1.381781 -0.134459  0.018743  0.351040  0.057493 -0.482758  0.404899   \n",
       "2 -1.515087  0.034690  0.186859  0.153084 -0.084930 -0.197776  0.441878   \n",
       "3 -0.939124 -0.136199  0.272463  0.414165  0.010418 -0.085339  0.256143   \n",
       "4 -0.913893 -0.228516  0.380395  0.543660  0.173594 -0.251597  0.144921   \n",
       "\n",
       "        738       739       740       741       742       743       744  \\\n",
       "0  0.141397 -0.180642  0.241143  0.359595 -0.112447  0.081373  0.075066   \n",
       "1 -0.045970 -0.130372  0.065098  0.099730 -0.543596  0.153142 -0.132317   \n",
       "2 -0.428810  0.024955 -0.056936 -0.217087 -0.544300  0.316671  0.039623   \n",
       "3 -0.101246 -0.149911 -0.089261 -0.003373 -0.333065  0.329606  0.159369   \n",
       "4 -0.185885 -0.031317 -0.159127 -0.084391 -0.335865  0.321858  0.203519   \n",
       "\n",
       "        745       746       747       748       749       750       751  \\\n",
       "0 -0.066500 -0.070188  0.090865 -0.291487 -0.171487  0.192506 -0.285695   \n",
       "1  0.248659 -0.178446  0.072395 -0.323997  0.151082 -0.027633  0.117040   \n",
       "2  0.168404 -0.297815 -0.111745 -0.261690 -0.002555  0.132198  0.130110   \n",
       "3  0.105106  0.136789 -0.185530 -0.109052  0.068948  0.058279 -0.078391   \n",
       "4  0.108051 -0.087100 -0.111201 -0.227971  0.101217 -0.015857  0.060778   \n",
       "\n",
       "        752       753       754       755       756       757       758  \\\n",
       "0  0.516094 -0.241200 -0.054998  0.609387  0.204573  0.157941  0.254089   \n",
       "1  0.360472 -0.200037  0.166041  0.654701  0.050500  0.113429  0.114728   \n",
       "2  0.299142 -0.296408  0.137298  0.827499 -0.239577  0.291484  0.062325   \n",
       "3  0.644802 -0.419815 -0.091957  0.794983 -0.016139 -0.033586  0.333985   \n",
       "4  0.529199 -0.160367 -0.153159  0.777417 -0.200691  0.036016  0.215525   \n",
       "\n",
       "        759       760       761       762       763       764       765  \\\n",
       "0 -0.092161 -0.177658 -0.322701  0.016352 -0.007543  0.031331  0.462816   \n",
       "1 -0.077202 -0.269712 -0.151349  0.033660  0.276272  0.065734  0.664379   \n",
       "2 -0.218971 -0.567088 -0.617004  0.170469  0.163738  0.165489  0.253347   \n",
       "3 -0.193174 -0.613174 -0.672665  0.012190  0.253517  0.135646  0.110566   \n",
       "4 -0.281756 -0.346513 -0.911152 -0.005457  0.121276  0.145280  0.193568   \n",
       "\n",
       "        766       767  \n",
       "0  0.165087 -0.072143  \n",
       "1 -0.040229 -0.356698  \n",
       "2  0.105800 -0.174792  \n",
       "3  0.269420 -0.121226  \n",
       "4  0.342988  0.069493  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "bert_embeddings_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined_features_train = bert_embeddings_df_train.drop(['suspended'], axis=1)\n",
    "acct_features_train = bert_embeddings_df_train.drop(['suspended'], axis=1).iloc[:,0:bert_embeddings_df_train.drop(['suspended'], axis=1).columns.get_loc('0')]\n",
    "text_feature_train = bert_embeddings_df_train.iloc[:, bert_embeddings_df_train.drop(['suspended'], axis=1).columns.get_loc('0') + 1:]\n",
    "labels_train = bert_embeddings_df_train['suspended']\n",
    "\n",
    "#bert_embeddings_df_train.drop(['suspended'], axis=1).columns.get_loc(0)\n",
    "\n",
    "#gets country label number\n",
    "num_of_labels=labels_train.nunique()\n",
    "num_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Use Random Forest Feature Selection for Account Features \n",
    "\n",
    "I specify the random forest instance, indicating the number of trees. Then I use selectFromModel object from sklearn to automatically select the features.\n",
    "\n",
    "To note, this is much faster than LASSO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check to ensure no missingness ahead of random forest feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              0\n",
       "created_at                      0\n",
       "retweet_count                1237\n",
       "favorite_count               1237\n",
       "quoted_status_id                0\n",
       "                             ... \n",
       "user.protected_False            0\n",
       "user.verified_False.1           0\n",
       "user.default_profile_True       0\n",
       "is_quote_status_False           0\n",
       "is_quote_status_True            0\n",
       "Length: 160, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "acct_features_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_train = acct_features_train.replace([np.inf, -np.inf], np.nan)\n",
    "acct_features_train['retweet_count'] = acct_features_train['retweet_count'].interpolate()\n",
    "acct_features_train['favorite_count'] = acct_features_train['favorite_count'].interpolate()\n",
    "acct_features_train['since_last_tweet_mins'] = acct_features_train['since_last_tweet_mins'].interpolate()\n",
    "acct_features_train['since_last_tweet_mins_min'] = acct_features_train['since_last_tweet_mins_min'].interpolate()\n",
    "acct_features_train['since_last_tweet_mins_max'] = acct_features_train['since_last_tweet_mins_max'].interpolate()\n",
    "acct_features_train['since_last_tweet_mins_mean'] = acct_features_train['since_last_tweet_mins_mean'].interpolate()\n",
    "acct_features_train['followers_per_followees'] = acct_features_train['followers_per_followees'].interpolate()\n",
    "acct_features_train['user.followers_countdailychange'] = acct_features_train['user.followers_countdailychange'].interpolate()\n",
    "acct_features_train['user.friends_countdailychange'] = acct_features_train['user.friends_countdailychange'].interpolate()\n",
    "acct_features_train['user.friend_rate'] = acct_features_train['user.friend_rate'].interpolate()\n",
    "acct_features_train['user.followers_rate'] = acct_features_train['user.followers_rate'].interpolate()\n",
    "acct_features_train['user.friend_rate'] = acct_features_train['user.friend_rate'].interpolate()\n",
    "acct_features_train['tweets_per_day'] = acct_features_train['tweets_per_day'].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random seed\n",
    "def random_seeds(seed):\n",
    "   os.environ['PYTHONHASHSEED']=str(seed)\n",
    "   tf.random.set_seed(seed)\n",
    "   np.random.seed(seed)\n",
    "   rn.seed(seed)\n",
    "\n",
    "random_seeds(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier())"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "sel.fit(acct_features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which features are important we can use get_support method on the fitted model. It will return an array of boolean values. True for the features whose importance is greater than the mean importance and False for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat= acct_features_train.columns[(sel.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'created_at', 'user.id', 'user.created_at',\n",
       "       'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
       "       'user.statuses_count', 'user_age', 'tweets_per_day',\n",
       "       'since_last_tweet_mins_min', 'since_last_tweet_mins_max',\n",
       "       'since_last_tweet_mins_mean', 'avg_tweets_per_hr', 'avg_tweets_per_day',\n",
       "       'followers_per_followees', 'user.urls_per_tweet',\n",
       "       'no_hashtags_per_tweet', 'no_mentions_per_tweet', 'no_urls_per_tweet',\n",
       "       'user.followers_countdailychange', 'user.friends_countdailychange',\n",
       "       'user.friend_rate', 'user.followers_rate', 'user.has_location',\n",
       "       'user.screen_name.digit_length', 'user.screen_name.length',\n",
       "       'source_Twitter Web App', 'source_Twitter for Android',\n",
       "       'source_Twitter for iPhone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pd.series(sel.estimator_,feature_importances_,.ravel()).hist()\n",
    "pd.Series(sel.estimator_.feature_importances_.ravel()).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the account features dataset to include only important vars as determined by RF feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_train = acct_features_train[selected_feat]\n",
    "combined_features_train = pd.concat([acct_features_train, text_feature_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Lasso RL1 egularization (with Random Forest classification) method for feature selection \n",
    "\n",
    "Chose to use RF, but can come back to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                             fit_intercept=True,\n",
       "                                             intercept_scaling=1, l1_ratio=None,\n",
       "                                             max_iter=100, multi_class='auto',\n",
       "                                             n_jobs=None, penalty='l1',\n",
       "                                             random_state=None,\n",
       "                                             solver='liblinear', tol=0.0001,\n",
       "                                             verbose=0, warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(acct_features_train.fillna(0))\n",
    "\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear')) # Requires two classes to work of course. \n",
    "sel_.fit(scaler.transform(acct_features_train.fillna(0)), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 160\n",
      "selected features: 30\n",
      "features with coefficients shrank to zero: 23\n"
     ]
    }
   ],
   "source": [
    "selected_feat= acct_features_train.columns[(sel.get_support())]\n",
    "\n",
    "print('total features: {}'.format((acct_features_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_BIGO LIVE', 'source_Paper.li', 'source_TeamSight Publisher',\n",
       "       'source_WShare', 'source_WordPress.com', 'lang_am', 'lang_bo',\n",
       "       'lang_ckb', 'lang_gu', 'lang_km', 'lang_ml', 'lang_si', 'lang_uk',\n",
       "       'lang_und', 'withheld_in_countries_['TR']', 'place.country_Mongolia',\n",
       "       'place.country_Republic of Korea', 'place.country_Russia',\n",
       "       'user.verified_False', 'user.protected_False', 'user.verified_False.1',\n",
       "       'user.default_profile_True', 'is_quote_status_False'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_feats = acct_features_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset account features based on LASSO-selected important features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_train = sel_.transform(acct_features_train.fillna(0))\n",
    "acct_features_valid = sel_.transform(acct_features_valid.fillna(0))\n",
    "acct_features_test = sel_.transform(acct_features_test.fillna(0))\n",
    "acct_features_train.shape, acct_features_valid.shape, acct_features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume after feature selection\n",
    "\n",
    "Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined_features_valid = bert_embeddings_df_valid.drop(['suspended'], axis=1)\n",
    "acct_features_valid = bert_embeddings_df_valid.drop(['suspended'], axis=1).iloc[:,0:bert_embeddings_df_valid.drop(['suspended'], axis=1).columns.get_loc('0')]\n",
    "text_feature_valid = bert_embeddings_df_valid.iloc[:, bert_embeddings_df_valid.drop(['suspended'], axis=1).columns.get_loc('0') + 1:]\n",
    "labels_valid = bert_embeddings_df_valid['suspended']\n",
    "\n",
    "#gets country label number\n",
    "num_of_labels_valid=labels_valid.nunique()\n",
    "num_of_labels_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_valid = acct_features_valid[selected_feat]\n",
    "combined_features_valid = pd.concat([acct_features_valid, text_feature_valid], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features_test = bert_embeddings_df_test.drop(['suspended'], axis=1)\n",
    "acct_features_test = bert_embeddings_df_test.drop(['suspended'], axis=1).iloc[:,0:bert_embeddings_df_test.drop(['suspended'], axis=1).columns.get_loc('0')]\n",
    "text_feature_test = bert_embeddings_df_test.iloc[:, bert_embeddings_df_test.drop(['suspended'], axis=1).columns.get_loc('0') + 1:]\n",
    "labels_test = bert_embeddings_df_test['suspended']\n",
    "\n",
    "#gets country label number\n",
    "num_of_labels_test=labels_test.nunique()\n",
    "num_of_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_test = acct_features_test[selected_feat]\n",
    "combined_features_test = pd.concat([acct_features_test, text_feature_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate class weights to address class imbalance problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0:0, 1:1}\n",
    "\n",
    "def data_prep(dataset):\n",
    "    y = []\n",
    "    for i in range(0,len(dataset)):\n",
    "        y_val = np.zeros(2)\n",
    "        y_val[mapping[dataset[i]]] = 1\n",
    "        y.append(y_val)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_fw = data_prep(labels_train)\n",
    "len(labels_train_fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [0, 0]\n",
    "for el in labels_train_fw :\n",
    "    class_counts[np.argmax(el)]+=1\n",
    "class_weights = {idx:sum(class_counts)/el for idx, el in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding (Have not had work yet, but may on full data)\n",
    "def label_encoding(labels):\n",
    "    LE = LabelEncoder()\n",
    "    fit=LE.fit(labels)\n",
    "    labels =fit.transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test label_encoding\n",
    "labels_train, fit_train=label_encoding(labels_train)\n",
    "labels_valid, fit_valid=label_encoding(labels_valid)\n",
    "labels_test, fit_test=label_encoding(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Only Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure numpy array\n",
    "# X\n",
    "train_data_tweettext = np.array(text_feature_train)\n",
    "valid_data_tweettext = np.array(text_feature_valid)\n",
    "test_data_tweettext = np.array(text_feature_test)\n",
    "\n",
    "# Y\n",
    "train_labels_tweettext = labels_train\n",
    "valid_labels_tweettext = labels_valid\n",
    "test_labels_tweettext = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input shape for tweet embeddings\n",
    "input_shape_tweettext=train_data_tweettext[0].shape\n",
    "input_shape_tweettext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random seed\n",
    "def random_seeds(seed):\n",
    "   os.environ['PYTHONHASHSEED']=str(seed)\n",
    "   tf.random.set_seed(seed)\n",
    "   np.random.seed(seed)\n",
    "   rn.seed(seed)\n",
    "\n",
    "#set seed\n",
    "random_seeds(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for model flow. Does not include any parameter tuning\n",
    "def model_flow(model_name, num_countries, input_shape):\n",
    "    inputs = keras.Input(shape=(input_shape), name=\"Combined_inputs\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(16, activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    outputs = layers.Dense(num_of_labels, activation=\"softmax\",name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Tweet_text\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Combined_inputs (InputLayer) [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "normalization_1 (BatchNormal (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 52,114\n",
      "Trainable params: 51,986\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model flow and summary\n",
    "Tweettext_model = model_flow(\"Tweet_text\",num_of_labels, input_shape_tweettext)\n",
    "Tweettext_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile  model\n",
    "Tweettext_model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on Tweet Text training data\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 19s 2ms/step - loss: 1.3542 - accuracy: 0.5712 - val_loss: 0.5589 - val_accuracy: 0.7623\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2909 - accuracy: 0.6360 - val_loss: 0.6911 - val_accuracy: 0.5709\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2759 - accuracy: 0.6483 - val_loss: 0.7406 - val_accuracy: 0.4818\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2360 - accuracy: 0.6665 - val_loss: 0.6667 - val_accuracy: 0.5972\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2357 - accuracy: 0.6612 - val_loss: 0.5952 - val_accuracy: 0.7256\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 12s 2ms/step - loss: 1.2074 - accuracy: 0.6919 - val_loss: 0.6986 - val_accuracy: 0.6033\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.1874 - accuracy: 0.6887 - val_loss: 0.7109 - val_accuracy: 0.5794\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.1828 - accuracy: 0.6864 - val_loss: 0.6430 - val_accuracy: 0.6561\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.1756 - accuracy: 0.6836 - val_loss: 0.6188 - val_accuracy: 0.6807\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.1604 - accuracy: 0.6959 - val_loss: 0.6633 - val_accuracy: 0.6207\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on Tweet Text training data\")\n",
    "history_tweettext = Tweettext_model.fit(train_data_tweettext, labels_train, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_tweettext, labels_valid), class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Tweet Text test data\n",
      "2914/2914 [==============================] - 3s 1ms/step - loss: 0.6768 - accuracy: 0.6048\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on Tweet Text test data\")\n",
    "Tweettext_model_results = Tweettext_model.evaluate(test_data_tweettext,labels_test, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweettext_model.save('Tweettext_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "client = boto3.client('s3')\n",
    "s3.upload_file(Filename='Tweettext_model.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Tweettext_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load back in model \n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Tweettext_model.h5',\n",
    "                     'Tweettext_model.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Tweettext_model = load_model('Tweettext_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (location probabilities)\n",
    "def predict_test(model,test_data, test_labels, fit):\n",
    "    predictions = model.predict(test_data)\n",
    "    #y_true=np.argmax(test_labels, axis=0) \n",
    "    #y_pred=np.argmax(predictions, axis =0)\n",
    "    #get labels of prediction\n",
    "    #label_pred=fit.inverse_transform(y_pred)\n",
    "    #metrics\n",
    "    report = classification_report(np.argmax(test_labels, axis=1), np.argmax(predictions, axis=1))\n",
    "    return report # label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.63      0.74     26238\n",
      "           1       0.10      0.38      0.16      2901\n",
      "\n",
      "    accuracy                           0.60     29139\n",
      "   macro avg       0.50      0.51      0.45     29139\n",
      "weighted avg       0.82      0.60      0.68     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_tweettext=predict_test(Tweettext_model,test_data_tweettext,labels_test, fit_test)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_tweettext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summed probabilities at the account level \n",
    "def predict_account(model, df_mod, df_bert, df_full):\n",
    "    predictions = model.predict(df_mod)[:,1]\n",
    "    df = pd.DataFrame(predictions, columns = ['pred'])\n",
    "    df_2 = pd.merge(df_bert['user.id'], df, left_index=True, right_index=True)\n",
    "    df_fm = df_full[['user.screen_name','user.id', 'suspended']].drop_duplicates(subset=['user.screen_name','user.id'])\n",
    "    df_3 = pd.merge(df_fm, df_2, left_on='user.id', right_on='user.id')\n",
    "    df_4 = df_3[['user.screen_name', 'suspended','pred']].groupby('user.screen_name').agg(['sum', 'mean'])\n",
    "    df_4.drop(('suspended', 'sum'), axis = 1, inplace = True)\n",
    "    df_4.columns = df_4.columns = [' '.join(col).strip() for col in df_4.columns.values]\n",
    "    df_4.reset_index(level=0, inplace=True)\n",
    "    df_4.columns = [\"user.screen_name\",\"suspended_label\", \"total_pre_prob\",\"mean_pred_prob\"]\n",
    "    df_4['pred_class'] = np.where(df_4['mean_pred_prob'] > 0.5, 1,0) # Double check to make sure it's .5\n",
    "\n",
    "    return df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext = predict_account(Tweettext_model, train_data_tweettext, bert_embeddings_df_train, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_tweettext.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.80      1440\n",
      "           1       0.16      0.83      0.27       110\n",
      "\n",
      "    accuracy                           0.69      1550\n",
      "   macro avg       0.57      0.75      0.54      1550\n",
      "weighted avg       0.92      0.69      0.77      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_tweettext = classification_report(np.array(train_account_preds_tweettext['suspended_label']), np.array(train_account_preds_tweettext['pred_class']))\n",
    "print(report_tr_tweettext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext = predict_account(Tweettext_model, valid_data_tweettext, bert_embeddings_df_valid, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_tweettext.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74       523\n",
      "           1       0.05      0.24      0.08        41\n",
      "\n",
      "    accuracy                           0.59       564\n",
      "   macro avg       0.48      0.43      0.41       564\n",
      "weighted avg       0.85      0.59      0.69       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_tweettext = classification_report(np.array(valid_account_preds_tweettext['suspended_label']), np.array(valid_account_preds_tweettext['pred_class']))\n",
    "print(report_v_tweettext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext = predict_account(Tweettext_model, test_data_tweettext, bert_embeddings_df_test, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_tweettext.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.63      0.75       467\n",
      "           1       0.10      0.43      0.16        44\n",
      "\n",
      "    accuracy                           0.62       511\n",
      "   macro avg       0.51      0.53      0.46       511\n",
      "weighted avg       0.85      0.62      0.70       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_tweettext = classification_report(np.array(test_account_preds_tweettext['suspended_label']), np.array(test_account_preds_tweettext['pred_class']))\n",
    "print(report_te_tweettext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet Text Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plot(title_, history_fit):\n",
    "    plt.plot(history_fit.history['accuracy'])\n",
    "    plt.plot(history_fit.history['val_accuracy'])\n",
    "    plt.title(title_)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [0.6767732501029968, 0.6047908067703247]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.63      0.74     26238\n",
      "           1       0.10      0.38      0.16      2901\n",
      "\n",
      "    accuracy                           0.60     29139\n",
      "   macro avg       0.50      0.51      0.45     29139\n",
      "weighted avg       0.82      0.60      0.68     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "plot_tweettext=accuracy_plot('Model accuracy of Tweet Text Model', history_tweettext)\n",
    "print(plot_tweettext)\n",
    "\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Tweettext_model_results)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_tweettext)\n",
    "#Classification report\n",
    "print(metrics_report_tweettext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Only Model with LabSe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prep Data to Convert to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>user.listed_count</th>\n",
       "      <th>user.statuses_count</th>\n",
       "      <th>quoted_status.user.followers_count</th>\n",
       "      <th>quoted_status.user.friends_count</th>\n",
       "      <th>retweeted_status.user.followers_count</th>\n",
       "      <th>retweeted_status.user.friends_count</th>\n",
       "      <th>user_age</th>\n",
       "      <th>tweets_per_day</th>\n",
       "      <th>since_last_tweet_mins</th>\n",
       "      <th>since_last_tweet_mins_min</th>\n",
       "      <th>since_last_tweet_mins_max</th>\n",
       "      <th>since_last_tweet_mins_mean</th>\n",
       "      <th>avg_tweets_per_hr</th>\n",
       "      <th>avg_tweets_per_day</th>\n",
       "      <th>no_hashtags</th>\n",
       "      <th>no_mentions</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>tw_len</th>\n",
       "      <th>followers_per_followees</th>\n",
       "      <th>containsURL</th>\n",
       "      <th>user.urls_per_tweet</th>\n",
       "      <th>no_hashtags_per_tweet</th>\n",
       "      <th>no_mentions_per_tweet</th>\n",
       "      <th>no_urls_per_tweet</th>\n",
       "      <th>user.followers_countdailychange</th>\n",
       "      <th>user.friends_countdailychange</th>\n",
       "      <th>user.friend_rate</th>\n",
       "      <th>user.followers_rate</th>\n",
       "      <th>user.has_url</th>\n",
       "      <th>user.has_location</th>\n",
       "      <th>user.screen_name.digit_length</th>\n",
       "      <th>user.screen_name.length</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>suspended</th>\n",
       "      <th>source_        Round   Year Fun!</th>\n",
       "      <th>source_      Round      Year   Fum</th>\n",
       "      <th>source_     Round    Year Fum</th>\n",
       "      <th>source_  Round     Year Fum</th>\n",
       "      <th>source_ Round      Year   Fum</th>\n",
       "      <th>source_ 「モンストからツイート」</th>\n",
       "      <th>source_Affinitweet.com</th>\n",
       "      <th>source_BIGO LIVE</th>\n",
       "      <th>source_Blog2Social APP</th>\n",
       "      <th>source_CallApp</th>\n",
       "      <th>source_Etsy</th>\n",
       "      <th>source_Instagram</th>\n",
       "      <th>source_Joinfsocial</th>\n",
       "      <th>source_Mobile Web (M2)</th>\n",
       "      <th>source_Nintendo Switch Share</th>\n",
       "      <th>source_Paper.li</th>\n",
       "      <th>source_Peing</th>\n",
       "      <th>source_SocialPilot.co</th>\n",
       "      <th>source_TeamSight Publisher</th>\n",
       "      <th>source_TweetDeck</th>\n",
       "      <th>source_Twibbon</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter Web Client</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for Mac</th>\n",
       "      <th>source_Twitter for iPad</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "      <th>source_WShare</th>\n",
       "      <th>source_WordPress.com</th>\n",
       "      <th>source_漫威超級戰爭（MARVEL Super War）</th>\n",
       "      <th>lang_False</th>\n",
       "      <th>lang_am</th>\n",
       "      <th>lang_ar</th>\n",
       "      <th>lang_bg</th>\n",
       "      <th>lang_bn</th>\n",
       "      <th>lang_bo</th>\n",
       "      <th>lang_ca</th>\n",
       "      <th>lang_ckb</th>\n",
       "      <th>lang_cs</th>\n",
       "      <th>lang_cy</th>\n",
       "      <th>lang_da</th>\n",
       "      <th>lang_de</th>\n",
       "      <th>lang_el</th>\n",
       "      <th>lang_en</th>\n",
       "      <th>lang_es</th>\n",
       "      <th>lang_et</th>\n",
       "      <th>lang_eu</th>\n",
       "      <th>lang_fa</th>\n",
       "      <th>lang_fi</th>\n",
       "      <th>lang_fr</th>\n",
       "      <th>lang_gu</th>\n",
       "      <th>lang_hi</th>\n",
       "      <th>lang_ht</th>\n",
       "      <th>lang_hu</th>\n",
       "      <th>lang_hy</th>\n",
       "      <th>lang_in</th>\n",
       "      <th>lang_is</th>\n",
       "      <th>lang_it</th>\n",
       "      <th>lang_iw</th>\n",
       "      <th>lang_ja</th>\n",
       "      <th>lang_km</th>\n",
       "      <th>lang_ko</th>\n",
       "      <th>lang_lt</th>\n",
       "      <th>lang_lv</th>\n",
       "      <th>lang_ml</th>\n",
       "      <th>lang_mr</th>\n",
       "      <th>lang_ne</th>\n",
       "      <th>lang_nl</th>\n",
       "      <th>lang_no</th>\n",
       "      <th>lang_pl</th>\n",
       "      <th>lang_ps</th>\n",
       "      <th>lang_pt</th>\n",
       "      <th>lang_ro</th>\n",
       "      <th>lang_ru</th>\n",
       "      <th>lang_sd</th>\n",
       "      <th>lang_si</th>\n",
       "      <th>lang_sl</th>\n",
       "      <th>lang_sr</th>\n",
       "      <th>lang_sv</th>\n",
       "      <th>lang_ta</th>\n",
       "      <th>lang_th</th>\n",
       "      <th>lang_tl</th>\n",
       "      <th>lang_tr</th>\n",
       "      <th>lang_uk</th>\n",
       "      <th>lang_und</th>\n",
       "      <th>lang_ur</th>\n",
       "      <th>lang_vi</th>\n",
       "      <th>lang_zh</th>\n",
       "      <th>possibly_sensitive_False</th>\n",
       "      <th>possibly_sensitive_True</th>\n",
       "      <th>withheld_in_countries_['IN']</th>\n",
       "      <th>withheld_in_countries_['TR']</th>\n",
       "      <th>place.country_Finland</th>\n",
       "      <th>place.country_Hong Kong</th>\n",
       "      <th>place.country_India</th>\n",
       "      <th>place.country_Indonesia</th>\n",
       "      <th>place.country_Italy</th>\n",
       "      <th>place.country_Mongolia</th>\n",
       "      <th>place.country_Pakistan</th>\n",
       "      <th>place.country_People's Republic of China</th>\n",
       "      <th>place.country_Portugal</th>\n",
       "      <th>place.country_Republic of Korea</th>\n",
       "      <th>place.country_Russia</th>\n",
       "      <th>place.country_Singapore</th>\n",
       "      <th>place.country_Sri Lanka</th>\n",
       "      <th>place.country_United Arab Emirates</th>\n",
       "      <th>place.country_United States</th>\n",
       "      <th>user.geo_enabled_False</th>\n",
       "      <th>user.geo_enabled_True</th>\n",
       "      <th>user.verified_False</th>\n",
       "      <th>user.has_extended_profile_False</th>\n",
       "      <th>user.has_extended_profile_True</th>\n",
       "      <th>user.protected_False</th>\n",
       "      <th>user.verified_False.1</th>\n",
       "      <th>user.default_profile_True</th>\n",
       "      <th>is_quote_status_False</th>\n",
       "      <th>is_quote_status_True</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "      <th>599</th>\n",
       "      <th>600</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>612</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>625</th>\n",
       "      <th>626</th>\n",
       "      <th>627</th>\n",
       "      <th>628</th>\n",
       "      <th>629</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "      <th>659</th>\n",
       "      <th>660</th>\n",
       "      <th>661</th>\n",
       "      <th>662</th>\n",
       "      <th>663</th>\n",
       "      <th>664</th>\n",
       "      <th>665</th>\n",
       "      <th>666</th>\n",
       "      <th>667</th>\n",
       "      <th>668</th>\n",
       "      <th>669</th>\n",
       "      <th>670</th>\n",
       "      <th>671</th>\n",
       "      <th>672</th>\n",
       "      <th>673</th>\n",
       "      <th>674</th>\n",
       "      <th>675</th>\n",
       "      <th>676</th>\n",
       "      <th>677</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>700</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "      <th>711</th>\n",
       "      <th>712</th>\n",
       "      <th>713</th>\n",
       "      <th>714</th>\n",
       "      <th>715</th>\n",
       "      <th>716</th>\n",
       "      <th>717</th>\n",
       "      <th>718</th>\n",
       "      <th>719</th>\n",
       "      <th>720</th>\n",
       "      <th>721</th>\n",
       "      <th>722</th>\n",
       "      <th>723</th>\n",
       "      <th>724</th>\n",
       "      <th>725</th>\n",
       "      <th>726</th>\n",
       "      <th>727</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.304799e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012684</td>\n",
       "      <td>0.032058</td>\n",
       "      <td>-0.012084</td>\n",
       "      <td>-0.058909</td>\n",
       "      <td>-0.010121</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>-0.048949</td>\n",
       "      <td>-0.039718</td>\n",
       "      <td>-0.057749</td>\n",
       "      <td>-0.045809</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>-0.023192</td>\n",
       "      <td>-0.079032</td>\n",
       "      <td>0.040669</td>\n",
       "      <td>-0.072587</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>-0.028862</td>\n",
       "      <td>0.046799</td>\n",
       "      <td>-0.012821</td>\n",
       "      <td>0.013112</td>\n",
       "      <td>-0.079651</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>-0.034028</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>-0.021980</td>\n",
       "      <td>0.030615</td>\n",
       "      <td>0.011269</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>-0.026737</td>\n",
       "      <td>-0.054069</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>-0.021249</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>-0.038505</td>\n",
       "      <td>-0.063732</td>\n",
       "      <td>-0.053111</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>-0.042972</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>-0.029163</td>\n",
       "      <td>-0.011124</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.028715</td>\n",
       "      <td>-0.014881</td>\n",
       "      <td>-0.046697</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>-0.022136</td>\n",
       "      <td>-0.045382</td>\n",
       "      <td>-0.018033</td>\n",
       "      <td>0.042627</td>\n",
       "      <td>-0.050698</td>\n",
       "      <td>-0.032358</td>\n",
       "      <td>-0.044754</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>0.023797</td>\n",
       "      <td>-0.045344</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>-0.006689</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>-0.041886</td>\n",
       "      <td>-0.031555</td>\n",
       "      <td>-0.054637</td>\n",
       "      <td>-0.036469</td>\n",
       "      <td>-0.052694</td>\n",
       "      <td>-0.029040</td>\n",
       "      <td>-0.045692</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>0.058955</td>\n",
       "      <td>-0.027599</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>-0.036078</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>-0.023220</td>\n",
       "      <td>-0.008169</td>\n",
       "      <td>-0.040580</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>-0.070895</td>\n",
       "      <td>-0.033936</td>\n",
       "      <td>0.018778</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>-0.022574</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.035646</td>\n",
       "      <td>-0.055151</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>0.019820</td>\n",
       "      <td>0.060649</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>-0.041217</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>-0.002927</td>\n",
       "      <td>-0.051279</td>\n",
       "      <td>-0.037429</td>\n",
       "      <td>-0.018154</td>\n",
       "      <td>-0.028611</td>\n",
       "      <td>-0.049466</td>\n",
       "      <td>-0.072750</td>\n",
       "      <td>-0.015042</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>-0.022654</td>\n",
       "      <td>-0.049825</td>\n",
       "      <td>-0.041341</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>-0.038731</td>\n",
       "      <td>-0.023381</td>\n",
       "      <td>-0.017388</td>\n",
       "      <td>-0.069509</td>\n",
       "      <td>-0.060831</td>\n",
       "      <td>-0.029683</td>\n",
       "      <td>-0.040292</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>-0.009884</td>\n",
       "      <td>-0.042129</td>\n",
       "      <td>-0.042998</td>\n",
       "      <td>0.045355</td>\n",
       "      <td>0.020952</td>\n",
       "      <td>0.054097</td>\n",
       "      <td>0.030807</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.020249</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>0.020506</td>\n",
       "      <td>0.052167</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>-0.029885</td>\n",
       "      <td>-0.057116</td>\n",
       "      <td>-0.024172</td>\n",
       "      <td>0.021789</td>\n",
       "      <td>-0.065039</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>-0.028273</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>-0.021805</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.058093</td>\n",
       "      <td>-0.024467</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>-0.025404</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>-0.012647</td>\n",
       "      <td>0.039335</td>\n",
       "      <td>-0.062787</td>\n",
       "      <td>-0.026857</td>\n",
       "      <td>-0.053948</td>\n",
       "      <td>-0.062015</td>\n",
       "      <td>-0.065928</td>\n",
       "      <td>-0.025099</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>-0.044511</td>\n",
       "      <td>-0.038252</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>-0.002527</td>\n",
       "      <td>-0.021905</td>\n",
       "      <td>-0.025735</td>\n",
       "      <td>0.015305</td>\n",
       "      <td>-0.022429</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.030772</td>\n",
       "      <td>-0.040644</td>\n",
       "      <td>-0.019000</td>\n",
       "      <td>-0.052515</td>\n",
       "      <td>-0.005107</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>-0.064072</td>\n",
       "      <td>-0.016188</td>\n",
       "      <td>-0.020646</td>\n",
       "      <td>-0.010242</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>0.063606</td>\n",
       "      <td>0.038598</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.026701</td>\n",
       "      <td>0.055996</td>\n",
       "      <td>0.019855</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>-0.036682</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>-0.031158</td>\n",
       "      <td>-0.032735</td>\n",
       "      <td>0.033899</td>\n",
       "      <td>-0.052497</td>\n",
       "      <td>-0.031882</td>\n",
       "      <td>0.043214</td>\n",
       "      <td>0.040524</td>\n",
       "      <td>0.028207</td>\n",
       "      <td>0.013128</td>\n",
       "      <td>-0.068048</td>\n",
       "      <td>0.015455</td>\n",
       "      <td>-0.030387</td>\n",
       "      <td>-0.058243</td>\n",
       "      <td>0.048650</td>\n",
       "      <td>-0.056857</td>\n",
       "      <td>-0.021475</td>\n",
       "      <td>-0.053283</td>\n",
       "      <td>-0.078646</td>\n",
       "      <td>-0.026252</td>\n",
       "      <td>-0.058822</td>\n",
       "      <td>0.015631</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>-0.085459</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.046952</td>\n",
       "      <td>-0.031430</td>\n",
       "      <td>-0.048673</td>\n",
       "      <td>-0.061280</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>-0.008721</td>\n",
       "      <td>0.036098</td>\n",
       "      <td>0.054009</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.011423</td>\n",
       "      <td>-0.053928</td>\n",
       "      <td>-0.035878</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>-0.012785</td>\n",
       "      <td>-0.060563</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.043167</td>\n",
       "      <td>0.015077</td>\n",
       "      <td>-0.032720</td>\n",
       "      <td>0.048639</td>\n",
       "      <td>-0.040042</td>\n",
       "      <td>-0.005152</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.037815</td>\n",
       "      <td>-0.022669</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>-0.021953</td>\n",
       "      <td>0.040487</td>\n",
       "      <td>-0.026461</td>\n",
       "      <td>-0.078215</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.022563</td>\n",
       "      <td>-0.081650</td>\n",
       "      <td>-0.024577</td>\n",
       "      <td>-0.029677</td>\n",
       "      <td>-0.054735</td>\n",
       "      <td>-0.055636</td>\n",
       "      <td>-0.039835</td>\n",
       "      <td>0.028341</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>-0.034544</td>\n",
       "      <td>-0.053626</td>\n",
       "      <td>-0.016691</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>-0.014086</td>\n",
       "      <td>-0.050197</td>\n",
       "      <td>-0.064220</td>\n",
       "      <td>0.015356</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>-0.033933</td>\n",
       "      <td>0.054778</td>\n",
       "      <td>-0.061413</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.042436</td>\n",
       "      <td>-0.069930</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.063352</td>\n",
       "      <td>-0.016221</td>\n",
       "      <td>-0.020024</td>\n",
       "      <td>-0.054722</td>\n",
       "      <td>-0.027237</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.037764</td>\n",
       "      <td>-0.060808</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.038232</td>\n",
       "      <td>-0.008557</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>-0.008958</td>\n",
       "      <td>-0.058989</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>-0.037424</td>\n",
       "      <td>-0.038551</td>\n",
       "      <td>0.012316</td>\n",
       "      <td>-0.054515</td>\n",
       "      <td>-0.005107</td>\n",
       "      <td>0.032695</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>-0.004137</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>-0.076708</td>\n",
       "      <td>-0.028251</td>\n",
       "      <td>-0.004270</td>\n",
       "      <td>-0.038534</td>\n",
       "      <td>0.044063</td>\n",
       "      <td>-0.016370</td>\n",
       "      <td>-0.062544</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>-0.014834</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>-0.021939</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>-0.054524</td>\n",
       "      <td>-0.044374</td>\n",
       "      <td>-0.014301</td>\n",
       "      <td>-0.063166</td>\n",
       "      <td>-0.023251</td>\n",
       "      <td>-0.065704</td>\n",
       "      <td>-0.047744</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.037139</td>\n",
       "      <td>-0.015856</td>\n",
       "      <td>-0.037671</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>-0.028133</td>\n",
       "      <td>-0.059081</td>\n",
       "      <td>0.049446</td>\n",
       "      <td>-0.064176</td>\n",
       "      <td>-0.023756</td>\n",
       "      <td>-0.055596</td>\n",
       "      <td>-0.059165</td>\n",
       "      <td>0.008643</td>\n",
       "      <td>0.060375</td>\n",
       "      <td>-0.030946</td>\n",
       "      <td>0.010688</td>\n",
       "      <td>-0.001439</td>\n",
       "      <td>-0.020018</td>\n",
       "      <td>-0.035089</td>\n",
       "      <td>-0.005154</td>\n",
       "      <td>-0.026912</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.050198</td>\n",
       "      <td>-0.030673</td>\n",
       "      <td>-0.033770</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>-0.015782</td>\n",
       "      <td>-0.023778</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>-0.007667</td>\n",
       "      <td>-0.008139</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>-0.075483</td>\n",
       "      <td>-0.041235</td>\n",
       "      <td>-0.004791</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>-0.039245</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>0.061607</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>-0.028836</td>\n",
       "      <td>-0.027864</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>-0.024530</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>-0.011256</td>\n",
       "      <td>0.038963</td>\n",
       "      <td>0.012131</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>-0.061403</td>\n",
       "      <td>0.033831</td>\n",
       "      <td>-0.062537</td>\n",
       "      <td>0.049702</td>\n",
       "      <td>-0.011401</td>\n",
       "      <td>-0.056452</td>\n",
       "      <td>-0.034077</td>\n",
       "      <td>-0.031405</td>\n",
       "      <td>-0.005557</td>\n",
       "      <td>-0.019793</td>\n",
       "      <td>-0.047722</td>\n",
       "      <td>-0.034338</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>0.020047</td>\n",
       "      <td>-0.048563</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.011097</td>\n",
       "      <td>0.058591</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>-0.038439</td>\n",
       "      <td>0.032881</td>\n",
       "      <td>-0.010134</td>\n",
       "      <td>-0.023956</td>\n",
       "      <td>-0.044026</td>\n",
       "      <td>-0.025120</td>\n",
       "      <td>0.066441</td>\n",
       "      <td>-0.074821</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>-0.003505</td>\n",
       "      <td>-0.005499</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>0.046678</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.033452</td>\n",
       "      <td>-0.044611</td>\n",
       "      <td>0.018085</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>-0.062089</td>\n",
       "      <td>-0.048794</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.060458</td>\n",
       "      <td>-0.057450</td>\n",
       "      <td>-0.043299</td>\n",
       "      <td>-0.025642</td>\n",
       "      <td>-0.030838</td>\n",
       "      <td>-0.035391</td>\n",
       "      <td>0.010264</td>\n",
       "      <td>0.040862</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>0.046733</td>\n",
       "      <td>-0.021436</td>\n",
       "      <td>-0.020737</td>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.040358</td>\n",
       "      <td>-0.006694</td>\n",
       "      <td>0.020653</td>\n",
       "      <td>-0.030670</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>-0.033957</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.043648</td>\n",
       "      <td>-0.039115</td>\n",
       "      <td>-0.019880</td>\n",
       "      <td>-0.043601</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>-0.026493</td>\n",
       "      <td>-0.050176</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.036572</td>\n",
       "      <td>-0.058249</td>\n",
       "      <td>-0.002921</td>\n",
       "      <td>0.055421</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>-0.025683</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>-0.033153</td>\n",
       "      <td>0.017731</td>\n",
       "      <td>-0.042585</td>\n",
       "      <td>-0.032140</td>\n",
       "      <td>0.038905</td>\n",
       "      <td>0.043402</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.033806</td>\n",
       "      <td>-0.032124</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>-0.014965</td>\n",
       "      <td>-0.030667</td>\n",
       "      <td>-0.010237</td>\n",
       "      <td>0.014229</td>\n",
       "      <td>-0.029927</td>\n",
       "      <td>0.049333</td>\n",
       "      <td>0.007827</td>\n",
       "      <td>-0.019476</td>\n",
       "      <td>0.062580</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>-0.021132</td>\n",
       "      <td>-0.016794</td>\n",
       "      <td>-0.036559</td>\n",
       "      <td>-0.035611</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>-0.001510</td>\n",
       "      <td>-0.016450</td>\n",
       "      <td>-0.021570</td>\n",
       "      <td>0.029265</td>\n",
       "      <td>-0.052247</td>\n",
       "      <td>-0.004660</td>\n",
       "      <td>-0.003738</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>-0.017634</td>\n",
       "      <td>-0.040621</td>\n",
       "      <td>-0.021514</td>\n",
       "      <td>-0.070704</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>-0.069007</td>\n",
       "      <td>-0.070804</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>-0.022498</td>\n",
       "      <td>-0.055149</td>\n",
       "      <td>-0.056354</td>\n",
       "      <td>-0.015443</td>\n",
       "      <td>-0.007421</td>\n",
       "      <td>0.039084</td>\n",
       "      <td>-0.040486</td>\n",
       "      <td>-0.000920</td>\n",
       "      <td>-0.021299</td>\n",
       "      <td>0.027961</td>\n",
       "      <td>-0.000931</td>\n",
       "      <td>-0.020922</td>\n",
       "      <td>-0.032573</td>\n",
       "      <td>0.013119</td>\n",
       "      <td>0.033797</td>\n",
       "      <td>0.045542</td>\n",
       "      <td>0.035572</td>\n",
       "      <td>-0.061040</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.037593</td>\n",
       "      <td>0.031198</td>\n",
       "      <td>-0.063085</td>\n",
       "      <td>-0.054206</td>\n",
       "      <td>-0.026588</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>-0.014704</td>\n",
       "      <td>-0.038299</td>\n",
       "      <td>-0.031824</td>\n",
       "      <td>-0.019739</td>\n",
       "      <td>-0.031320</td>\n",
       "      <td>-0.006948</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>-0.055126</td>\n",
       "      <td>-0.026795</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.016510</td>\n",
       "      <td>-0.014776</td>\n",
       "      <td>-0.037681</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>0.017547</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>-0.016385</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>-0.068767</td>\n",
       "      <td>-0.045178</td>\n",
       "      <td>-0.014487</td>\n",
       "      <td>-0.078171</td>\n",
       "      <td>-0.023002</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>-0.052431</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>0.045363</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>-0.047980</td>\n",
       "      <td>-0.027432</td>\n",
       "      <td>-0.027310</td>\n",
       "      <td>0.038483</td>\n",
       "      <td>-0.026820</td>\n",
       "      <td>0.036034</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>-0.031180</td>\n",
       "      <td>-0.059132</td>\n",
       "      <td>-0.007731</td>\n",
       "      <td>-0.027518</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>-0.032036</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>-0.034186</td>\n",
       "      <td>-0.058650</td>\n",
       "      <td>-0.004650</td>\n",
       "      <td>-0.029011</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>0.025438</td>\n",
       "      <td>-0.016285</td>\n",
       "      <td>-0.068860</td>\n",
       "      <td>-0.014764</td>\n",
       "      <td>-0.070535</td>\n",
       "      <td>-0.036185</td>\n",
       "      <td>-0.006802</td>\n",
       "      <td>0.023873</td>\n",
       "      <td>-0.037405</td>\n",
       "      <td>-0.008423</td>\n",
       "      <td>-0.009682</td>\n",
       "      <td>-0.013933</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>-0.056361</td>\n",
       "      <td>-0.002160</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>-0.051472</td>\n",
       "      <td>-0.021516</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>-0.070884</td>\n",
       "      <td>-0.051379</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.025921</td>\n",
       "      <td>-0.065846</td>\n",
       "      <td>-0.067918</td>\n",
       "      <td>-0.051060</td>\n",
       "      <td>-0.057959</td>\n",
       "      <td>-0.037975</td>\n",
       "      <td>-0.056151</td>\n",
       "      <td>-0.046440</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.022025</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>-0.057889</td>\n",
       "      <td>-0.027978</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>-0.062871</td>\n",
       "      <td>-0.050490</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.053673</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>-0.024664</td>\n",
       "      <td>-0.010300</td>\n",
       "      <td>-0.041777</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>-0.047875</td>\n",
       "      <td>-0.008419</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>-0.043248</td>\n",
       "      <td>0.051303</td>\n",
       "      <td>-0.046123</td>\n",
       "      <td>-0.010920</td>\n",
       "      <td>-0.049496</td>\n",
       "      <td>-0.047422</td>\n",
       "      <td>-0.055813</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>-0.033612</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.039050</td>\n",
       "      <td>-0.024479</td>\n",
       "      <td>-0.003441</td>\n",
       "      <td>-0.003868</td>\n",
       "      <td>-0.050178</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.007625</td>\n",
       "      <td>-0.038461</td>\n",
       "      <td>-0.008437</td>\n",
       "      <td>-0.042519</td>\n",
       "      <td>-0.052310</td>\n",
       "      <td>-0.022626</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>-0.050896</td>\n",
       "      <td>0.046169</td>\n",
       "      <td>-0.050201</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>-0.039249</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.020891</td>\n",
       "      <td>0.056057</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.070106</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>-0.052152</td>\n",
       "      <td>0.055809</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>-0.042174</td>\n",
       "      <td>-0.056090</td>\n",
       "      <td>-0.049169</td>\n",
       "      <td>-0.049363</td>\n",
       "      <td>0.053747</td>\n",
       "      <td>0.032561</td>\n",
       "      <td>-0.028358</td>\n",
       "      <td>0.010958</td>\n",
       "      <td>-0.045892</td>\n",
       "      <td>0.040456</td>\n",
       "      <td>-0.037873</td>\n",
       "      <td>-0.043589</td>\n",
       "      <td>-0.079459</td>\n",
       "      <td>-0.006884</td>\n",
       "      <td>0.041131</td>\n",
       "      <td>-0.010238</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>-0.011273</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>-0.032969</td>\n",
       "      <td>-0.057758</td>\n",
       "      <td>-0.003042</td>\n",
       "      <td>0.017432</td>\n",
       "      <td>0.054796</td>\n",
       "      <td>-0.032331</td>\n",
       "      <td>-0.028657</td>\n",
       "      <td>-0.072479</td>\n",
       "      <td>-0.066265</td>\n",
       "      <td>-0.054768</td>\n",
       "      <td>-0.040680</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.010429</td>\n",
       "      <td>0.024509</td>\n",
       "      <td>-0.038953</td>\n",
       "      <td>0.038253</td>\n",
       "      <td>0.013392</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>-0.031836</td>\n",
       "      <td>-0.045536</td>\n",
       "      <td>-0.044118</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>-0.016759</td>\n",
       "      <td>-0.056777</td>\n",
       "      <td>-0.048227</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>-0.020328</td>\n",
       "      <td>0.036249</td>\n",
       "      <td>-0.004574</td>\n",
       "      <td>0.045056</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>0.059654</td>\n",
       "      <td>0.026574</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.053398</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>-0.026796</td>\n",
       "      <td>-0.021011</td>\n",
       "      <td>0.057906</td>\n",
       "      <td>-0.002990</td>\n",
       "      <td>0.061430</td>\n",
       "      <td>-0.053808</td>\n",
       "      <td>-0.064560</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.045201</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>-0.028130</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.049347</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>-0.020762</td>\n",
       "      <td>-0.050510</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.045674</td>\n",
       "      <td>0.016957</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.055194</td>\n",
       "      <td>-0.044291</td>\n",
       "      <td>0.034094</td>\n",
       "      <td>-0.038885</td>\n",
       "      <td>-0.036568</td>\n",
       "      <td>-0.032016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.071428</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>-0.030672</td>\n",
       "      <td>-0.013456</td>\n",
       "      <td>-0.012440</td>\n",
       "      <td>-0.018908</td>\n",
       "      <td>-0.031231</td>\n",
       "      <td>-0.077490</td>\n",
       "      <td>0.044540</td>\n",
       "      <td>-0.031018</td>\n",
       "      <td>-0.045638</td>\n",
       "      <td>-0.055999</td>\n",
       "      <td>-0.081450</td>\n",
       "      <td>-0.012998</td>\n",
       "      <td>0.018460</td>\n",
       "      <td>-0.009418</td>\n",
       "      <td>0.037660</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>-0.040534</td>\n",
       "      <td>-0.003758</td>\n",
       "      <td>-0.051248</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.025957</td>\n",
       "      <td>-0.039417</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>-0.057495</td>\n",
       "      <td>-0.017595</td>\n",
       "      <td>-0.050963</td>\n",
       "      <td>-0.036229</td>\n",
       "      <td>-0.029492</td>\n",
       "      <td>-0.033471</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.049225</td>\n",
       "      <td>0.057893</td>\n",
       "      <td>-0.051001</td>\n",
       "      <td>0.039209</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>-0.028121</td>\n",
       "      <td>-0.022803</td>\n",
       "      <td>0.032745</td>\n",
       "      <td>-0.057217</td>\n",
       "      <td>-0.021886</td>\n",
       "      <td>-0.045973</td>\n",
       "      <td>0.057541</td>\n",
       "      <td>-0.026861</td>\n",
       "      <td>-0.067797</td>\n",
       "      <td>0.058958</td>\n",
       "      <td>-0.059584</td>\n",
       "      <td>-0.050733</td>\n",
       "      <td>-0.059008</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>-0.049431</td>\n",
       "      <td>-0.013537</td>\n",
       "      <td>-0.039153</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>-0.033792</td>\n",
       "      <td>-0.053318</td>\n",
       "      <td>-0.004010</td>\n",
       "      <td>-0.010678</td>\n",
       "      <td>-0.055194</td>\n",
       "      <td>0.023063</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>-0.024484</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>-0.050410</td>\n",
       "      <td>-0.065115</td>\n",
       "      <td>-0.059771</td>\n",
       "      <td>-0.000246</td>\n",
       "      <td>-0.058024</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>-0.013553</td>\n",
       "      <td>0.019451</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>-0.071764</td>\n",
       "      <td>-0.009920</td>\n",
       "      <td>0.062180</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>-0.050513</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>-0.025193</td>\n",
       "      <td>-0.003190</td>\n",
       "      <td>-0.060368</td>\n",
       "      <td>-0.037959</td>\n",
       "      <td>-0.012182</td>\n",
       "      <td>-0.053345</td>\n",
       "      <td>0.041564</td>\n",
       "      <td>-0.060537</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.034163</td>\n",
       "      <td>-0.012904</td>\n",
       "      <td>-0.036963</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>-0.008359</td>\n",
       "      <td>-0.035451</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>-0.043065</td>\n",
       "      <td>-0.028333</td>\n",
       "      <td>-0.024705</td>\n",
       "      <td>-0.018967</td>\n",
       "      <td>-0.024230</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.010815</td>\n",
       "      <td>-0.045273</td>\n",
       "      <td>-0.040499</td>\n",
       "      <td>-0.031301</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>-0.041313</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>-0.008628</td>\n",
       "      <td>-0.016825</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>-0.002088</td>\n",
       "      <td>-0.067313</td>\n",
       "      <td>-0.027284</td>\n",
       "      <td>-0.016430</td>\n",
       "      <td>-0.017021</td>\n",
       "      <td>0.045932</td>\n",
       "      <td>-0.024103</td>\n",
       "      <td>-0.017397</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>-0.038079</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>0.047446</td>\n",
       "      <td>-0.038267</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>-0.027974</td>\n",
       "      <td>-0.052660</td>\n",
       "      <td>-0.045014</td>\n",
       "      <td>0.027004</td>\n",
       "      <td>-0.011624</td>\n",
       "      <td>0.024504</td>\n",
       "      <td>-0.058782</td>\n",
       "      <td>-0.051111</td>\n",
       "      <td>-0.039329</td>\n",
       "      <td>-0.048663</td>\n",
       "      <td>-0.043818</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>-0.044706</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.024433</td>\n",
       "      <td>-0.037887</td>\n",
       "      <td>0.020904</td>\n",
       "      <td>-0.006212</td>\n",
       "      <td>-0.035133</td>\n",
       "      <td>-0.015360</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>-0.020923</td>\n",
       "      <td>0.028903</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.054304</td>\n",
       "      <td>-0.073347</td>\n",
       "      <td>-0.004396</td>\n",
       "      <td>-0.015981</td>\n",
       "      <td>-0.005769</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>0.017139</td>\n",
       "      <td>-0.047879</td>\n",
       "      <td>-0.039180</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>-0.028003</td>\n",
       "      <td>-0.004390</td>\n",
       "      <td>-0.055328</td>\n",
       "      <td>-0.065056</td>\n",
       "      <td>-0.062104</td>\n",
       "      <td>-0.062825</td>\n",
       "      <td>-0.021902</td>\n",
       "      <td>-0.032651</td>\n",
       "      <td>0.017248</td>\n",
       "      <td>-0.036045</td>\n",
       "      <td>-0.070820</td>\n",
       "      <td>-0.010375</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>-0.024553</td>\n",
       "      <td>-0.001835</td>\n",
       "      <td>-0.050494</td>\n",
       "      <td>-0.067199</td>\n",
       "      <td>-0.064832</td>\n",
       "      <td>-0.008620</td>\n",
       "      <td>-0.047122</td>\n",
       "      <td>-0.021271</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>-0.072632</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>-0.053682</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>-0.013576</td>\n",
       "      <td>0.048936</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>-0.018688</td>\n",
       "      <td>-0.034227</td>\n",
       "      <td>-0.037879</td>\n",
       "      <td>0.039943</td>\n",
       "      <td>-0.033647</td>\n",
       "      <td>0.020257</td>\n",
       "      <td>-0.057439</td>\n",
       "      <td>-0.030871</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>-0.034224</td>\n",
       "      <td>-0.046174</td>\n",
       "      <td>-0.031649</td>\n",
       "      <td>-0.014695</td>\n",
       "      <td>-0.132223</td>\n",
       "      <td>-0.024963</td>\n",
       "      <td>-0.024907</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.024054</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>-0.020300</td>\n",
       "      <td>0.042878</td>\n",
       "      <td>-0.043980</td>\n",
       "      <td>-0.022271</td>\n",
       "      <td>-0.043112</td>\n",
       "      <td>0.005239</td>\n",
       "      <td>-0.030760</td>\n",
       "      <td>-0.060127</td>\n",
       "      <td>-0.050701</td>\n",
       "      <td>-0.060860</td>\n",
       "      <td>-0.013946</td>\n",
       "      <td>-0.046568</td>\n",
       "      <td>-0.010687</td>\n",
       "      <td>-0.040630</td>\n",
       "      <td>0.024113</td>\n",
       "      <td>-0.060536</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>-0.044205</td>\n",
       "      <td>-0.048382</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>-0.033334</td>\n",
       "      <td>0.022661</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.044073</td>\n",
       "      <td>0.031779</td>\n",
       "      <td>-0.038928</td>\n",
       "      <td>-0.003178</td>\n",
       "      <td>-0.058736</td>\n",
       "      <td>-0.008807</td>\n",
       "      <td>0.034724</td>\n",
       "      <td>-0.068505</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>-0.008528</td>\n",
       "      <td>-0.015663</td>\n",
       "      <td>-0.025105</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.035809</td>\n",
       "      <td>-0.056446</td>\n",
       "      <td>-0.059206</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>-0.049463</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>-0.057196</td>\n",
       "      <td>-0.010605</td>\n",
       "      <td>-0.005667</td>\n",
       "      <td>-0.050112</td>\n",
       "      <td>-0.058301</td>\n",
       "      <td>-0.009496</td>\n",
       "      <td>-0.046851</td>\n",
       "      <td>0.037343</td>\n",
       "      <td>-0.053403</td>\n",
       "      <td>-0.014253</td>\n",
       "      <td>0.031818</td>\n",
       "      <td>-0.030606</td>\n",
       "      <td>-0.035720</td>\n",
       "      <td>-0.002055</td>\n",
       "      <td>0.028619</td>\n",
       "      <td>-0.037756</td>\n",
       "      <td>-0.058990</td>\n",
       "      <td>-0.036828</td>\n",
       "      <td>0.011469</td>\n",
       "      <td>-0.043592</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-0.071860</td>\n",
       "      <td>-0.042504</td>\n",
       "      <td>-0.027704</td>\n",
       "      <td>-0.058134</td>\n",
       "      <td>-0.057571</td>\n",
       "      <td>-0.005588</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.025648</td>\n",
       "      <td>-0.046387</td>\n",
       "      <td>-0.032408</td>\n",
       "      <td>-0.058367</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>-0.058075</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>-0.055576</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>-0.013446</td>\n",
       "      <td>0.020848</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>-0.051643</td>\n",
       "      <td>-0.017939</td>\n",
       "      <td>-0.058548</td>\n",
       "      <td>-0.009486</td>\n",
       "      <td>0.021758</td>\n",
       "      <td>-0.057022</td>\n",
       "      <td>-0.009951</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.046696</td>\n",
       "      <td>-0.058660</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.009657</td>\n",
       "      <td>-0.028284</td>\n",
       "      <td>0.037977</td>\n",
       "      <td>0.013025</td>\n",
       "      <td>-0.040821</td>\n",
       "      <td>-0.025729</td>\n",
       "      <td>-0.029452</td>\n",
       "      <td>-0.053803</td>\n",
       "      <td>-0.012388</td>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.034418</td>\n",
       "      <td>0.046840</td>\n",
       "      <td>-0.051162</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>-0.052037</td>\n",
       "      <td>0.043989</td>\n",
       "      <td>-0.049349</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>-0.045221</td>\n",
       "      <td>-0.046871</td>\n",
       "      <td>-0.006051</td>\n",
       "      <td>-0.009531</td>\n",
       "      <td>0.038348</td>\n",
       "      <td>-0.051193</td>\n",
       "      <td>0.050519</td>\n",
       "      <td>-0.019161</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>-0.064356</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>-0.054494</td>\n",
       "      <td>0.023120</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.059278</td>\n",
       "      <td>-0.024523</td>\n",
       "      <td>-0.069670</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.021530</td>\n",
       "      <td>0.042663</td>\n",
       "      <td>-0.031639</td>\n",
       "      <td>-0.009952</td>\n",
       "      <td>-0.052521</td>\n",
       "      <td>-0.020216</td>\n",
       "      <td>-0.043306</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.019863</td>\n",
       "      <td>-0.034234</td>\n",
       "      <td>-0.012106</td>\n",
       "      <td>-0.076982</td>\n",
       "      <td>-0.021089</td>\n",
       "      <td>-0.032529</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>-0.015409</td>\n",
       "      <td>-0.036220</td>\n",
       "      <td>-0.024190</td>\n",
       "      <td>-0.025237</td>\n",
       "      <td>-0.052166</td>\n",
       "      <td>-0.041133</td>\n",
       "      <td>0.026204</td>\n",
       "      <td>-0.018844</td>\n",
       "      <td>-0.050077</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.019855</td>\n",
       "      <td>0.020973</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>-0.061723</td>\n",
       "      <td>-0.040731</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>-0.015012</td>\n",
       "      <td>-0.008618</td>\n",
       "      <td>0.055333</td>\n",
       "      <td>-0.062544</td>\n",
       "      <td>-0.013444</td>\n",
       "      <td>0.032236</td>\n",
       "      <td>-0.030142</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>-0.069567</td>\n",
       "      <td>-0.075792</td>\n",
       "      <td>-0.043926</td>\n",
       "      <td>-0.034297</td>\n",
       "      <td>-0.028991</td>\n",
       "      <td>-0.010748</td>\n",
       "      <td>-0.048759</td>\n",
       "      <td>0.033360</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>-0.009353</td>\n",
       "      <td>-0.010127</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>-0.044779</td>\n",
       "      <td>-0.009634</td>\n",
       "      <td>-0.021868</td>\n",
       "      <td>-0.031937</td>\n",
       "      <td>-0.046726</td>\n",
       "      <td>-0.027837</td>\n",
       "      <td>0.014695</td>\n",
       "      <td>-0.015575</td>\n",
       "      <td>-0.043078</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>-0.062061</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>-0.028108</td>\n",
       "      <td>-0.014859</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>-0.055049</td>\n",
       "      <td>-0.041980</td>\n",
       "      <td>-0.025730</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>-0.057979</td>\n",
       "      <td>0.028223</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>0.030896</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>0.026305</td>\n",
       "      <td>-0.043724</td>\n",
       "      <td>-0.008223</td>\n",
       "      <td>-0.025230</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>-0.044566</td>\n",
       "      <td>-0.059452</td>\n",
       "      <td>-0.021281</td>\n",
       "      <td>0.026165</td>\n",
       "      <td>0.028602</td>\n",
       "      <td>-0.005644</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>-0.022547</td>\n",
       "      <td>-0.025565</td>\n",
       "      <td>0.020375</td>\n",
       "      <td>-0.012497</td>\n",
       "      <td>-0.059997</td>\n",
       "      <td>-0.033381</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.035229</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>-0.043477</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.022041</td>\n",
       "      <td>-0.032493</td>\n",
       "      <td>0.046335</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>-0.066077</td>\n",
       "      <td>-0.044808</td>\n",
       "      <td>-0.035860</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>-0.060185</td>\n",
       "      <td>-0.070611</td>\n",
       "      <td>-0.056435</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>-0.062969</td>\n",
       "      <td>-0.057098</td>\n",
       "      <td>0.030148</td>\n",
       "      <td>-0.029601</td>\n",
       "      <td>-0.054524</td>\n",
       "      <td>-0.021627</td>\n",
       "      <td>-0.054563</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>-0.020683</td>\n",
       "      <td>-0.044945</td>\n",
       "      <td>-0.053670</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>-0.058823</td>\n",
       "      <td>-0.054704</td>\n",
       "      <td>-0.053761</td>\n",
       "      <td>0.045445</td>\n",
       "      <td>-0.018459</td>\n",
       "      <td>0.016836</td>\n",
       "      <td>-0.005326</td>\n",
       "      <td>-0.064511</td>\n",
       "      <td>-0.013540</td>\n",
       "      <td>-0.078402</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>-0.012687</td>\n",
       "      <td>-0.063992</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>-0.031982</td>\n",
       "      <td>-0.002981</td>\n",
       "      <td>-0.004428</td>\n",
       "      <td>-0.008878</td>\n",
       "      <td>0.033945</td>\n",
       "      <td>-0.035079</td>\n",
       "      <td>-0.027763</td>\n",
       "      <td>-0.054834</td>\n",
       "      <td>-0.047795</td>\n",
       "      <td>-0.006555</td>\n",
       "      <td>-0.029269</td>\n",
       "      <td>-0.031009</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.009577</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.026573</td>\n",
       "      <td>0.024791</td>\n",
       "      <td>-0.044695</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.035465</td>\n",
       "      <td>-0.032914</td>\n",
       "      <td>0.048790</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.050386</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>-0.042430</td>\n",
       "      <td>-0.046832</td>\n",
       "      <td>0.046444</td>\n",
       "      <td>-0.008148</td>\n",
       "      <td>-0.001692</td>\n",
       "      <td>-0.055412</td>\n",
       "      <td>0.040139</td>\n",
       "      <td>-0.035217</td>\n",
       "      <td>-0.043953</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>-0.050073</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>-0.012039</td>\n",
       "      <td>-0.059430</td>\n",
       "      <td>-0.046196</td>\n",
       "      <td>-0.060626</td>\n",
       "      <td>-0.041724</td>\n",
       "      <td>-0.052979</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>-0.019205</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.038205</td>\n",
       "      <td>-0.072295</td>\n",
       "      <td>-0.041036</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>-0.010375</td>\n",
       "      <td>-0.044046</td>\n",
       "      <td>-0.067721</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>-0.057155</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>-0.053026</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.003877</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>-0.017378</td>\n",
       "      <td>0.009125</td>\n",
       "      <td>-0.055527</td>\n",
       "      <td>-0.041034</td>\n",
       "      <td>-0.024052</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>-0.039330</td>\n",
       "      <td>-0.047653</td>\n",
       "      <td>0.042111</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>-0.015446</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>-0.031258</td>\n",
       "      <td>-0.055496</td>\n",
       "      <td>-0.059705</td>\n",
       "      <td>-0.023875</td>\n",
       "      <td>-0.042165</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>0.027358</td>\n",
       "      <td>-0.048551</td>\n",
       "      <td>0.028619</td>\n",
       "      <td>-0.023517</td>\n",
       "      <td>-0.009893</td>\n",
       "      <td>0.042249</td>\n",
       "      <td>-0.050274</td>\n",
       "      <td>0.028570</td>\n",
       "      <td>0.044214</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>-0.062726</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>0.026510</td>\n",
       "      <td>0.013826</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>0.053254</td>\n",
       "      <td>-0.032539</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>-0.006991</td>\n",
       "      <td>-0.027323</td>\n",
       "      <td>-0.027311</td>\n",
       "      <td>-0.049971</td>\n",
       "      <td>-0.063147</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>0.058354</td>\n",
       "      <td>-0.032042</td>\n",
       "      <td>-0.030554</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>-0.058864</td>\n",
       "      <td>-0.033108</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>-0.031755</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>-0.011048</td>\n",
       "      <td>-0.010535</td>\n",
       "      <td>-0.031720</td>\n",
       "      <td>-0.039835</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>-0.008278</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>-0.031928</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>-0.040171</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>-0.023325</td>\n",
       "      <td>0.022757</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>-0.011220</td>\n",
       "      <td>-0.052424</td>\n",
       "      <td>-0.021003</td>\n",
       "      <td>-0.059515</td>\n",
       "      <td>0.031493</td>\n",
       "      <td>-0.025217</td>\n",
       "      <td>0.058204</td>\n",
       "      <td>-0.029380</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>-0.045685</td>\n",
       "      <td>-0.044427</td>\n",
       "      <td>-0.015865</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>-0.053030</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>-0.051657</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>-0.049349</td>\n",
       "      <td>-0.022269</td>\n",
       "      <td>0.027957</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>-0.038253</td>\n",
       "      <td>-0.042656</td>\n",
       "      <td>0.025039</td>\n",
       "      <td>-0.050888</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>-0.035658</td>\n",
       "      <td>-0.078400</td>\n",
       "      <td>-0.014437</td>\n",
       "      <td>-0.070811</td>\n",
       "      <td>-0.027193</td>\n",
       "      <td>0.044431</td>\n",
       "      <td>-0.041020</td>\n",
       "      <td>-0.008999</td>\n",
       "      <td>-0.034534</td>\n",
       "      <td>0.019317</td>\n",
       "      <td>-0.074715</td>\n",
       "      <td>-0.019871</td>\n",
       "      <td>-0.050320</td>\n",
       "      <td>-0.032099</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>-0.025217</td>\n",
       "      <td>-0.031488</td>\n",
       "      <td>-0.003999</td>\n",
       "      <td>0.031495</td>\n",
       "      <td>-0.036209</td>\n",
       "      <td>-0.075756</td>\n",
       "      <td>-0.041112</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>-0.021566</td>\n",
       "      <td>-0.004238</td>\n",
       "      <td>-0.012103</td>\n",
       "      <td>-0.023819</td>\n",
       "      <td>0.017329</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>-0.062967</td>\n",
       "      <td>-0.020506</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>-0.075328</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>-0.042582</td>\n",
       "      <td>-0.017590</td>\n",
       "      <td>-0.027338</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>-0.025668</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>0.020732</td>\n",
       "      <td>-0.003057</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>-0.045396</td>\n",
       "      <td>-0.000815</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>-0.023224</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.017693</td>\n",
       "      <td>0.029330</td>\n",
       "      <td>0.024083</td>\n",
       "      <td>-0.050235</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>-0.060817</td>\n",
       "      <td>-0.081728</td>\n",
       "      <td>0.027943</td>\n",
       "      <td>0.017330</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>0.025109</td>\n",
       "      <td>-0.044979</td>\n",
       "      <td>0.028983</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>-0.031516</td>\n",
       "      <td>-0.010376</td>\n",
       "      <td>0.022773</td>\n",
       "      <td>0.027381</td>\n",
       "      <td>0.039309</td>\n",
       "      <td>-0.034558</td>\n",
       "      <td>-0.029595</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>-0.045198</td>\n",
       "      <td>-0.018821</td>\n",
       "      <td>-0.048212</td>\n",
       "      <td>-0.057645</td>\n",
       "      <td>-0.065396</td>\n",
       "      <td>-0.035478</td>\n",
       "      <td>-0.042057</td>\n",
       "      <td>-0.012445</td>\n",
       "      <td>-0.039846</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>-0.014138</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58746.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019513</td>\n",
       "      <td>-0.010960</td>\n",
       "      <td>-0.057681</td>\n",
       "      <td>-0.032907</td>\n",
       "      <td>-0.037026</td>\n",
       "      <td>0.054790</td>\n",
       "      <td>-0.027800</td>\n",
       "      <td>-0.012979</td>\n",
       "      <td>-0.085419</td>\n",
       "      <td>-0.027678</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>0.021589</td>\n",
       "      <td>-0.070805</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>-0.069739</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>-0.018928</td>\n",
       "      <td>-0.003137</td>\n",
       "      <td>-0.066233</td>\n",
       "      <td>0.032976</td>\n",
       "      <td>-0.065034</td>\n",
       "      <td>0.033201</td>\n",
       "      <td>-0.056407</td>\n",
       "      <td>0.050745</td>\n",
       "      <td>0.027317</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.027160</td>\n",
       "      <td>0.027250</td>\n",
       "      <td>-0.059269</td>\n",
       "      <td>-0.036120</td>\n",
       "      <td>0.037503</td>\n",
       "      <td>-0.032100</td>\n",
       "      <td>-0.019312</td>\n",
       "      <td>-0.022517</td>\n",
       "      <td>-0.058025</td>\n",
       "      <td>-0.038338</td>\n",
       "      <td>0.007366</td>\n",
       "      <td>0.036771</td>\n",
       "      <td>-0.061164</td>\n",
       "      <td>0.048462</td>\n",
       "      <td>0.021861</td>\n",
       "      <td>-0.048065</td>\n",
       "      <td>-0.024787</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>-0.054749</td>\n",
       "      <td>-0.002271</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>-0.032456</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>-0.044380</td>\n",
       "      <td>-0.030845</td>\n",
       "      <td>-0.022430</td>\n",
       "      <td>-0.066008</td>\n",
       "      <td>-0.020078</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.035726</td>\n",
       "      <td>-0.060627</td>\n",
       "      <td>-0.019230</td>\n",
       "      <td>-0.009873</td>\n",
       "      <td>0.035434</td>\n",
       "      <td>-0.010532</td>\n",
       "      <td>0.035820</td>\n",
       "      <td>0.030388</td>\n",
       "      <td>-0.022084</td>\n",
       "      <td>-0.033331</td>\n",
       "      <td>-0.047660</td>\n",
       "      <td>0.016659</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>-0.060040</td>\n",
       "      <td>-0.049726</td>\n",
       "      <td>-0.030440</td>\n",
       "      <td>-0.025063</td>\n",
       "      <td>-0.032255</td>\n",
       "      <td>-0.020624</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>-0.038211</td>\n",
       "      <td>-0.041909</td>\n",
       "      <td>-0.020006</td>\n",
       "      <td>0.024421</td>\n",
       "      <td>0.034367</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.021557</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>-0.008977</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>0.024436</td>\n",
       "      <td>0.010569</td>\n",
       "      <td>-0.052433</td>\n",
       "      <td>-0.046885</td>\n",
       "      <td>0.052143</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>-0.019835</td>\n",
       "      <td>0.068253</td>\n",
       "      <td>0.026576</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.041462</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.009661</td>\n",
       "      <td>0.050481</td>\n",
       "      <td>-0.068515</td>\n",
       "      <td>-0.054983</td>\n",
       "      <td>-0.001160</td>\n",
       "      <td>-0.024509</td>\n",
       "      <td>-0.051372</td>\n",
       "      <td>-0.063344</td>\n",
       "      <td>-0.041517</td>\n",
       "      <td>-0.009673</td>\n",
       "      <td>-0.037621</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>-0.044874</td>\n",
       "      <td>-0.015007</td>\n",
       "      <td>-0.021086</td>\n",
       "      <td>-0.056174</td>\n",
       "      <td>-0.023262</td>\n",
       "      <td>0.063454</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.020157</td>\n",
       "      <td>-0.043945</td>\n",
       "      <td>-0.026554</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>0.006131</td>\n",
       "      <td>-0.005232</td>\n",
       "      <td>-0.046493</td>\n",
       "      <td>-0.026550</td>\n",
       "      <td>-0.020351</td>\n",
       "      <td>0.048224</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.025412</td>\n",
       "      <td>-0.052168</td>\n",
       "      <td>-0.034330</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.060821</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>-0.008740</td>\n",
       "      <td>-0.034960</td>\n",
       "      <td>-0.005468</td>\n",
       "      <td>-0.008729</td>\n",
       "      <td>-0.007196</td>\n",
       "      <td>-0.013251</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>-0.057963</td>\n",
       "      <td>-0.021385</td>\n",
       "      <td>0.043342</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>-0.006739</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.037606</td>\n",
       "      <td>-0.003123</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.043163</td>\n",
       "      <td>-0.035388</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>-0.013143</td>\n",
       "      <td>-0.035578</td>\n",
       "      <td>-0.050378</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.033277</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.034552</td>\n",
       "      <td>0.055608</td>\n",
       "      <td>0.026745</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>-0.049728</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>-0.032591</td>\n",
       "      <td>-0.049064</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.008510</td>\n",
       "      <td>-0.006228</td>\n",
       "      <td>-0.009495</td>\n",
       "      <td>-0.006116</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>-0.026851</td>\n",
       "      <td>-0.041777</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>-0.040167</td>\n",
       "      <td>-0.050369</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.014722</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>-0.023859</td>\n",
       "      <td>-0.046808</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>-0.020792</td>\n",
       "      <td>-0.001804</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>-0.012364</td>\n",
       "      <td>-0.031165</td>\n",
       "      <td>-0.010554</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>0.051521</td>\n",
       "      <td>-0.005691</td>\n",
       "      <td>-0.003245</td>\n",
       "      <td>0.057232</td>\n",
       "      <td>-0.079294</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>-0.013036</td>\n",
       "      <td>-0.012459</td>\n",
       "      <td>-0.060464</td>\n",
       "      <td>-0.009393</td>\n",
       "      <td>-0.053511</td>\n",
       "      <td>0.038751</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.026593</td>\n",
       "      <td>-0.089739</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>-0.023431</td>\n",
       "      <td>-0.046462</td>\n",
       "      <td>0.029822</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.069064</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.043045</td>\n",
       "      <td>-0.004415</td>\n",
       "      <td>-0.043049</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>-0.009260</td>\n",
       "      <td>-0.043302</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>0.003403</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.065873</td>\n",
       "      <td>-0.045331</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>-0.030392</td>\n",
       "      <td>-0.035519</td>\n",
       "      <td>-0.007047</td>\n",
       "      <td>-0.024870</td>\n",
       "      <td>-0.032678</td>\n",
       "      <td>-0.042166</td>\n",
       "      <td>0.048920</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>0.026165</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>-0.029102</td>\n",
       "      <td>-0.049491</td>\n",
       "      <td>-0.082290</td>\n",
       "      <td>0.026326</td>\n",
       "      <td>-0.019302</td>\n",
       "      <td>-0.048432</td>\n",
       "      <td>-0.055487</td>\n",
       "      <td>-0.010260</td>\n",
       "      <td>0.034458</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.014433</td>\n",
       "      <td>-0.010339</td>\n",
       "      <td>-0.008518</td>\n",
       "      <td>0.024141</td>\n",
       "      <td>-0.032981</td>\n",
       "      <td>-0.067955</td>\n",
       "      <td>-0.050547</td>\n",
       "      <td>0.052541</td>\n",
       "      <td>-0.015817</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.015150</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>0.023616</td>\n",
       "      <td>0.053558</td>\n",
       "      <td>-0.066518</td>\n",
       "      <td>-0.025649</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>-0.050877</td>\n",
       "      <td>-0.020718</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.067537</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.016293</td>\n",
       "      <td>-0.013182</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>-0.054192</td>\n",
       "      <td>-0.006398</td>\n",
       "      <td>-0.010536</td>\n",
       "      <td>-0.020308</td>\n",
       "      <td>0.058227</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.019161</td>\n",
       "      <td>-0.052709</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>-0.058244</td>\n",
       "      <td>-0.053822</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>0.051148</td>\n",
       "      <td>0.022017</td>\n",
       "      <td>-0.003954</td>\n",
       "      <td>-0.067461</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>0.038058</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.018230</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>-0.040157</td>\n",
       "      <td>-0.027095</td>\n",
       "      <td>-0.052737</td>\n",
       "      <td>-0.038650</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>-0.008378</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>0.028649</td>\n",
       "      <td>0.040239</td>\n",
       "      <td>0.024245</td>\n",
       "      <td>0.070656</td>\n",
       "      <td>-0.011620</td>\n",
       "      <td>-0.059243</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>-0.051657</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>-0.013779</td>\n",
       "      <td>-0.027204</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>-0.052142</td>\n",
       "      <td>0.029672</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>0.050352</td>\n",
       "      <td>0.029731</td>\n",
       "      <td>0.066881</td>\n",
       "      <td>-0.063964</td>\n",
       "      <td>-0.014514</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>-0.021431</td>\n",
       "      <td>0.074399</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>0.031231</td>\n",
       "      <td>0.023608</td>\n",
       "      <td>-0.038730</td>\n",
       "      <td>-0.032499</td>\n",
       "      <td>0.032016</td>\n",
       "      <td>-0.066692</td>\n",
       "      <td>-0.013078</td>\n",
       "      <td>-0.020541</td>\n",
       "      <td>-0.016344</td>\n",
       "      <td>-0.082868</td>\n",
       "      <td>-0.044288</td>\n",
       "      <td>-0.017430</td>\n",
       "      <td>-0.042802</td>\n",
       "      <td>-0.060323</td>\n",
       "      <td>0.016079</td>\n",
       "      <td>0.033834</td>\n",
       "      <td>-0.072490</td>\n",
       "      <td>-0.033961</td>\n",
       "      <td>0.025959</td>\n",
       "      <td>0.034973</td>\n",
       "      <td>0.030573</td>\n",
       "      <td>-0.022477</td>\n",
       "      <td>-0.033175</td>\n",
       "      <td>-0.030592</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>-0.068339</td>\n",
       "      <td>-0.036247</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>-0.033766</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>-0.023121</td>\n",
       "      <td>-0.066087</td>\n",
       "      <td>-0.042670</td>\n",
       "      <td>-0.035675</td>\n",
       "      <td>-0.043509</td>\n",
       "      <td>-0.010099</td>\n",
       "      <td>-0.066843</td>\n",
       "      <td>-0.015705</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>-0.004701</td>\n",
       "      <td>0.029062</td>\n",
       "      <td>-0.040716</td>\n",
       "      <td>0.060369</td>\n",
       "      <td>0.059726</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>-0.074127</td>\n",
       "      <td>-0.044512</td>\n",
       "      <td>-0.041543</td>\n",
       "      <td>0.027740</td>\n",
       "      <td>-0.050536</td>\n",
       "      <td>-0.024398</td>\n",
       "      <td>0.018480</td>\n",
       "      <td>-0.077055</td>\n",
       "      <td>0.055942</td>\n",
       "      <td>0.037710</td>\n",
       "      <td>-0.036036</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.022773</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.027950</td>\n",
       "      <td>-0.026780</td>\n",
       "      <td>-0.043733</td>\n",
       "      <td>-0.021582</td>\n",
       "      <td>0.041542</td>\n",
       "      <td>0.034917</td>\n",
       "      <td>-0.034974</td>\n",
       "      <td>-0.036343</td>\n",
       "      <td>-0.019457</td>\n",
       "      <td>-0.019818</td>\n",
       "      <td>-0.042045</td>\n",
       "      <td>-0.008003</td>\n",
       "      <td>0.029776</td>\n",
       "      <td>0.057665</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.033165</td>\n",
       "      <td>0.047233</td>\n",
       "      <td>0.042405</td>\n",
       "      <td>-0.042446</td>\n",
       "      <td>-0.015038</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>-0.068683</td>\n",
       "      <td>0.068838</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>-0.000853</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>-0.081806</td>\n",
       "      <td>-0.019911</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>-0.054334</td>\n",
       "      <td>-0.029874</td>\n",
       "      <td>-0.010412</td>\n",
       "      <td>-0.060177</td>\n",
       "      <td>0.051677</td>\n",
       "      <td>0.041015</td>\n",
       "      <td>-0.024855</td>\n",
       "      <td>-0.011949</td>\n",
       "      <td>-0.044116</td>\n",
       "      <td>-0.014593</td>\n",
       "      <td>-0.025919</td>\n",
       "      <td>-0.061265</td>\n",
       "      <td>-0.007169</td>\n",
       "      <td>0.018168</td>\n",
       "      <td>0.030079</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>0.023966</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>0.046755</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>-0.052076</td>\n",
       "      <td>-0.026423</td>\n",
       "      <td>-0.017431</td>\n",
       "      <td>-0.026269</td>\n",
       "      <td>0.027149</td>\n",
       "      <td>-0.014962</td>\n",
       "      <td>-0.004338</td>\n",
       "      <td>0.051730</td>\n",
       "      <td>-0.008679</td>\n",
       "      <td>-0.010420</td>\n",
       "      <td>-0.035210</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>-0.039272</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.034935</td>\n",
       "      <td>-0.067422</td>\n",
       "      <td>-0.018009</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>-0.004579</td>\n",
       "      <td>-0.067054</td>\n",
       "      <td>0.037026</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>-0.045728</td>\n",
       "      <td>-0.048394</td>\n",
       "      <td>-0.030780</td>\n",
       "      <td>-0.074595</td>\n",
       "      <td>0.019029</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.029871</td>\n",
       "      <td>-0.001668</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>-0.085181</td>\n",
       "      <td>-0.056523</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015715</td>\n",
       "      <td>-0.024961</td>\n",
       "      <td>-0.012671</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>-0.042361</td>\n",
       "      <td>0.076179</td>\n",
       "      <td>0.021334</td>\n",
       "      <td>-0.025536</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>-0.024533</td>\n",
       "      <td>-0.045116</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.062544</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>-0.035544</td>\n",
       "      <td>0.063520</td>\n",
       "      <td>-0.037774</td>\n",
       "      <td>-0.068834</td>\n",
       "      <td>0.024402</td>\n",
       "      <td>-0.039003</td>\n",
       "      <td>-0.007822</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>-0.040305</td>\n",
       "      <td>0.050261</td>\n",
       "      <td>-0.014737</td>\n",
       "      <td>-0.003474</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>-0.045799</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>-0.032458</td>\n",
       "      <td>0.077030</td>\n",
       "      <td>-0.001690</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>0.026819</td>\n",
       "      <td>-0.040538</td>\n",
       "      <td>-0.062240</td>\n",
       "      <td>-0.006705</td>\n",
       "      <td>-0.055694</td>\n",
       "      <td>-0.037191</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>-0.013722</td>\n",
       "      <td>-0.060653</td>\n",
       "      <td>-0.066045</td>\n",
       "      <td>-0.015517</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>-0.043011</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>-0.059165</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>-0.040612</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>-0.009979</td>\n",
       "      <td>-0.024970</td>\n",
       "      <td>0.035551</td>\n",
       "      <td>0.061707</td>\n",
       "      <td>-0.076678</td>\n",
       "      <td>-0.024015</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>-0.023932</td>\n",
       "      <td>0.034398</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>-0.057774</td>\n",
       "      <td>-0.019418</td>\n",
       "      <td>-0.039449</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.024801</td>\n",
       "      <td>-0.039026</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>-0.038913</td>\n",
       "      <td>-0.044096</td>\n",
       "      <td>0.027865</td>\n",
       "      <td>-0.014355</td>\n",
       "      <td>-0.009583</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.036662</td>\n",
       "      <td>-0.015503</td>\n",
       "      <td>-0.034403</td>\n",
       "      <td>0.027502</td>\n",
       "      <td>-0.011552</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>-0.029743</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>0.040959</td>\n",
       "      <td>-0.041394</td>\n",
       "      <td>-0.015465</td>\n",
       "      <td>-0.053601</td>\n",
       "      <td>-0.043798</td>\n",
       "      <td>-0.049690</td>\n",
       "      <td>-0.068626</td>\n",
       "      <td>-0.034806</td>\n",
       "      <td>-0.014614</td>\n",
       "      <td>-0.016748</td>\n",
       "      <td>-0.048023</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>-0.053831</td>\n",
       "      <td>0.035710</td>\n",
       "      <td>-0.040100</td>\n",
       "      <td>0.030246</td>\n",
       "      <td>-0.046397</td>\n",
       "      <td>-0.023452</td>\n",
       "      <td>-0.045955</td>\n",
       "      <td>-0.039339</td>\n",
       "      <td>-0.027649</td>\n",
       "      <td>-0.012623</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>0.049829</td>\n",
       "      <td>-0.030375</td>\n",
       "      <td>0.014987</td>\n",
       "      <td>-0.009591</td>\n",
       "      <td>-0.011586</td>\n",
       "      <td>-0.026485</td>\n",
       "      <td>0.060765</td>\n",
       "      <td>-0.053228</td>\n",
       "      <td>0.016675</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.055550</td>\n",
       "      <td>-0.063425</td>\n",
       "      <td>0.048862</td>\n",
       "      <td>-0.067048</td>\n",
       "      <td>-0.034671</td>\n",
       "      <td>-0.019439</td>\n",
       "      <td>-0.029480</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>-0.032935</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.060325</td>\n",
       "      <td>0.054141</td>\n",
       "      <td>-0.016188</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>-0.010343</td>\n",
       "      <td>0.034404</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>-0.012243</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.045913</td>\n",
       "      <td>0.029367</td>\n",
       "      <td>-0.016169</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>-0.004596</td>\n",
       "      <td>0.050391</td>\n",
       "      <td>-0.039973</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>-0.031544</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>-0.010299</td>\n",
       "      <td>0.031299</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.062524</td>\n",
       "      <td>-0.016833</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>0.058165</td>\n",
       "      <td>-0.022931</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>-0.038324</td>\n",
       "      <td>-0.005457</td>\n",
       "      <td>-0.003796</td>\n",
       "      <td>-0.005050</td>\n",
       "      <td>-0.045222</td>\n",
       "      <td>-0.008217</td>\n",
       "      <td>0.061915</td>\n",
       "      <td>0.019607</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>-0.047831</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>-0.062441</td>\n",
       "      <td>-0.087240</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>-0.017256</td>\n",
       "      <td>-0.033613</td>\n",
       "      <td>-0.011071</td>\n",
       "      <td>0.049141</td>\n",
       "      <td>-0.045517</td>\n",
       "      <td>-0.006953</td>\n",
       "      <td>0.042029</td>\n",
       "      <td>-0.030503</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>-0.040365</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.034939</td>\n",
       "      <td>0.033515</td>\n",
       "      <td>-0.035650</td>\n",
       "      <td>-0.045078</td>\n",
       "      <td>-0.029359</td>\n",
       "      <td>0.024757</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>0.008007</td>\n",
       "      <td>-0.035556</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>-0.043322</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.042928</td>\n",
       "      <td>-0.044471</td>\n",
       "      <td>-0.050518</td>\n",
       "      <td>0.039983</td>\n",
       "      <td>-0.018226</td>\n",
       "      <td>0.018707</td>\n",
       "      <td>-0.075105</td>\n",
       "      <td>-0.043529</td>\n",
       "      <td>-0.005360</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>-0.058748</td>\n",
       "      <td>-0.019642</td>\n",
       "      <td>0.027507</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>-0.008890</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>-0.038546</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>-0.005038</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>-0.023666</td>\n",
       "      <td>0.069415</td>\n",
       "      <td>0.032804</td>\n",
       "      <td>-0.035571</td>\n",
       "      <td>0.051074</td>\n",
       "      <td>-0.030803</td>\n",
       "      <td>-0.041638</td>\n",
       "      <td>0.019941</td>\n",
       "      <td>-0.015864</td>\n",
       "      <td>-0.046084</td>\n",
       "      <td>-0.065974</td>\n",
       "      <td>-0.043167</td>\n",
       "      <td>-0.016037</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>-0.073447</td>\n",
       "      <td>0.030275</td>\n",
       "      <td>-0.026399</td>\n",
       "      <td>-0.044892</td>\n",
       "      <td>-0.014220</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.026414</td>\n",
       "      <td>0.023692</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304795e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>58746.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.012415</td>\n",
       "      <td>-0.032357</td>\n",
       "      <td>-0.003216</td>\n",
       "      <td>-0.051747</td>\n",
       "      <td>0.027254</td>\n",
       "      <td>0.017501</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>0.025628</td>\n",
       "      <td>-0.076540</td>\n",
       "      <td>-0.037001</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>-0.017379</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>-0.069874</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>-0.069844</td>\n",
       "      <td>-0.006924</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>-0.019316</td>\n",
       "      <td>-0.022597</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>-0.056065</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>-0.061511</td>\n",
       "      <td>-0.008160</td>\n",
       "      <td>-0.019329</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>-0.005092</td>\n",
       "      <td>-0.003075</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>-0.053142</td>\n",
       "      <td>0.040678</td>\n",
       "      <td>-0.012526</td>\n",
       "      <td>-0.038992</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>-0.033641</td>\n",
       "      <td>-0.027966</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.044672</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.052543</td>\n",
       "      <td>0.025477</td>\n",
       "      <td>-0.007544</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.012305</td>\n",
       "      <td>0.034803</td>\n",
       "      <td>-0.035991</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>-0.013913</td>\n",
       "      <td>-0.009902</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-0.015773</td>\n",
       "      <td>-0.024805</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>-0.029689</td>\n",
       "      <td>-0.037778</td>\n",
       "      <td>-0.045778</td>\n",
       "      <td>0.040465</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>-0.030550</td>\n",
       "      <td>0.042470</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>0.037721</td>\n",
       "      <td>-0.034012</td>\n",
       "      <td>-0.061356</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>-0.016261</td>\n",
       "      <td>-0.060464</td>\n",
       "      <td>-0.022696</td>\n",
       "      <td>-0.024922</td>\n",
       "      <td>0.014629</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.037269</td>\n",
       "      <td>0.039240</td>\n",
       "      <td>-0.045430</td>\n",
       "      <td>-0.051789</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>0.012582</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.041235</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>-0.038494</td>\n",
       "      <td>-0.023607</td>\n",
       "      <td>0.024606</td>\n",
       "      <td>0.016334</td>\n",
       "      <td>-0.040178</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.059885</td>\n",
       "      <td>-0.005224</td>\n",
       "      <td>0.043395</td>\n",
       "      <td>0.051490</td>\n",
       "      <td>-0.007906</td>\n",
       "      <td>0.014598</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.029266</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.039741</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>-0.065606</td>\n",
       "      <td>0.028422</td>\n",
       "      <td>0.022238</td>\n",
       "      <td>0.030466</td>\n",
       "      <td>-0.069865</td>\n",
       "      <td>-0.069696</td>\n",
       "      <td>-0.032788</td>\n",
       "      <td>-0.018257</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>-0.033589</td>\n",
       "      <td>-0.021467</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>-0.041761</td>\n",
       "      <td>-0.007212</td>\n",
       "      <td>0.019007</td>\n",
       "      <td>-0.046112</td>\n",
       "      <td>0.035580</td>\n",
       "      <td>-0.040751</td>\n",
       "      <td>-0.021094</td>\n",
       "      <td>0.040147</td>\n",
       "      <td>-0.031103</td>\n",
       "      <td>-0.016132</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.015510</td>\n",
       "      <td>-0.023539</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>-0.006993</td>\n",
       "      <td>-0.004820</td>\n",
       "      <td>0.012151</td>\n",
       "      <td>0.057016</td>\n",
       "      <td>-0.028273</td>\n",
       "      <td>-0.039979</td>\n",
       "      <td>-0.070333</td>\n",
       "      <td>-0.056884</td>\n",
       "      <td>-0.028251</td>\n",
       "      <td>-0.033710</td>\n",
       "      <td>-0.029687</td>\n",
       "      <td>-0.031643</td>\n",
       "      <td>-0.046253</td>\n",
       "      <td>-0.066934</td>\n",
       "      <td>0.029965</td>\n",
       "      <td>0.062815</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>-0.025202</td>\n",
       "      <td>-0.006467</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.036854</td>\n",
       "      <td>0.036611</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>-0.069008</td>\n",
       "      <td>-0.036498</td>\n",
       "      <td>-0.034765</td>\n",
       "      <td>-0.032292</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>-0.026161</td>\n",
       "      <td>-0.038480</td>\n",
       "      <td>-0.014087</td>\n",
       "      <td>-0.015962</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.056834</td>\n",
       "      <td>-0.031295</td>\n",
       "      <td>-0.057769</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>-0.033881</td>\n",
       "      <td>-0.013165</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.024376</td>\n",
       "      <td>-0.022895</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>-0.040938</td>\n",
       "      <td>-0.047268</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>-0.020561</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>-0.027007</td>\n",
       "      <td>0.020262</td>\n",
       "      <td>0.011959</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.034593</td>\n",
       "      <td>0.047163</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>-0.046822</td>\n",
       "      <td>-0.005476</td>\n",
       "      <td>0.059302</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>-0.015005</td>\n",
       "      <td>0.028240</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>-0.034700</td>\n",
       "      <td>-0.021105</td>\n",
       "      <td>0.020945</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.018599</td>\n",
       "      <td>-0.013190</td>\n",
       "      <td>-0.021110</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>-0.039836</td>\n",
       "      <td>0.035668</td>\n",
       "      <td>-0.057073</td>\n",
       "      <td>0.057995</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.074602</td>\n",
       "      <td>-0.040854</td>\n",
       "      <td>-0.065541</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.017608</td>\n",
       "      <td>-0.018122</td>\n",
       "      <td>-0.081354</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>-0.016465</td>\n",
       "      <td>-0.030215</td>\n",
       "      <td>-0.010478</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>-0.030309</td>\n",
       "      <td>0.046174</td>\n",
       "      <td>0.024959</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>0.039746</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>-0.042187</td>\n",
       "      <td>-0.029751</td>\n",
       "      <td>0.058508</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>-0.029876</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>0.034398</td>\n",
       "      <td>-0.054379</td>\n",
       "      <td>0.009520</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>-0.021807</td>\n",
       "      <td>-0.014395</td>\n",
       "      <td>-0.006513</td>\n",
       "      <td>0.038608</td>\n",
       "      <td>-0.051181</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>-0.048581</td>\n",
       "      <td>-0.044484</td>\n",
       "      <td>0.054334</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>0.013860</td>\n",
       "      <td>-0.077872</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>-0.034617</td>\n",
       "      <td>-0.044730</td>\n",
       "      <td>-0.059697</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>-0.018007</td>\n",
       "      <td>0.075957</td>\n",
       "      <td>-0.030186</td>\n",
       "      <td>-0.026742</td>\n",
       "      <td>-0.018267</td>\n",
       "      <td>0.048242</td>\n",
       "      <td>-0.002874</td>\n",
       "      <td>-0.062575</td>\n",
       "      <td>-0.020571</td>\n",
       "      <td>-0.005177</td>\n",
       "      <td>0.016267</td>\n",
       "      <td>0.011860</td>\n",
       "      <td>0.036161</td>\n",
       "      <td>-0.012050</td>\n",
       "      <td>0.014347</td>\n",
       "      <td>-0.004388</td>\n",
       "      <td>-0.021748</td>\n",
       "      <td>-0.022410</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-0.017778</td>\n",
       "      <td>-0.021504</td>\n",
       "      <td>-0.020692</td>\n",
       "      <td>-0.037629</td>\n",
       "      <td>0.041481</td>\n",
       "      <td>-0.029364</td>\n",
       "      <td>-0.041790</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>-0.006792</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>-0.038216</td>\n",
       "      <td>0.023683</td>\n",
       "      <td>0.043142</td>\n",
       "      <td>-0.038172</td>\n",
       "      <td>-0.069311</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>-0.066833</td>\n",
       "      <td>0.030748</td>\n",
       "      <td>-0.036074</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.006826</td>\n",
       "      <td>0.038987</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.029801</td>\n",
       "      <td>-0.061734</td>\n",
       "      <td>-0.034215</td>\n",
       "      <td>-0.034809</td>\n",
       "      <td>0.044035</td>\n",
       "      <td>0.050266</td>\n",
       "      <td>-0.059598</td>\n",
       "      <td>-0.041459</td>\n",
       "      <td>-0.010368</td>\n",
       "      <td>0.065164</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.031548</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>-0.030222</td>\n",
       "      <td>0.044417</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>-0.065902</td>\n",
       "      <td>-0.059039</td>\n",
       "      <td>-0.072675</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>-0.054349</td>\n",
       "      <td>0.014285</td>\n",
       "      <td>0.042173</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>-0.000467</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.044382</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>-0.050245</td>\n",
       "      <td>-0.022704</td>\n",
       "      <td>0.062202</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>-0.028636</td>\n",
       "      <td>0.023736</td>\n",
       "      <td>-0.069240</td>\n",
       "      <td>0.017473</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>-0.028994</td>\n",
       "      <td>0.025293</td>\n",
       "      <td>0.044596</td>\n",
       "      <td>-0.007748</td>\n",
       "      <td>-0.033838</td>\n",
       "      <td>-0.014207</td>\n",
       "      <td>0.050722</td>\n",
       "      <td>0.012569</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>-0.024039</td>\n",
       "      <td>-0.010729</td>\n",
       "      <td>-0.024232</td>\n",
       "      <td>-0.024182</td>\n",
       "      <td>-0.057425</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>-0.030658</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.018967</td>\n",
       "      <td>-0.042958</td>\n",
       "      <td>-0.078298</td>\n",
       "      <td>-0.049557</td>\n",
       "      <td>-0.042589</td>\n",
       "      <td>-0.069088</td>\n",
       "      <td>-0.045939</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.054872</td>\n",
       "      <td>-0.034204</td>\n",
       "      <td>-0.015421</td>\n",
       "      <td>0.028170</td>\n",
       "      <td>0.063644</td>\n",
       "      <td>-0.014624</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>-0.038929</td>\n",
       "      <td>-0.003993</td>\n",
       "      <td>-0.014354</td>\n",
       "      <td>0.041989</td>\n",
       "      <td>-0.044893</td>\n",
       "      <td>-0.039765</td>\n",
       "      <td>-0.000757</td>\n",
       "      <td>-0.067462</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.011468</td>\n",
       "      <td>-0.064382</td>\n",
       "      <td>-0.026811</td>\n",
       "      <td>-0.008620</td>\n",
       "      <td>-0.034056</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.026489</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>-0.003901</td>\n",
       "      <td>-0.010155</td>\n",
       "      <td>-0.001700</td>\n",
       "      <td>0.048769</td>\n",
       "      <td>0.047941</td>\n",
       "      <td>-0.030793</td>\n",
       "      <td>0.045149</td>\n",
       "      <td>-0.045497</td>\n",
       "      <td>-0.029377</td>\n",
       "      <td>-0.021335</td>\n",
       "      <td>-0.041732</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.032710</td>\n",
       "      <td>0.033064</td>\n",
       "      <td>-0.064384</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.035646</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>-0.054057</td>\n",
       "      <td>-0.037450</td>\n",
       "      <td>0.036423</td>\n",
       "      <td>0.035801</td>\n",
       "      <td>-0.001905</td>\n",
       "      <td>0.018992</td>\n",
       "      <td>-0.010711</td>\n",
       "      <td>0.042425</td>\n",
       "      <td>0.050421</td>\n",
       "      <td>-0.063924</td>\n",
       "      <td>-0.036919</td>\n",
       "      <td>-0.040148</td>\n",
       "      <td>0.034210</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.019884</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>-0.026894</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.022168</td>\n",
       "      <td>0.043360</td>\n",
       "      <td>-0.034301</td>\n",
       "      <td>0.031476</td>\n",
       "      <td>0.044415</td>\n",
       "      <td>-0.062098</td>\n",
       "      <td>-0.013322</td>\n",
       "      <td>0.035706</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>-0.028577</td>\n",
       "      <td>0.030244</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>-0.055833</td>\n",
       "      <td>-0.053326</td>\n",
       "      <td>-0.060028</td>\n",
       "      <td>0.015801</td>\n",
       "      <td>-0.017981</td>\n",
       "      <td>-0.037173</td>\n",
       "      <td>0.014053</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>0.057975</td>\n",
       "      <td>0.029556</td>\n",
       "      <td>-0.054920</td>\n",
       "      <td>-0.004234</td>\n",
       "      <td>-0.011067</td>\n",
       "      <td>0.045467</td>\n",
       "      <td>-0.025380</td>\n",
       "      <td>-0.012809</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.049449</td>\n",
       "      <td>0.054997</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>0.035135</td>\n",
       "      <td>-0.013567</td>\n",
       "      <td>-0.042255</td>\n",
       "      <td>-0.015039</td>\n",
       "      <td>-0.032442</td>\n",
       "      <td>0.012681</td>\n",
       "      <td>-0.017328</td>\n",
       "      <td>0.016732</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.031414</td>\n",
       "      <td>0.031735</td>\n",
       "      <td>-0.018684</td>\n",
       "      <td>-0.017713</td>\n",
       "      <td>-0.053451</td>\n",
       "      <td>-0.028899</td>\n",
       "      <td>-0.029981</td>\n",
       "      <td>-0.012296</td>\n",
       "      <td>0.013980</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>0.035472</td>\n",
       "      <td>0.017066</td>\n",
       "      <td>-0.007163</td>\n",
       "      <td>-0.037021</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>-0.005736</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>-0.011976</td>\n",
       "      <td>-0.012992</td>\n",
       "      <td>-0.067666</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.036582</td>\n",
       "      <td>0.063653</td>\n",
       "      <td>-0.016117</td>\n",
       "      <td>-0.031707</td>\n",
       "      <td>-0.070545</td>\n",
       "      <td>-0.028672</td>\n",
       "      <td>-0.016142</td>\n",
       "      <td>-0.037535</td>\n",
       "      <td>0.024257</td>\n",
       "      <td>-0.015881</td>\n",
       "      <td>0.035338</td>\n",
       "      <td>-0.027082</td>\n",
       "      <td>0.034739</td>\n",
       "      <td>0.013127</td>\n",
       "      <td>-0.014858</td>\n",
       "      <td>0.029429</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>-0.044343</td>\n",
       "      <td>-0.014829</td>\n",
       "      <td>-0.006513</td>\n",
       "      <td>-0.022683</td>\n",
       "      <td>0.038416</td>\n",
       "      <td>0.041620</td>\n",
       "      <td>0.064459</td>\n",
       "      <td>-0.039872</td>\n",
       "      <td>0.061613</td>\n",
       "      <td>-0.042957</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.031210</td>\n",
       "      <td>-0.039890</td>\n",
       "      <td>-0.022487</td>\n",
       "      <td>-0.001909</td>\n",
       "      <td>-0.024828</td>\n",
       "      <td>-0.015127</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>-0.031402</td>\n",
       "      <td>0.022955</td>\n",
       "      <td>-0.018509</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>-0.050349</td>\n",
       "      <td>-0.065345</td>\n",
       "      <td>0.023599</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>0.026402</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>-0.012851</td>\n",
       "      <td>0.017554</td>\n",
       "      <td>0.040091</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>-0.052622</td>\n",
       "      <td>-0.006762</td>\n",
       "      <td>-0.039172</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.053123</td>\n",
       "      <td>-0.021578</td>\n",
       "      <td>-0.052645</td>\n",
       "      <td>-0.068273</td>\n",
       "      <td>-0.022926</td>\n",
       "      <td>-0.015902</td>\n",
       "      <td>-0.041432</td>\n",
       "      <td>-0.040667</td>\n",
       "      <td>-0.069364</td>\n",
       "      <td>0.013315</td>\n",
       "      <td>-0.026633</td>\n",
       "      <td>-0.046440</td>\n",
       "      <td>0.051262</td>\n",
       "      <td>0.032154</td>\n",
       "      <td>-0.043774</td>\n",
       "      <td>-0.016167</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.052867</td>\n",
       "      <td>-0.007713</td>\n",
       "      <td>0.018891</td>\n",
       "      <td>0.025292</td>\n",
       "      <td>-0.011269</td>\n",
       "      <td>-0.040812</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>-0.005320</td>\n",
       "      <td>0.028203</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.029359</td>\n",
       "      <td>-0.034545</td>\n",
       "      <td>-0.061794</td>\n",
       "      <td>-0.058531</td>\n",
       "      <td>-0.030271</td>\n",
       "      <td>-0.051671</td>\n",
       "      <td>0.033150</td>\n",
       "      <td>0.023626</td>\n",
       "      <td>-0.019329</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>-0.056636</td>\n",
       "      <td>-0.020651</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>-0.019423</td>\n",
       "      <td>-0.030430</td>\n",
       "      <td>-0.038732</td>\n",
       "      <td>-0.024972</td>\n",
       "      <td>-0.013444</td>\n",
       "      <td>0.058141</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.020304</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>0.016561</td>\n",
       "      <td>-0.069910</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>-0.040205</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-0.051796</td>\n",
       "      <td>-0.028993</td>\n",
       "      <td>-0.060027</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.040879</td>\n",
       "      <td>-0.051498</td>\n",
       "      <td>-0.006144</td>\n",
       "      <td>-0.033865</td>\n",
       "      <td>-0.055437</td>\n",
       "      <td>-0.048156</td>\n",
       "      <td>-0.040606</td>\n",
       "      <td>-0.048795</td>\n",
       "      <td>-0.043679</td>\n",
       "      <td>-0.022880</td>\n",
       "      <td>-0.030084</td>\n",
       "      <td>-0.072675</td>\n",
       "      <td>-0.012751</td>\n",
       "      <td>-0.029991</td>\n",
       "      <td>0.061299</td>\n",
       "      <td>-0.056816</td>\n",
       "      <td>-0.002892</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>-0.036331</td>\n",
       "      <td>0.040686</td>\n",
       "      <td>-0.063263</td>\n",
       "      <td>0.015379</td>\n",
       "      <td>-0.048215</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>0.050578</td>\n",
       "      <td>-0.069272</td>\n",
       "      <td>0.044709</td>\n",
       "      <td>-0.052623</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>-0.042926</td>\n",
       "      <td>-0.051903</td>\n",
       "      <td>-0.032711</td>\n",
       "      <td>-0.054837</td>\n",
       "      <td>-0.022148</td>\n",
       "      <td>0.024751</td>\n",
       "      <td>0.051745</td>\n",
       "      <td>-0.051832</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>-0.035666</td>\n",
       "      <td>-0.044307</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>-0.042235</td>\n",
       "      <td>0.059724</td>\n",
       "      <td>-0.042001</td>\n",
       "      <td>-0.046602</td>\n",
       "      <td>-0.056383</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>0.047642</td>\n",
       "      <td>0.046315</td>\n",
       "      <td>-0.008817</td>\n",
       "      <td>-0.028448</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>-0.030082</td>\n",
       "      <td>-0.015233</td>\n",
       "      <td>-0.054357</td>\n",
       "      <td>-0.055159</td>\n",
       "      <td>0.043098</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>0.054177</td>\n",
       "      <td>-0.017742</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.034210</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>0.053110</td>\n",
       "      <td>-0.040369</td>\n",
       "      <td>-0.004762</td>\n",
       "      <td>0.036295</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>-0.046518</td>\n",
       "      <td>-0.051892</td>\n",
       "      <td>0.026491</td>\n",
       "      <td>-0.016430</td>\n",
       "      <td>-0.031188</td>\n",
       "      <td>-0.056881</td>\n",
       "      <td>-0.058733</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.059658</td>\n",
       "      <td>-0.050335</td>\n",
       "      <td>-0.080230</td>\n",
       "      <td>-0.011831</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.073854</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.058198</td>\n",
       "      <td>-0.037490</td>\n",
       "      <td>-0.043100</td>\n",
       "      <td>0.051441</td>\n",
       "      <td>-0.047889</td>\n",
       "      <td>-0.054388</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>-0.021159</td>\n",
       "      <td>0.057263</td>\n",
       "      <td>-0.025591</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>-0.066811</td>\n",
       "      <td>-0.048771</td>\n",
       "      <td>-0.045087</td>\n",
       "      <td>0.009543</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>-0.038227</td>\n",
       "      <td>-0.040773</td>\n",
       "      <td>-0.049714</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>-0.075023</td>\n",
       "      <td>-0.039188</td>\n",
       "      <td>-0.003765</td>\n",
       "      <td>0.040923</td>\n",
       "      <td>-0.057215</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>-0.050240</td>\n",
       "      <td>-0.040368</td>\n",
       "      <td>-0.060560</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>-0.064781</td>\n",
       "      <td>-0.017414</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>-0.035831</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>-0.006664</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>0.039847</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>-0.015462</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>0.023723</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>0.025077</td>\n",
       "      <td>0.067034</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>-0.016186</td>\n",
       "      <td>0.024116</td>\n",
       "      <td>-0.036549</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>-0.072148</td>\n",
       "      <td>-0.030169</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>-0.042314</td>\n",
       "      <td>-0.040760</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>-0.026526</td>\n",
       "      <td>0.010523</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.030677</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>-0.000807</td>\n",
       "      <td>-0.029937</td>\n",
       "      <td>-0.075138</td>\n",
       "      <td>-0.037401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304794e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>58746.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.821918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1380.216667</td>\n",
       "      <td>38.853925</td>\n",
       "      <td>5.060942</td>\n",
       "      <td>35.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.48708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>1.860427</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>7.08</td>\n",
       "      <td>12.82</td>\n",
       "      <td>10.60274</td>\n",
       "      <td>5.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.054881</td>\n",
       "      <td>-0.023647</td>\n",
       "      <td>-0.010028</td>\n",
       "      <td>-0.047675</td>\n",
       "      <td>0.034321</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>-0.086250</td>\n",
       "      <td>-0.049660</td>\n",
       "      <td>-0.037532</td>\n",
       "      <td>-0.036105</td>\n",
       "      <td>-0.018089</td>\n",
       "      <td>-0.064382</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>-0.057219</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>-0.022639</td>\n",
       "      <td>0.042465</td>\n",
       "      <td>-0.055263</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>-0.035339</td>\n",
       "      <td>0.016142</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.048463</td>\n",
       "      <td>0.030750</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>-0.039728</td>\n",
       "      <td>0.017460</td>\n",
       "      <td>-0.004436</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>-0.054100</td>\n",
       "      <td>-0.059679</td>\n",
       "      <td>0.022575</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>-0.075584</td>\n",
       "      <td>0.029238</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>-0.035654</td>\n",
       "      <td>-0.027266</td>\n",
       "      <td>0.043231</td>\n",
       "      <td>-0.020378</td>\n",
       "      <td>-0.047516</td>\n",
       "      <td>0.047808</td>\n",
       "      <td>-0.017460</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.018869</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>-0.029885</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.053657</td>\n",
       "      <td>-0.037753</td>\n",
       "      <td>-0.062391</td>\n",
       "      <td>-0.041248</td>\n",
       "      <td>0.041752</td>\n",
       "      <td>0.057789</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.028687</td>\n",
       "      <td>-0.007140</td>\n",
       "      <td>0.021609</td>\n",
       "      <td>-0.045735</td>\n",
       "      <td>-0.050799</td>\n",
       "      <td>-0.026367</td>\n",
       "      <td>-0.024023</td>\n",
       "      <td>-0.063793</td>\n",
       "      <td>-0.031616</td>\n",
       "      <td>-0.061337</td>\n",
       "      <td>-0.061695</td>\n",
       "      <td>-0.058365</td>\n",
       "      <td>-0.065402</td>\n",
       "      <td>-0.004568</td>\n",
       "      <td>-0.018722</td>\n",
       "      <td>-0.047516</td>\n",
       "      <td>-0.015339</td>\n",
       "      <td>-0.011169</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>-0.031914</td>\n",
       "      <td>0.018203</td>\n",
       "      <td>-0.042064</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.061937</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>-0.051947</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>0.037559</td>\n",
       "      <td>-0.038491</td>\n",
       "      <td>-0.014307</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.024144</td>\n",
       "      <td>0.040089</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>-0.061694</td>\n",
       "      <td>0.020459</td>\n",
       "      <td>0.024173</td>\n",
       "      <td>0.054541</td>\n",
       "      <td>-0.061544</td>\n",
       "      <td>-0.075089</td>\n",
       "      <td>-0.036567</td>\n",
       "      <td>0.062974</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>-0.054939</td>\n",
       "      <td>-0.041075</td>\n",
       "      <td>0.046513</td>\n",
       "      <td>-0.072019</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>-0.000345</td>\n",
       "      <td>-0.022083</td>\n",
       "      <td>-0.038334</td>\n",
       "      <td>0.028651</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>-0.003197</td>\n",
       "      <td>0.024168</td>\n",
       "      <td>0.040932</td>\n",
       "      <td>0.071355</td>\n",
       "      <td>0.014250</td>\n",
       "      <td>-0.015070</td>\n",
       "      <td>-0.023656</td>\n",
       "      <td>-0.032432</td>\n",
       "      <td>-0.028231</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.029276</td>\n",
       "      <td>-0.014107</td>\n",
       "      <td>-0.063586</td>\n",
       "      <td>-0.066848</td>\n",
       "      <td>-0.027840</td>\n",
       "      <td>0.020189</td>\n",
       "      <td>-0.053970</td>\n",
       "      <td>0.046559</td>\n",
       "      <td>-0.008395</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>-0.054333</td>\n",
       "      <td>0.071043</td>\n",
       "      <td>0.034798</td>\n",
       "      <td>-0.012342</td>\n",
       "      <td>0.024077</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>-0.007022</td>\n",
       "      <td>-0.008878</td>\n",
       "      <td>0.043638</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.054078</td>\n",
       "      <td>-0.043187</td>\n",
       "      <td>0.004041</td>\n",
       "      <td>-0.033467</td>\n",
       "      <td>-0.037586</td>\n",
       "      <td>-0.006985</td>\n",
       "      <td>0.025494</td>\n",
       "      <td>0.018199</td>\n",
       "      <td>0.025948</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>-0.014204</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.053319</td>\n",
       "      <td>-0.028740</td>\n",
       "      <td>0.030350</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>-0.044594</td>\n",
       "      <td>0.036331</td>\n",
       "      <td>0.045419</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>-0.054523</td>\n",
       "      <td>-0.043617</td>\n",
       "      <td>-0.029892</td>\n",
       "      <td>-0.015337</td>\n",
       "      <td>0.017519</td>\n",
       "      <td>-0.036844</td>\n",
       "      <td>-0.004541</td>\n",
       "      <td>-0.025270</td>\n",
       "      <td>0.055876</td>\n",
       "      <td>-0.016467</td>\n",
       "      <td>-0.007058</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>-0.068118</td>\n",
       "      <td>0.034601</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.014992</td>\n",
       "      <td>0.043118</td>\n",
       "      <td>-0.027265</td>\n",
       "      <td>-0.054553</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.014071</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>-0.064358</td>\n",
       "      <td>0.032144</td>\n",
       "      <td>-0.042294</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>-0.077035</td>\n",
       "      <td>-0.054018</td>\n",
       "      <td>-0.048548</td>\n",
       "      <td>-0.050309</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>0.035732</td>\n",
       "      <td>-0.091933</td>\n",
       "      <td>0.036436</td>\n",
       "      <td>-0.016304</td>\n",
       "      <td>-0.031171</td>\n",
       "      <td>0.015011</td>\n",
       "      <td>-0.036591</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>-0.013091</td>\n",
       "      <td>0.064677</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>-0.043697</td>\n",
       "      <td>0.019829</td>\n",
       "      <td>-0.007790</td>\n",
       "      <td>-0.056596</td>\n",
       "      <td>0.036173</td>\n",
       "      <td>0.064855</td>\n",
       "      <td>-0.020905</td>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.053960</td>\n",
       "      <td>-0.061140</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>-0.038366</td>\n",
       "      <td>-0.032508</td>\n",
       "      <td>-0.044673</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>-0.009847</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>-0.059702</td>\n",
       "      <td>-0.033976</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.019840</td>\n",
       "      <td>-0.019124</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.025751</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>-0.004765</td>\n",
       "      <td>-0.046062</td>\n",
       "      <td>-0.020823</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.085431</td>\n",
       "      <td>-0.037022</td>\n",
       "      <td>-0.050355</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>0.038924</td>\n",
       "      <td>-0.047586</td>\n",
       "      <td>-0.071209</td>\n",
       "      <td>-0.044032</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.037372</td>\n",
       "      <td>-0.014546</td>\n",
       "      <td>0.028268</td>\n",
       "      <td>-0.017059</td>\n",
       "      <td>0.023508</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>-0.061794</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.071932</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>-0.063502</td>\n",
       "      <td>-0.039313</td>\n",
       "      <td>-0.008857</td>\n",
       "      <td>0.066541</td>\n",
       "      <td>0.025489</td>\n",
       "      <td>-0.019589</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>-0.036118</td>\n",
       "      <td>0.023459</td>\n",
       "      <td>-0.035730</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.026513</td>\n",
       "      <td>-0.036824</td>\n",
       "      <td>-0.029555</td>\n",
       "      <td>-0.009097</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.008972</td>\n",
       "      <td>-0.010894</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.033000</td>\n",
       "      <td>-0.004466</td>\n",
       "      <td>-0.019519</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>-0.077665</td>\n",
       "      <td>-0.060188</td>\n",
       "      <td>-0.012254</td>\n",
       "      <td>0.056199</td>\n",
       "      <td>0.065949</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>-0.070428</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>-0.018742</td>\n",
       "      <td>0.033919</td>\n",
       "      <td>0.052279</td>\n",
       "      <td>-0.025325</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.038935</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>-0.063535</td>\n",
       "      <td>-0.031420</td>\n",
       "      <td>-0.060079</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>-0.035623</td>\n",
       "      <td>0.053268</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>0.028629</td>\n",
       "      <td>-0.026768</td>\n",
       "      <td>-0.029447</td>\n",
       "      <td>0.032630</td>\n",
       "      <td>-0.038669</td>\n",
       "      <td>-0.025306</td>\n",
       "      <td>-0.002398</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.016042</td>\n",
       "      <td>-0.029711</td>\n",
       "      <td>-0.016850</td>\n",
       "      <td>-0.025724</td>\n",
       "      <td>0.024141</td>\n",
       "      <td>0.062058</td>\n",
       "      <td>0.042931</td>\n",
       "      <td>0.043269</td>\n",
       "      <td>-0.035548</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.007725</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.041026</td>\n",
       "      <td>0.036212</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>-0.019648</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.048574</td>\n",
       "      <td>-0.037337</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>-0.019469</td>\n",
       "      <td>-0.079375</td>\n",
       "      <td>-0.063959</td>\n",
       "      <td>0.025661</td>\n",
       "      <td>-0.059434</td>\n",
       "      <td>-0.039322</td>\n",
       "      <td>-0.011829</td>\n",
       "      <td>0.037479</td>\n",
       "      <td>-0.017828</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>-0.023232</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>-0.002231</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>-0.028282</td>\n",
       "      <td>0.030304</td>\n",
       "      <td>-0.008350</td>\n",
       "      <td>0.065973</td>\n",
       "      <td>-0.007827</td>\n",
       "      <td>-0.025300</td>\n",
       "      <td>0.054881</td>\n",
       "      <td>-0.042662</td>\n",
       "      <td>-0.003364</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>-0.045828</td>\n",
       "      <td>-0.026556</td>\n",
       "      <td>-0.026906</td>\n",
       "      <td>-0.019873</td>\n",
       "      <td>-0.038703</td>\n",
       "      <td>-0.061755</td>\n",
       "      <td>-0.037111</td>\n",
       "      <td>0.028138</td>\n",
       "      <td>0.025494</td>\n",
       "      <td>-0.023941</td>\n",
       "      <td>-0.026760</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>0.019285</td>\n",
       "      <td>0.026527</td>\n",
       "      <td>-0.006441</td>\n",
       "      <td>-0.062933</td>\n",
       "      <td>-0.008385</td>\n",
       "      <td>-0.007794</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>-0.055790</td>\n",
       "      <td>0.024424</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>-0.007982</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>0.043236</td>\n",
       "      <td>-0.040784</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>0.066415</td>\n",
       "      <td>-0.007824</td>\n",
       "      <td>-0.020967</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>-0.015308</td>\n",
       "      <td>0.020027</td>\n",
       "      <td>-0.037675</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.044218</td>\n",
       "      <td>-0.049950</td>\n",
       "      <td>-0.041732</td>\n",
       "      <td>-0.047035</td>\n",
       "      <td>-0.033882</td>\n",
       "      <td>0.024838</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.042439</td>\n",
       "      <td>-0.047982</td>\n",
       "      <td>-0.023436</td>\n",
       "      <td>0.069657</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.053325</td>\n",
       "      <td>-0.057066</td>\n",
       "      <td>0.034266</td>\n",
       "      <td>-0.062832</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>-0.009303</td>\n",
       "      <td>0.035855</td>\n",
       "      <td>0.019821</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.045036</td>\n",
       "      <td>-0.043001</td>\n",
       "      <td>-0.024296</td>\n",
       "      <td>-0.041737</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>-0.028926</td>\n",
       "      <td>-0.013747</td>\n",
       "      <td>-0.004190</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>-0.027855</td>\n",
       "      <td>0.062042</td>\n",
       "      <td>0.057830</td>\n",
       "      <td>-0.017357</td>\n",
       "      <td>-0.002691</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.020804</td>\n",
       "      <td>-0.002348</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.047149</td>\n",
       "      <td>0.051695</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.066303</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.061075</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>-0.004727</td>\n",
       "      <td>-0.007571</td>\n",
       "      <td>0.054487</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>-0.027595</td>\n",
       "      <td>0.049119</td>\n",
       "      <td>0.024895</td>\n",
       "      <td>0.039589</td>\n",
       "      <td>-0.005180</td>\n",
       "      <td>-0.030776</td>\n",
       "      <td>-0.027065</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.035560</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>-0.028919</td>\n",
       "      <td>0.031606</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.029746</td>\n",
       "      <td>-0.020197</td>\n",
       "      <td>0.016982</td>\n",
       "      <td>-0.044610</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>-0.032372</td>\n",
       "      <td>0.040891</td>\n",
       "      <td>-0.008191</td>\n",
       "      <td>-0.015572</td>\n",
       "      <td>-0.084396</td>\n",
       "      <td>-0.016877</td>\n",
       "      <td>-0.034199</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.020157</td>\n",
       "      <td>0.035027</td>\n",
       "      <td>-0.012004</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.042955</td>\n",
       "      <td>0.031588</td>\n",
       "      <td>0.038404</td>\n",
       "      <td>0.034823</td>\n",
       "      <td>-0.052799</td>\n",
       "      <td>-0.052401</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.056452</td>\n",
       "      <td>0.058365</td>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>-0.042657</td>\n",
       "      <td>0.067626</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.042008</td>\n",
       "      <td>0.029812</td>\n",
       "      <td>-0.057214</td>\n",
       "      <td>-0.015844</td>\n",
       "      <td>-0.029933</td>\n",
       "      <td>-0.030955</td>\n",
       "      <td>0.032629</td>\n",
       "      <td>-0.005154</td>\n",
       "      <td>-0.011434</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>-0.020914</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>-0.064025</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.025851</td>\n",
       "      <td>-0.015046</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.079044</td>\n",
       "      <td>-0.016647</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.043950</td>\n",
       "      <td>-0.014483</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>-0.060449</td>\n",
       "      <td>-0.031162</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.028506</td>\n",
       "      <td>-0.036522</td>\n",
       "      <td>-0.040901</td>\n",
       "      <td>-0.066498</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>-0.054073</td>\n",
       "      <td>-0.070110</td>\n",
       "      <td>-0.068026</td>\n",
       "      <td>-0.038565</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.023650</td>\n",
       "      <td>0.048563</td>\n",
       "      <td>0.060248</td>\n",
       "      <td>-0.008320</td>\n",
       "      <td>-0.020068</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.042014</td>\n",
       "      <td>-0.007218</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>0.064126</td>\n",
       "      <td>0.026652</td>\n",
       "      <td>-0.071584</td>\n",
       "      <td>-0.028738</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.048558</td>\n",
       "      <td>0.023424</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>0.047301</td>\n",
       "      <td>-0.017419</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>-0.027638</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>-0.022966</td>\n",
       "      <td>-0.025931</td>\n",
       "      <td>-0.017633</td>\n",
       "      <td>-0.022676</td>\n",
       "      <td>0.052170</td>\n",
       "      <td>-0.040386</td>\n",
       "      <td>0.019453</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>-0.045418</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.054025</td>\n",
       "      <td>-0.027351</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.003595</td>\n",
       "      <td>-0.036281</td>\n",
       "      <td>-0.024828</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.040060</td>\n",
       "      <td>-0.016915</td>\n",
       "      <td>-0.045029</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>-0.042960</td>\n",
       "      <td>-0.039728</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>-0.007777</td>\n",
       "      <td>-0.024108</td>\n",
       "      <td>-0.023775</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>-0.063811</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>-0.033360</td>\n",
       "      <td>-0.028815</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>-0.027291</td>\n",
       "      <td>-0.055037</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>-0.021793</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>-0.042828</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>-0.032198</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>-0.035848</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>-0.025492</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>-0.056653</td>\n",
       "      <td>0.041571</td>\n",
       "      <td>-0.057128</td>\n",
       "      <td>0.041394</td>\n",
       "      <td>-0.052616</td>\n",
       "      <td>-0.043863</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>-0.053773</td>\n",
       "      <td>-0.027309</td>\n",
       "      <td>0.062212</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>-0.023669</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>-0.008739</td>\n",
       "      <td>-0.019718</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>-0.019890</td>\n",
       "      <td>-0.024340</td>\n",
       "      <td>-0.011125</td>\n",
       "      <td>-0.022231</td>\n",
       "      <td>0.017658</td>\n",
       "      <td>-0.021805</td>\n",
       "      <td>-0.003099</td>\n",
       "      <td>-0.025080</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>-0.062823</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>0.031108</td>\n",
       "      <td>-0.057578</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>-0.040028</td>\n",
       "      <td>-0.004269</td>\n",
       "      <td>-0.023626</td>\n",
       "      <td>0.038350</td>\n",
       "      <td>-0.013341</td>\n",
       "      <td>0.041302</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.046680</td>\n",
       "      <td>0.021299</td>\n",
       "      <td>-0.036904</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>0.045327</td>\n",
       "      <td>-0.021670</td>\n",
       "      <td>-0.024917</td>\n",
       "      <td>-0.010478</td>\n",
       "      <td>-0.035350</td>\n",
       "      <td>-0.059572</td>\n",
       "      <td>0.042985</td>\n",
       "      <td>0.047083</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>-0.012715</td>\n",
       "      <td>-0.051293</td>\n",
       "      <td>0.022870</td>\n",
       "      <td>0.041177</td>\n",
       "      <td>-0.023152</td>\n",
       "      <td>-0.077932</td>\n",
       "      <td>-0.014958</td>\n",
       "      <td>0.023975</td>\n",
       "      <td>-0.063354</td>\n",
       "      <td>-0.006642</td>\n",
       "      <td>-0.008652</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>0.041365</td>\n",
       "      <td>-0.059022</td>\n",
       "      <td>-0.039613</td>\n",
       "      <td>-0.032852</td>\n",
       "      <td>-0.027261</td>\n",
       "      <td>0.045291</td>\n",
       "      <td>0.026285</td>\n",
       "      <td>-0.049288</td>\n",
       "      <td>-0.068103</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.033851</td>\n",
       "      <td>-0.009923</td>\n",
       "      <td>-0.006535</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>0.026662</td>\n",
       "      <td>-0.063127</td>\n",
       "      <td>0.019787</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.023238</td>\n",
       "      <td>-0.080117</td>\n",
       "      <td>-0.068065</td>\n",
       "      <td>-0.055372</td>\n",
       "      <td>-0.002805</td>\n",
       "      <td>-0.017776</td>\n",
       "      <td>-0.045444</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.020486</td>\n",
       "      <td>-0.071744</td>\n",
       "      <td>-0.038359</td>\n",
       "      <td>-0.015980</td>\n",
       "      <td>0.030378</td>\n",
       "      <td>0.071237</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.043080</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>-0.009842</td>\n",
       "      <td>-0.009323</td>\n",
       "      <td>0.045603</td>\n",
       "      <td>-0.035345</td>\n",
       "      <td>-0.039754</td>\n",
       "      <td>0.054821</td>\n",
       "      <td>-0.009467</td>\n",
       "      <td>-0.019029</td>\n",
       "      <td>0.037464</td>\n",
       "      <td>-0.003999</td>\n",
       "      <td>-0.002397</td>\n",
       "      <td>-0.062597</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>-0.004103</td>\n",
       "      <td>-0.035170</td>\n",
       "      <td>-0.044910</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>-0.052389</td>\n",
       "      <td>-0.027406</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.071624</td>\n",
       "      <td>0.034412</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.056166</td>\n",
       "      <td>-0.014121</td>\n",
       "      <td>-0.036367</td>\n",
       "      <td>0.041306</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>-0.051179</td>\n",
       "      <td>-0.029693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  retweet_count  favorite_count  \\\n",
       "0  1.304799e+18  1.599923e+09            0.0             1.0   \n",
       "1  1.304796e+18  1.599923e+09            1.0             0.0   \n",
       "2  1.304796e+18  1.599923e+09          802.0             0.0   \n",
       "3  1.304795e+18  1.599922e+09            0.0            15.0   \n",
       "4  1.304794e+18  1.599922e+09            0.0            10.0   \n",
       "\n",
       "   quoted_status_id       user.id  user.created_at  user.favourites_count  \\\n",
       "0               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "1               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "2               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "3               1.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "4               1.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "\n",
       "   user.followers_count  user.friends_count  user.listed_count  \\\n",
       "0                 377.0               774.0                1.0   \n",
       "1                 377.0               774.0                1.0   \n",
       "2                 377.0               774.0                1.0   \n",
       "3                 377.0               774.0                1.0   \n",
       "4                 377.0               774.0                1.0   \n",
       "\n",
       "   user.statuses_count  quoted_status.user.followers_count  \\\n",
       "0               1447.0                                 0.0   \n",
       "1               1447.0                                 0.0   \n",
       "2               1447.0                                 0.0   \n",
       "3               1447.0                             58746.0   \n",
       "4               1447.0                             58746.0   \n",
       "\n",
       "   quoted_status.user.friends_count  retweeted_status.user.followers_count  \\\n",
       "0                               0.0                                    0.0   \n",
       "1                               0.0                                   15.0   \n",
       "2                               0.0                                58746.0   \n",
       "3                            1102.0                                    0.0   \n",
       "4                            1102.0                                    0.0   \n",
       "\n",
       "   retweeted_status.user.friends_count  user_age  tweets_per_day  \\\n",
       "0                                  0.0      73.0       19.821918   \n",
       "1                                 35.0      73.0       19.821918   \n",
       "2                               1102.0      73.0       19.821918   \n",
       "3                                  0.0      73.0       19.821918   \n",
       "4                                  0.0      73.0       19.821918   \n",
       "\n",
       "   since_last_tweet_mins  since_last_tweet_mins_min  \\\n",
       "0                    0.0                        0.0   \n",
       "1                    0.0                        0.0   \n",
       "2                    0.0                        0.0   \n",
       "3                    0.0                        0.0   \n",
       "4                    0.0                        0.0   \n",
       "\n",
       "   since_last_tweet_mins_max  since_last_tweet_mins_mean  avg_tweets_per_hr  \\\n",
       "0                1380.216667                   38.853925           5.060942   \n",
       "1                1380.216667                   38.853925           5.060942   \n",
       "2                1380.216667                   38.853925           5.060942   \n",
       "3                1380.216667                   38.853925           5.060942   \n",
       "4                1380.216667                   38.853925           5.060942   \n",
       "\n",
       "   avg_tweets_per_day  no_hashtags  no_mentions  no_urls  tw_len  \\\n",
       "0           35.823529          0.0          1.0      0.0    66.0   \n",
       "1           35.823529          0.0          3.0      0.0   123.0   \n",
       "2           35.823529          0.0          1.0      1.0    60.0   \n",
       "3           35.823529          0.0          0.0      2.0    71.0   \n",
       "4           35.823529          0.0          0.0      1.0    37.0   \n",
       "\n",
       "   followers_per_followees  containsURL  user.urls_per_tweet  \\\n",
       "0                  0.48708          0.0             0.137384   \n",
       "1                  0.48708          0.0             0.137384   \n",
       "2                  0.48708          1.0             0.137384   \n",
       "3                  0.48708          1.0             0.137384   \n",
       "4                  0.48708          1.0             0.137384   \n",
       "\n",
       "   no_hashtags_per_tweet  no_mentions_per_tweet  no_urls_per_tweet  \\\n",
       "0               0.019704               1.860427           0.145594   \n",
       "1               0.019704               1.860427           0.145594   \n",
       "2               0.019704               1.860427           0.145594   \n",
       "3               0.019704               1.860427           0.145594   \n",
       "4               0.019704               1.860427           0.145594   \n",
       "\n",
       "   user.followers_countdailychange  user.friends_countdailychange  \\\n",
       "0                             7.08                          12.82   \n",
       "1                             7.08                          12.82   \n",
       "2                             7.08                          12.82   \n",
       "3                             7.08                          12.82   \n",
       "4                             7.08                          12.82   \n",
       "\n",
       "   user.friend_rate  user.followers_rate  user.has_url  user.has_location  \\\n",
       "0          10.60274             5.164384           0.0                1.0   \n",
       "1          10.60274             5.164384           0.0                1.0   \n",
       "2          10.60274             5.164384           0.0                1.0   \n",
       "3          10.60274             5.164384           0.0                1.0   \n",
       "4          10.60274             5.164384           0.0                1.0   \n",
       "\n",
       "   user.screen_name.digit_length  user.screen_name.length  is_reply  \\\n",
       "0                            0.0                     11.0       1.0   \n",
       "1                            0.0                     11.0       0.0   \n",
       "2                            0.0                     11.0       0.0   \n",
       "3                            0.0                     11.0       0.0   \n",
       "4                            0.0                     11.0       0.0   \n",
       "\n",
       "   suspended  source_        Round   Year Fun!       \\\n",
       "0          1                                      0   \n",
       "1          1                                      0   \n",
       "2          1                                      0   \n",
       "3          1                                      0   \n",
       "4          1                                      0   \n",
       "\n",
       "   source_      Round      Year   Fum    source_     Round    Year Fum         \\\n",
       "0                                     0                                     0   \n",
       "1                                     0                                     0   \n",
       "2                                     0                                     0   \n",
       "3                                     0                                     0   \n",
       "4                                     0                                     0   \n",
       "\n",
       "   source_  Round     Year Fum           source_ Round      Year   Fum         \\\n",
       "0                                     0                                     0   \n",
       "1                                     0                                     0   \n",
       "2                                     0                                     0   \n",
       "3                                     0                                     0   \n",
       "4                                     0                                     0   \n",
       "\n",
       "   source_ 「モンストからツイート」  source_Affinitweet.com  source_BIGO LIVE  \\\n",
       "0                     0                       0                 0   \n",
       "1                     0                       0                 0   \n",
       "2                     0                       0                 0   \n",
       "3                     0                       0                 0   \n",
       "4                     0                       0                 0   \n",
       "\n",
       "   source_Blog2Social APP  source_CallApp  source_Etsy  source_Instagram  \\\n",
       "0                       0               0            0                 0   \n",
       "1                       0               0            0                 0   \n",
       "2                       0               0            0                 0   \n",
       "3                       0               0            0                 0   \n",
       "4                       0               0            0                 0   \n",
       "\n",
       "   source_Joinfsocial  source_Mobile Web (M2)  source_Nintendo Switch Share  \\\n",
       "0                   0                       0                             0   \n",
       "1                   0                       0                             0   \n",
       "2                   0                       0                             0   \n",
       "3                   0                       0                             0   \n",
       "4                   0                       0                             0   \n",
       "\n",
       "   source_Paper.li  source_Peing  source_SocialPilot.co  \\\n",
       "0                0             0                      0   \n",
       "1                0             0                      0   \n",
       "2                0             0                      0   \n",
       "3                0             0                      0   \n",
       "4                0             0                      0   \n",
       "\n",
       "   source_TeamSight Publisher  source_TweetDeck  source_Twibbon  \\\n",
       "0                           0                 0               0   \n",
       "1                           0                 0               0   \n",
       "2                           0                 0               0   \n",
       "3                           0                 0               0   \n",
       "4                           0                 0               0   \n",
       "\n",
       "   source_Twitter Web App  source_Twitter Web Client  \\\n",
       "0                       1                          0   \n",
       "1                       1                          0   \n",
       "2                       1                          0   \n",
       "3                       1                          0   \n",
       "4                       1                          0   \n",
       "\n",
       "   source_Twitter for Android  source_Twitter for Mac  \\\n",
       "0                           0                       0   \n",
       "1                           0                       0   \n",
       "2                           0                       0   \n",
       "3                           0                       0   \n",
       "4                           0                       0   \n",
       "\n",
       "   source_Twitter for iPad  source_Twitter for iPhone  source_WShare  \\\n",
       "0                        0                          0              0   \n",
       "1                        0                          0              0   \n",
       "2                        0                          0              0   \n",
       "3                        0                          0              0   \n",
       "4                        0                          0              0   \n",
       "\n",
       "   source_WordPress.com  source_漫威超級戰爭（MARVEL Super War）  lang_False  lang_am  \\\n",
       "0                     0                                0           0        0   \n",
       "1                     0                                0           0        0   \n",
       "2                     0                                0           0        0   \n",
       "3                     0                                0           0        0   \n",
       "4                     0                                0           0        0   \n",
       "\n",
       "   lang_ar  lang_bg  lang_bn  lang_bo  lang_ca  lang_ckb  lang_cs  lang_cy  \\\n",
       "0        0        0        0        0        0         0        0        0   \n",
       "1        0        0        0        0        0         0        0        0   \n",
       "2        0        0        0        0        0         0        0        0   \n",
       "3        0        0        0        0        0         0        0        0   \n",
       "4        0        0        0        0        0         0        0        0   \n",
       "\n",
       "   lang_da  lang_de  lang_el  lang_en  lang_es  lang_et  lang_eu  lang_fa  \\\n",
       "0        0        0        0        1        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_fi  lang_fr  lang_gu  lang_hi  lang_ht  lang_hu  lang_hy  lang_in  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_is  lang_it  lang_iw  lang_ja  lang_km  lang_ko  lang_lt  lang_lv  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        1        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_ml  lang_mr  lang_ne  lang_nl  lang_no  lang_pl  lang_ps  lang_pt  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_ro  lang_ru  lang_sd  lang_si  lang_sl  lang_sr  lang_sv  lang_ta  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   lang_th  lang_tl  lang_tr  lang_uk  lang_und  lang_ur  lang_vi  lang_zh  \\\n",
       "0        0        0        0        0         0        0        0        0   \n",
       "1        0        0        0        0         0        0        0        1   \n",
       "2        0        0        0        0         0        0        0        0   \n",
       "3        0        0        0        0         0        0        0        1   \n",
       "4        0        0        0        0         0        0        0        1   \n",
       "\n",
       "   possibly_sensitive_False  possibly_sensitive_True  \\\n",
       "0                         0                        0   \n",
       "1                         0                        0   \n",
       "2                         0                        0   \n",
       "3                         0                        0   \n",
       "4                         0                        0   \n",
       "\n",
       "   withheld_in_countries_['IN']  withheld_in_countries_['TR']  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             0   \n",
       "4                             0                             0   \n",
       "\n",
       "   place.country_Finland  place.country_Hong Kong  place.country_India  \\\n",
       "0                      0                        0                    0   \n",
       "1                      0                        0                    0   \n",
       "2                      0                        0                    0   \n",
       "3                      0                        0                    0   \n",
       "4                      0                        0                    0   \n",
       "\n",
       "   place.country_Indonesia  place.country_Italy  place.country_Mongolia  \\\n",
       "0                        0                    0                       0   \n",
       "1                        0                    0                       0   \n",
       "2                        0                    0                       0   \n",
       "3                        0                    0                       0   \n",
       "4                        0                    0                       0   \n",
       "\n",
       "   place.country_Pakistan  place.country_People's Republic of China  \\\n",
       "0                       0                                         0   \n",
       "1                       0                                         0   \n",
       "2                       0                                         0   \n",
       "3                       0                                         0   \n",
       "4                       0                                         0   \n",
       "\n",
       "   place.country_Portugal  place.country_Republic of Korea  \\\n",
       "0                       0                                0   \n",
       "1                       0                                0   \n",
       "2                       0                                0   \n",
       "3                       0                                0   \n",
       "4                       0                                0   \n",
       "\n",
       "   place.country_Russia  place.country_Singapore  place.country_Sri Lanka  \\\n",
       "0                     0                        0                        0   \n",
       "1                     0                        0                        0   \n",
       "2                     0                        0                        0   \n",
       "3                     0                        0                        0   \n",
       "4                     0                        0                        0   \n",
       "\n",
       "   place.country_United Arab Emirates  place.country_United States  \\\n",
       "0                                   0                            0   \n",
       "1                                   0                            0   \n",
       "2                                   0                            0   \n",
       "3                                   0                            0   \n",
       "4                                   0                            0   \n",
       "\n",
       "   user.geo_enabled_False  user.geo_enabled_True  user.verified_False  \\\n",
       "0                       1                      0                    1   \n",
       "1                       1                      0                    1   \n",
       "2                       1                      0                    1   \n",
       "3                       1                      0                    1   \n",
       "4                       1                      0                    1   \n",
       "\n",
       "   user.has_extended_profile_False  user.has_extended_profile_True  \\\n",
       "0                                0                               1   \n",
       "1                                0                               1   \n",
       "2                                0                               1   \n",
       "3                                0                               1   \n",
       "4                                0                               1   \n",
       "\n",
       "   user.protected_False  user.verified_False.1  user.default_profile_True  \\\n",
       "0                     1                      1                          1   \n",
       "1                     1                      1                          1   \n",
       "2                     1                      1                          1   \n",
       "3                     1                      1                          1   \n",
       "4                     1                      1                          1   \n",
       "\n",
       "   is_quote_status_False  is_quote_status_True         0         1         2  \\\n",
       "0                      1                     0 -0.012684  0.032058 -0.012084   \n",
       "1                      1                     0 -0.071428  0.036865  0.000080   \n",
       "2                      1                     0  0.019513 -0.010960 -0.057681   \n",
       "3                      0                     1 -0.012415 -0.032357 -0.003216   \n",
       "4                      0                     1 -0.054881 -0.023647 -0.010028   \n",
       "\n",
       "          3         4         5         6         7         8         9  \\\n",
       "0 -0.058909 -0.010121  0.029514 -0.048949 -0.039718 -0.057749 -0.045809   \n",
       "1 -0.030672 -0.013456 -0.012440 -0.018908 -0.031231 -0.077490  0.044540   \n",
       "2 -0.032907 -0.037026  0.054790 -0.027800 -0.012979 -0.085419 -0.027678   \n",
       "3 -0.051747  0.027254  0.017501 -0.002711  0.025628 -0.076540 -0.037001   \n",
       "4 -0.047675  0.034321  0.033684  0.023333  0.000513 -0.086250 -0.049660   \n",
       "\n",
       "         10        11        12        13        14        15        16  \\\n",
       "0  0.001253  0.023857 -0.023192 -0.079032  0.040669 -0.072587 -0.009440   \n",
       "1 -0.031018 -0.045638 -0.055999 -0.081450 -0.012998  0.018460 -0.009418   \n",
       "2  0.012853 -0.008013  0.021589 -0.070805 -0.002169 -0.069739  0.007371   \n",
       "3  0.006617 -0.017379  0.035795 -0.069874  0.010566 -0.069844 -0.006924   \n",
       "4 -0.037532 -0.036105 -0.018089 -0.064382  0.008481 -0.057219 -0.014783   \n",
       "\n",
       "         17        18        19        20        21        22        23  \\\n",
       "0 -0.028862  0.046799 -0.012821  0.013112 -0.079651  0.011496 -0.034028   \n",
       "1  0.037660  0.004303 -0.040534 -0.003758 -0.051248  0.004886  0.025957   \n",
       "2 -0.018928 -0.003137 -0.066233  0.032976 -0.065034  0.033201 -0.056407   \n",
       "3  0.018001 -0.019316 -0.022597  0.003079 -0.056065 -0.043089 -0.061511   \n",
       "4  0.029099  0.004409 -0.022639  0.042465 -0.055263  0.006521 -0.035339   \n",
       "\n",
       "         24        25        26        27        28        29        30  \\\n",
       "0  0.017402 -0.021980  0.030615  0.011269  0.015496 -0.026737 -0.054069   \n",
       "1 -0.039417  0.006259 -0.057495 -0.017595 -0.050963 -0.036229 -0.029492   \n",
       "2  0.050745  0.027317  0.010837  0.027160  0.027250 -0.059269 -0.036120   \n",
       "3 -0.008160 -0.019329  0.036958 -0.005092 -0.003075 -0.034039 -0.053142   \n",
       "4  0.016142  0.025885  0.034862  0.048463  0.030750  0.000754 -0.039728   \n",
       "\n",
       "         31        32        33        34        35        36        37  \\\n",
       "0  0.011594 -0.021249  0.000350 -0.038505 -0.063732 -0.053111  0.005463   \n",
       "1 -0.033471  0.044689 -0.049225  0.057893 -0.051001  0.039209  0.018550   \n",
       "2  0.037503 -0.032100 -0.019312 -0.022517 -0.058025 -0.038338  0.007366   \n",
       "3  0.040678 -0.012526 -0.038992  0.012522 -0.033641 -0.027966  0.019221   \n",
       "4  0.017460 -0.004436  0.000032  0.007218 -0.054100 -0.059679  0.022575   \n",
       "\n",
       "         38        39        40        41        42        43        44  \\\n",
       "0  0.001640 -0.042972 -0.013115  0.002608 -0.000904 -0.029163 -0.011124   \n",
       "1 -0.028121 -0.022803  0.032745 -0.057217 -0.021886 -0.045973  0.057541   \n",
       "2  0.036771 -0.061164  0.048462  0.021861 -0.048065 -0.024787  0.012505   \n",
       "3 -0.042878 -0.044672  0.040196  0.052543  0.025477 -0.007544  0.007495   \n",
       "4  0.001170 -0.075584  0.029238  0.050847 -0.035654 -0.027266  0.043231   \n",
       "\n",
       "         45        46        47        48        49        50        51  \\\n",
       "0  0.009872  0.028715 -0.014881 -0.046697  0.000153 -0.013703  0.010752   \n",
       "1 -0.026861 -0.067797  0.058958 -0.059584 -0.050733 -0.059008  0.001968   \n",
       "2 -0.054749 -0.002271  0.014820 -0.032456  0.015248 -0.044380 -0.030845   \n",
       "3  0.011400  0.012305  0.034803 -0.035991  0.009920 -0.013913 -0.009902   \n",
       "4 -0.020378 -0.047516  0.047808 -0.017460  0.003107  0.011244  0.018869   \n",
       "\n",
       "         52        53        54        55        56        57        58  \\\n",
       "0 -0.022136 -0.045382 -0.018033  0.042627 -0.050698 -0.032358 -0.044754   \n",
       "1 -0.049431 -0.013537 -0.039153  0.009148 -0.033792 -0.053318 -0.004010   \n",
       "2 -0.022430 -0.066008 -0.020078 -0.001191 -0.035726 -0.060627 -0.019230   \n",
       "3  0.010032 -0.015773 -0.024805  0.003887 -0.029689 -0.037778 -0.045778   \n",
       "4  0.006024 -0.029885 -0.036385  0.053657 -0.037753 -0.062391 -0.041248   \n",
       "\n",
       "         59        60        61        62        63        64        65  \\\n",
       "0 -0.006920  0.023797 -0.045344  0.016531 -0.006689  0.031439 -0.041886   \n",
       "1 -0.010678 -0.055194  0.023063 -0.026490 -0.024484  0.016402 -0.050410   \n",
       "2 -0.009873  0.035434 -0.010532  0.035820  0.030388 -0.022084 -0.033331   \n",
       "3  0.040465  0.020352 -0.030550  0.042470  0.055424  0.037721 -0.034012   \n",
       "4  0.041752  0.057789 -0.002230  0.028687 -0.007140  0.021609 -0.045735   \n",
       "\n",
       "         66        67        68        69        70        71        72  \\\n",
       "0 -0.031555 -0.054637 -0.036469 -0.052694 -0.029040 -0.045692  0.010561   \n",
       "1 -0.065115 -0.059771 -0.000246 -0.058024  0.020649 -0.013553  0.019451   \n",
       "2 -0.047660  0.016659  0.032242 -0.060040 -0.049726 -0.030440 -0.025063   \n",
       "3 -0.061356  0.026725 -0.016261 -0.060464 -0.022696 -0.024922  0.014629   \n",
       "4 -0.050799 -0.026367 -0.024023 -0.063793 -0.031616 -0.061337 -0.061695   \n",
       "\n",
       "         73        74        75        76        77        78        79  \\\n",
       "0 -0.056232 -0.052826  0.058955 -0.027599  0.034541 -0.036078  0.023143   \n",
       "1 -0.006534 -0.071764 -0.009920  0.062180 -0.026307  0.022470 -0.050513   \n",
       "2 -0.032255 -0.020624  0.006145 -0.038211 -0.041909 -0.020006  0.024421   \n",
       "3 -0.001660 -0.037269  0.039240 -0.045430 -0.051789  0.001683  0.021815   \n",
       "4 -0.058365 -0.065402 -0.004568 -0.018722 -0.047516 -0.015339 -0.011169   \n",
       "\n",
       "         80        81        82        83        84        85        86  \\\n",
       "0 -0.023220 -0.008169 -0.040580  0.007478 -0.070895 -0.033936  0.018778   \n",
       "1  0.032156  0.003212 -0.025193 -0.003190 -0.060368 -0.037959 -0.012182   \n",
       "2  0.034367  0.008950  0.021557  0.015745 -0.008977 -0.004998  0.024436   \n",
       "3  0.012582  0.022863  0.041235  0.034137 -0.038494 -0.023607  0.024606   \n",
       "4  0.024208  0.011884 -0.031914  0.018203 -0.042064  0.025840  0.061937   \n",
       "\n",
       "         87        88        89        90        91        92        93  \\\n",
       "0  0.012737 -0.022574  0.019673  0.035646 -0.055151 -0.016667  0.019820   \n",
       "1 -0.053345  0.041564 -0.060537 -0.001746 -0.025970 -0.034163 -0.012904   \n",
       "2  0.010569 -0.052433 -0.046885  0.052143  0.013450  0.009536 -0.019835   \n",
       "3  0.016334 -0.040178  0.001785  0.059885 -0.005224  0.043395  0.051490   \n",
       "4  0.001602 -0.051947 -0.015082  0.037559 -0.038491 -0.014307  0.002693   \n",
       "\n",
       "         94        95        96        97        98        99       100  \\\n",
       "0  0.060649 -0.000062  0.009627  0.007304 -0.041217  0.007271 -0.002927   \n",
       "1 -0.036963  0.010317 -0.008359 -0.035451  0.008460 -0.043065 -0.028333   \n",
       "2  0.068253  0.026576  0.014641  0.041462  0.017623  0.009661  0.050481   \n",
       "3 -0.007906  0.014598  0.006685  0.029266  0.038697  0.039741  0.002094   \n",
       "4  0.024144  0.040089  0.001617  0.006000  0.025084  0.001069  0.065689   \n",
       "\n",
       "        101       102       103       104       105       106       107  \\\n",
       "0 -0.051279 -0.037429 -0.018154 -0.028611 -0.049466 -0.072750 -0.015042   \n",
       "1 -0.024705 -0.018967 -0.024230 -0.000574 -0.010815 -0.045273 -0.040499   \n",
       "2 -0.068515 -0.054983 -0.001160 -0.024509 -0.051372 -0.063344 -0.041517   \n",
       "3 -0.065606  0.028422  0.022238  0.030466 -0.069865 -0.069696 -0.032788   \n",
       "4 -0.061694  0.020459  0.024173  0.054541 -0.061544 -0.075089 -0.036567   \n",
       "\n",
       "        108       109       110       111       112       113       114  \\\n",
       "0  0.033877  0.002524 -0.022654 -0.049825 -0.041341  0.011809 -0.038731   \n",
       "1 -0.031301 -0.010261  0.026841 -0.041313  0.023154 -0.008628 -0.016825   \n",
       "2 -0.009673 -0.037621  0.002574 -0.044874 -0.015007 -0.021086 -0.056174   \n",
       "3 -0.018257 -0.063434  0.001024 -0.033589 -0.021467  0.013782 -0.041761   \n",
       "4  0.062974 -0.034190  0.001656 -0.054939 -0.041075  0.046513 -0.072019   \n",
       "\n",
       "        115       116       117       118       119       120       121  \\\n",
       "0 -0.023381 -0.017388 -0.069509 -0.060831 -0.029683 -0.040292  0.014650   \n",
       "1  0.001764 -0.002088 -0.067313 -0.027284 -0.016430 -0.017021  0.045932   \n",
       "2 -0.023262  0.063454  0.003523  0.020157 -0.043945 -0.026554  0.033530   \n",
       "3 -0.007212  0.019007 -0.046112  0.035580 -0.040751 -0.021094  0.040147   \n",
       "4  0.007193  0.067308 -0.040033 -0.000345 -0.022083 -0.038334  0.028651   \n",
       "\n",
       "        122       123       124       125       126       127       128  \\\n",
       "0 -0.009884 -0.042129 -0.042998  0.045355  0.020952  0.054097  0.030807   \n",
       "1 -0.024103 -0.017397  0.010837 -0.038079  0.003202  0.047446 -0.038267   \n",
       "2  0.006131 -0.005232 -0.046493 -0.026550 -0.020351  0.048224  0.012223   \n",
       "3 -0.031103 -0.016132  0.017775 -0.000401 -0.014177  0.009410  0.015510   \n",
       "4  0.005748 -0.011853 -0.003197  0.024168  0.040932  0.071355  0.014250   \n",
       "\n",
       "        129       130       131       132       133       134       135  \\\n",
       "0 -0.008072 -0.020249  0.000179 -0.014839  0.020506  0.052167  0.010197   \n",
       "1  0.002687 -0.027974 -0.052660 -0.045014  0.027004 -0.011624  0.024504   \n",
       "2 -0.000536 -0.025412 -0.052168 -0.034330  0.004978  0.060821  0.003246   \n",
       "3 -0.023539 -0.014549 -0.006993 -0.004820  0.012151  0.057016 -0.028273   \n",
       "4 -0.015070 -0.023656 -0.032432 -0.028231  0.018921  0.029276 -0.014107   \n",
       "\n",
       "        136       137       138       139       140       141       142  \\\n",
       "0 -0.029885 -0.057116 -0.024172  0.021789 -0.065039  0.015260 -0.028273   \n",
       "1 -0.058782 -0.051111 -0.039329 -0.048663 -0.043818  0.003213  0.008730   \n",
       "2 -0.033213 -0.008740 -0.034960 -0.005468 -0.008729 -0.007196 -0.013251   \n",
       "3 -0.039979 -0.070333 -0.056884 -0.028251 -0.033710 -0.029687 -0.031643   \n",
       "4 -0.063586 -0.066848 -0.027840  0.020189 -0.053970  0.046559 -0.008395   \n",
       "\n",
       "        143       144       145       146       147       148       149  \\\n",
       "0  0.003900 -0.021805  0.015064  0.005733  0.058093 -0.024467  0.009445   \n",
       "1 -0.044706 -0.003561 -0.024433 -0.037887  0.020904 -0.006212 -0.035133   \n",
       "2  0.018551 -0.057963 -0.021385  0.043342  0.000570 -0.006739  0.000013   \n",
       "3 -0.046253 -0.066934  0.029965  0.062815  0.003537 -0.025202 -0.006467   \n",
       "4  0.003628 -0.054333  0.071043  0.034798 -0.012342  0.024077 -0.041755   \n",
       "\n",
       "        150       151       152       153       154       155       156  \\\n",
       "0 -0.025404  0.004268  0.041809 -0.012647  0.039335 -0.062787 -0.026857   \n",
       "1 -0.015360 -0.009036 -0.002016 -0.020923  0.028903 -0.000166 -0.054304   \n",
       "2 -0.037606 -0.003123  0.036070  0.010976  0.043163 -0.035388  0.007109   \n",
       "3  0.002210  0.036854  0.036611  0.020711  0.003118 -0.069008 -0.036498   \n",
       "4 -0.007022 -0.008878  0.043638  0.011376  0.054078 -0.043187  0.004041   \n",
       "\n",
       "        157       158       159       160       161       162       163  \\\n",
       "0 -0.053948 -0.062015 -0.065928 -0.025099  0.027733 -0.044511 -0.038252   \n",
       "1 -0.073347 -0.004396 -0.015981 -0.005769 -0.013129  0.017139 -0.047879   \n",
       "2 -0.013143 -0.035578 -0.050378 -0.026316  0.041962  0.033277  0.005835   \n",
       "3 -0.034765 -0.032292 -0.040044 -0.026161 -0.038480 -0.014087 -0.015962   \n",
       "4 -0.033467 -0.037586 -0.006985  0.025494  0.018199  0.025948  0.022596   \n",
       "\n",
       "        164       165       166       167       168       169       170  \\\n",
       "0  0.009667  0.028804 -0.002527 -0.021905 -0.025735  0.015305 -0.022429   \n",
       "1 -0.039180 -0.001112  0.005739  0.012901  0.000826 -0.028003 -0.004390   \n",
       "2  0.034552  0.055608  0.026745  0.002646 -0.049728  0.018244 -0.032591   \n",
       "3  0.015421  0.028761  0.056834 -0.031295 -0.057769  0.004899 -0.033881   \n",
       "4 -0.014204  0.000429  0.044334 -0.053319 -0.028740  0.030350  0.006293   \n",
       "\n",
       "        171       172       173       174       175       176       177  \\\n",
       "0  0.008045  0.047290  0.030772 -0.040644 -0.019000 -0.052515 -0.005107   \n",
       "1 -0.055328 -0.065056 -0.062104 -0.062825 -0.021902 -0.032651  0.017248   \n",
       "2 -0.049064  0.022606  0.008510 -0.006228 -0.009495 -0.006116  0.022614   \n",
       "3 -0.013165  0.045751  0.024376 -0.022895  0.004996  0.019019 -0.040938   \n",
       "4 -0.044594  0.036331  0.045419  0.002476  0.003720 -0.054523 -0.043617   \n",
       "\n",
       "        178       179       180       181       182       183       184  \\\n",
       "0  0.012194 -0.064072 -0.016188 -0.020646 -0.010242  0.020962  0.063606   \n",
       "1 -0.036045 -0.070820 -0.010375  0.019931  0.029400 -0.024553 -0.001835   \n",
       "2 -0.026851 -0.041777  0.006415 -0.040167 -0.050369  0.002176  0.027030   \n",
       "3 -0.047268 -0.009612 -0.020561  0.008488  0.007828 -0.027007  0.020262   \n",
       "4 -0.029892 -0.015337  0.017519 -0.036844 -0.004541 -0.025270  0.055876   \n",
       "\n",
       "        185       186       187       188       189       190       191  \\\n",
       "0  0.038598  0.004050  0.026701  0.055996  0.019855  0.022246 -0.036682   \n",
       "1 -0.050494 -0.067199 -0.064832 -0.008620 -0.047122 -0.021271  0.051736   \n",
       "2  0.025516  0.014722  0.009441  0.035387 -0.023859 -0.046808  0.030996   \n",
       "3  0.011959  0.009094  0.034593  0.047163 -0.003007 -0.046822 -0.005476   \n",
       "4 -0.016467 -0.007058  0.004489  0.037879  0.009581 -0.068118  0.034601   \n",
       "\n",
       "        192       193       194       195       196       197       198  \\\n",
       "0  0.036925 -0.031158 -0.032735  0.033899 -0.052497 -0.031882  0.043214   \n",
       "1 -0.072632  0.024036 -0.053682  0.023048  0.025233 -0.013576  0.048936   \n",
       "2  0.001520 -0.020792 -0.001804  0.056882 -0.012364 -0.031165 -0.010554   \n",
       "3  0.059302  0.007377 -0.015005  0.028240  0.017567 -0.034700 -0.021105   \n",
       "4  0.002625  0.010729  0.014992  0.043118 -0.027265 -0.054553  0.014100   \n",
       "\n",
       "        199       200       201       202       203       204       205  \\\n",
       "0  0.040524  0.028207  0.013128 -0.068048  0.015455 -0.030387 -0.058243   \n",
       "1  0.009921  0.005839 -0.031985  0.015938 -0.018688 -0.034227 -0.037879   \n",
       "2  0.023834  0.021917  0.051521 -0.005691 -0.003245  0.057232 -0.079294   \n",
       "3  0.020945  0.049853  0.018599 -0.013190 -0.021110  0.011055 -0.039836   \n",
       "4  0.014071  0.007603  0.011318  0.015828  0.004791  0.009633 -0.064358   \n",
       "\n",
       "        206       207       208       209       210       211       212  \\\n",
       "0  0.048650 -0.056857 -0.021475 -0.053283 -0.078646 -0.026252 -0.058822   \n",
       "1  0.039943 -0.033647  0.020257 -0.057439 -0.030871  0.001268 -0.034224   \n",
       "2  0.000942 -0.053467 -0.013036 -0.012459 -0.060464 -0.009393 -0.053511   \n",
       "3  0.035668 -0.057073  0.057995 -0.025726 -0.074602 -0.040854 -0.065541   \n",
       "4  0.032144 -0.042294  0.027679  0.038812 -0.077035 -0.054018 -0.048548   \n",
       "\n",
       "        213       214       215       216       217       218       219  \\\n",
       "0  0.015631  0.011295  0.023175 -0.085459  0.026487  0.046952 -0.031430   \n",
       "1 -0.046174 -0.031649 -0.014695 -0.132223 -0.024963 -0.024907  0.009796   \n",
       "2  0.038751  0.006821  0.026593 -0.089739  0.006973  0.004710 -0.023431   \n",
       "3  0.007663  0.017608 -0.018122 -0.081354  0.006059 -0.016465 -0.030215   \n",
       "4 -0.050309  0.032505  0.035732 -0.091933  0.036436 -0.016304 -0.031171   \n",
       "\n",
       "        220       221       222       223       224       225       226  \\\n",
       "0 -0.048673 -0.061280 -0.012950 -0.008721  0.036098  0.054009  0.038912   \n",
       "1  0.001262  0.024054  0.042253 -0.020300  0.042878 -0.043980 -0.022271   \n",
       "2 -0.046462  0.029822  0.043397  0.069064 -0.014733  0.035360  0.005746   \n",
       "3 -0.010478  0.026500 -0.030309  0.046174  0.024959  0.017901  0.039746   \n",
       "4  0.015011 -0.036591  0.007091  0.034341 -0.013091  0.064677  0.059275   \n",
       "\n",
       "        227       228       229       230       231       232       233  \\\n",
       "0 -0.011423 -0.053928 -0.035878  0.044500 -0.012785 -0.060563  0.009997   \n",
       "1 -0.043112  0.005239 -0.030760 -0.060127 -0.050701 -0.060860 -0.013946   \n",
       "2  0.043045 -0.004415 -0.043049  0.037380 -0.009260 -0.043302  0.009094   \n",
       "3  0.010771 -0.042187 -0.029751  0.058508  0.007461 -0.029876 -0.000500   \n",
       "4  0.015814  0.002148 -0.043697  0.019829 -0.007790 -0.056596  0.036173   \n",
       "\n",
       "        234       235       236       237       238       239       240  \\\n",
       "0  0.043167  0.015077 -0.032720  0.048639 -0.040042 -0.005152  0.000094   \n",
       "1 -0.046568 -0.010687 -0.040630  0.024113 -0.060536  0.003780  0.011048   \n",
       "2 -0.000412  0.003403  0.051412  0.065873 -0.045331  0.010003 -0.030392   \n",
       "3  0.056522 -0.025930  0.015181  0.034398 -0.054379  0.009520 -0.005920   \n",
       "4  0.064855 -0.020905  0.045280  0.053960 -0.061140  0.005509 -0.038366   \n",
       "\n",
       "        241       242       243       244       245       246       247  \\\n",
       "0 -0.035867  0.037815 -0.022669 -0.000402 -0.021953  0.040487 -0.026461   \n",
       "1  0.005095 -0.044205 -0.048382  0.001305 -0.033334  0.022661  0.008762   \n",
       "2 -0.035519 -0.007047 -0.024870 -0.032678 -0.042166  0.048920 -0.004612   \n",
       "3 -0.021807 -0.014395 -0.006513  0.038608 -0.051181 -0.016653 -0.048581   \n",
       "4 -0.032508 -0.044673  0.004457 -0.009847  0.004380 -0.012298 -0.059702   \n",
       "\n",
       "        248       249       250       251       252       253       254  \\\n",
       "0 -0.078215 -0.013578  0.003245  0.022563 -0.081650 -0.024577 -0.029677   \n",
       "1  0.044073  0.031779 -0.038928 -0.003178 -0.058736 -0.008807  0.034724   \n",
       "2  0.026165  0.021550 -0.029102 -0.049491 -0.082290  0.026326 -0.019302   \n",
       "3 -0.044484  0.054334 -0.007514  0.013860 -0.077872  0.024835 -0.034617   \n",
       "4 -0.033976  0.055937  0.019840 -0.019124 -0.089443 -0.025751 -0.014895   \n",
       "\n",
       "        255       256       257       258       259       260       261  \\\n",
       "0 -0.054735 -0.055636 -0.039835  0.028341  0.042441 -0.034544 -0.053626   \n",
       "1 -0.068505 -0.008388 -0.008528 -0.015663 -0.025105  0.007202  0.000019   \n",
       "2 -0.048432 -0.055487 -0.010260  0.034458  0.076416  0.014433 -0.010339   \n",
       "3 -0.044730 -0.059697 -0.034797 -0.018007  0.075957 -0.030186 -0.026742   \n",
       "4 -0.004765 -0.046062 -0.020823  0.004950  0.085431 -0.037022 -0.050355   \n",
       "\n",
       "        262       263       264       265       266       267       268  \\\n",
       "0 -0.016691 -0.003435 -0.014086 -0.050197 -0.064220  0.015356  0.005871   \n",
       "1 -0.035809 -0.056446 -0.059206 -0.006269  0.026632 -0.049463  0.027638   \n",
       "2 -0.008518  0.024141 -0.032981 -0.067955 -0.050547  0.052541 -0.015817   \n",
       "3 -0.018267  0.048242 -0.002874 -0.062575 -0.020571 -0.005177  0.016267   \n",
       "4  0.014878  0.038924 -0.047586 -0.071209 -0.044032  0.000070  0.037372   \n",
       "\n",
       "        269       270       271       272       273       274       275  \\\n",
       "0 -0.033933  0.054778 -0.061413  0.005681  0.042436 -0.069930  0.007641   \n",
       "1  0.045788  0.013609 -0.057196 -0.010605 -0.005667 -0.050112 -0.058301   \n",
       "2  0.007916  0.015150 -0.000471  0.023616  0.053558 -0.066518 -0.025649   \n",
       "3  0.011860  0.036161 -0.012050  0.014347 -0.004388 -0.021748 -0.022410   \n",
       "4 -0.014546  0.028268 -0.017059  0.023508  0.005840 -0.061794  0.032057   \n",
       "\n",
       "        276       277       278       279       280       281       282  \\\n",
       "0  0.063352 -0.016221 -0.020024 -0.054722 -0.027237  0.069767  0.037764   \n",
       "1 -0.009496 -0.046851  0.037343 -0.053403 -0.014253  0.031818 -0.030606   \n",
       "2  0.012867 -0.006539 -0.050877 -0.020718 -0.030536  0.067537  0.001948   \n",
       "3  0.002707 -0.017778 -0.021504 -0.020692 -0.037629  0.041481 -0.029364   \n",
       "4  0.071932  0.007137 -0.063502 -0.039313 -0.008857  0.066541  0.025489   \n",
       "\n",
       "        283       284       285       286       287       288       289  \\\n",
       "0 -0.060808  0.002359 -0.038232 -0.008557 -0.025271 -0.003708 -0.008958   \n",
       "1 -0.035720 -0.002055  0.028619 -0.037756 -0.058990 -0.036828  0.011469   \n",
       "2  0.000840  0.016293 -0.013182  0.021970 -0.001305  0.009074  0.044859   \n",
       "3 -0.041790 -0.004578 -0.006792  0.028978 -0.038216  0.023683  0.043142   \n",
       "4 -0.019589  0.006827 -0.036118  0.023459 -0.035730 -0.000139  0.026513   \n",
       "\n",
       "        290       291       292       293       294       295       296  \\\n",
       "0 -0.058989  0.004064 -0.037424 -0.038551  0.012316 -0.054515 -0.005107   \n",
       "1 -0.043592 -0.024012 -0.071860 -0.042504 -0.027704 -0.058134 -0.057571   \n",
       "2 -0.054192 -0.006398 -0.010536 -0.020308  0.058227  0.020292  0.005206   \n",
       "3 -0.038172 -0.069311  0.035794 -0.066833  0.030748 -0.036074 -0.000310   \n",
       "4 -0.036824 -0.029555 -0.009097 -0.022957 -0.010810 -0.008972 -0.010894   \n",
       "\n",
       "        297       298       299       300       301       302       303  \\\n",
       "0  0.032695 -0.016780 -0.004137  0.006574  0.013929 -0.076708 -0.028251   \n",
       "1 -0.005588 -0.012474 -0.025648 -0.046387 -0.032408 -0.058367  0.023123   \n",
       "2  0.016266 -0.001991 -0.019161 -0.052709  0.001932 -0.058244 -0.053822   \n",
       "3 -0.006826  0.038987  0.004869  0.012419  0.029801 -0.061734 -0.034215   \n",
       "4  0.042969 -0.033000 -0.004466 -0.019519  0.034654 -0.077665 -0.060188   \n",
       "\n",
       "        304       305       306       307       308       309       310  \\\n",
       "0 -0.004270 -0.038534  0.044063 -0.016370 -0.062544  0.033366  0.009463   \n",
       "1 -0.058075  0.022698  0.002675 -0.055576  0.008333 -0.001419 -0.013446   \n",
       "2 -0.001495  0.051148  0.022017 -0.003954 -0.067461  0.055583  0.015595   \n",
       "3 -0.034809  0.044035  0.050266 -0.059598 -0.041459 -0.010368  0.065164   \n",
       "4 -0.012254  0.056199  0.065949 -0.011557 -0.070428  0.011874 -0.018742   \n",
       "\n",
       "        311       312       313       314       315       316       317  \\\n",
       "0 -0.014834 -0.017944  0.021337 -0.021939  0.004993 -0.054524 -0.044374   \n",
       "1  0.020848  0.006329 -0.051643 -0.017939 -0.058548 -0.009486  0.021758   \n",
       "2  0.038058  0.049482  0.000490  0.013106  0.018230  0.019610 -0.002195   \n",
       "3  0.047619  0.003889  0.031548  0.003910 -0.030222  0.044417 -0.001980   \n",
       "4  0.033919  0.052279 -0.025325  0.008039  0.038935  0.009317  0.005516   \n",
       "\n",
       "        318       319       320       321       322       323       324  \\\n",
       "0 -0.014301 -0.063166 -0.023251 -0.065704 -0.047744  0.012386  0.008053   \n",
       "1 -0.057022 -0.009951 -0.000074  0.000962  0.046696 -0.058660 -0.000222   \n",
       "2 -0.040157 -0.027095 -0.052737 -0.038650  0.000883 -0.008378 -0.004352   \n",
       "3  0.026644 -0.065902 -0.059039 -0.072675  0.002491  0.013754 -0.054349   \n",
       "4  0.005129 -0.063535 -0.031420 -0.060079  0.017401 -0.000591  0.005086   \n",
       "\n",
       "        325       326       327       328       329       330       331  \\\n",
       "0  0.011492  0.001815  0.037139 -0.015856 -0.037671 -0.049118  0.002907   \n",
       "1  0.014479 -0.009657 -0.028284  0.037977  0.013025 -0.040821 -0.025729   \n",
       "2  0.028649  0.040239  0.024245  0.070656 -0.011620 -0.059243  0.071307   \n",
       "3  0.014285  0.042173  0.002451 -0.000467 -0.002773 -0.044382  0.054519   \n",
       "4 -0.035623  0.053268  0.026074  0.028629 -0.026768 -0.029447  0.032630   \n",
       "\n",
       "        332       333       334       335       336       337       338  \\\n",
       "0 -0.028133 -0.059081  0.049446 -0.064176 -0.023756 -0.055596 -0.059165   \n",
       "1 -0.029452 -0.053803 -0.012388  0.022945  0.034418  0.046840 -0.051162   \n",
       "2 -0.051657  0.009274  0.001370 -0.013779 -0.027204  0.002622 -0.052142   \n",
       "3 -0.050245 -0.022704  0.062202 -0.003223 -0.028636  0.023736 -0.069240   \n",
       "4 -0.038669 -0.025306 -0.002398 -0.000179 -0.016042 -0.029711 -0.016850   \n",
       "\n",
       "        339       340       341       342       343       344       345  \\\n",
       "0  0.008643  0.060375 -0.030946  0.010688 -0.001439 -0.020018 -0.035089   \n",
       "1  0.004083 -0.052037  0.043989 -0.049349 -0.011715 -0.045221 -0.046871   \n",
       "2  0.029672  0.036294  0.050352  0.029731  0.066881 -0.063964 -0.014514   \n",
       "3  0.017473  0.015797 -0.028994  0.025293  0.044596 -0.007748 -0.033838   \n",
       "4 -0.025724  0.024141  0.062058  0.042931  0.043269 -0.035548  0.006081   \n",
       "\n",
       "        346       347       348       349       350       351       352  \\\n",
       "0 -0.005154 -0.026912  0.025179  0.050198 -0.030673 -0.033770  0.022423   \n",
       "1 -0.006051 -0.009531  0.038348 -0.051193  0.050519 -0.019161  0.011998   \n",
       "2  0.002919  0.017935 -0.021431  0.074399 -0.000298  0.031231  0.023608   \n",
       "3 -0.014207  0.050722  0.012569  0.007833 -0.024039 -0.010729 -0.024232   \n",
       "4  0.007725 -0.001456  0.003061  0.041026  0.036212  0.010870  0.004362   \n",
       "\n",
       "        353       354       355       356       357       358       359  \\\n",
       "0 -0.015782 -0.023778  0.031749 -0.007667 -0.008139  0.022339  0.018733   \n",
       "1 -0.064356  0.027009 -0.054494  0.023120  0.005248  0.059278 -0.024523   \n",
       "2 -0.038730 -0.032499  0.032016 -0.066692 -0.013078 -0.020541 -0.016344   \n",
       "3 -0.024182 -0.057425  0.022572 -0.030658  0.006892  0.018967 -0.042958   \n",
       "4 -0.019648  0.002984  0.048574 -0.037337  0.015110 -0.007433 -0.019469   \n",
       "\n",
       "        360       361       362       363       364       365       366  \\\n",
       "0 -0.075483 -0.041235 -0.004791 -0.001485 -0.039245  0.030111  0.061607   \n",
       "1 -0.069670  0.000200 -0.021530  0.042663 -0.031639 -0.009952 -0.052521   \n",
       "2 -0.082868 -0.044288 -0.017430 -0.042802 -0.060323  0.016079  0.033834   \n",
       "3 -0.078298 -0.049557 -0.042589 -0.069088 -0.045939  0.059275  0.054872   \n",
       "4 -0.079375 -0.063959  0.025661 -0.059434 -0.039322 -0.011829  0.037479   \n",
       "\n",
       "        367       368       369       370       371       372       373  \\\n",
       "0  0.008507 -0.028836 -0.027864  0.009745 -0.024530  0.013808 -0.011256   \n",
       "1 -0.020216 -0.043306  0.052419  0.019863 -0.034234 -0.012106 -0.076982   \n",
       "2 -0.072490 -0.033961  0.025959  0.034973  0.030573 -0.022477 -0.033175   \n",
       "3 -0.034204 -0.015421  0.028170  0.063644 -0.014624 -0.000721 -0.038929   \n",
       "4 -0.017828  0.026969 -0.023232  0.033300 -0.002231  0.019587 -0.028282   \n",
       "\n",
       "        374       375       376       377       378       379       380  \\\n",
       "0  0.038963  0.012131  0.019753  0.015486 -0.061403  0.033831 -0.062537   \n",
       "1 -0.021089 -0.032529  0.039107 -0.015409 -0.036220 -0.024190 -0.025237   \n",
       "2 -0.030592  0.039595  0.032288 -0.068339 -0.036247  0.009057 -0.033766   \n",
       "3 -0.003993 -0.014354  0.041989 -0.044893 -0.039765 -0.000757 -0.067462   \n",
       "4  0.030304 -0.008350  0.065973 -0.007827 -0.025300  0.054881 -0.042662   \n",
       "\n",
       "        381       382       383       384       385       386       387  \\\n",
       "0  0.049702 -0.011401 -0.056452 -0.034077 -0.031405 -0.005557 -0.019793   \n",
       "1 -0.052166 -0.041133  0.026204 -0.018844 -0.050077 -0.010526  0.017758   \n",
       "2  0.032742 -0.023121 -0.066087 -0.042670 -0.035675 -0.043509 -0.010099   \n",
       "3 -0.012037 -0.011468 -0.064382 -0.026811 -0.008620 -0.034056 -0.004239   \n",
       "4 -0.003364 -0.000788 -0.045828 -0.026556 -0.026906 -0.019873 -0.038703   \n",
       "\n",
       "        388       389       390       391       392       393       394  \\\n",
       "0 -0.047722 -0.034338  0.015657  0.026838  0.020047 -0.048563 -0.000408   \n",
       "1  0.019855  0.020973  0.002610 -0.004294 -0.061723 -0.040731  0.012935   \n",
       "2 -0.066843 -0.015705  0.021815 -0.004701  0.029062 -0.040716  0.060369   \n",
       "3 -0.065724 -0.026489  0.008085 -0.003901 -0.010155 -0.001700  0.048769   \n",
       "4 -0.061755 -0.037111  0.028138  0.025494 -0.023941 -0.026760  0.024890   \n",
       "\n",
       "        395       396       397       398       399       400       401  \\\n",
       "0 -0.011097  0.058591  0.053035 -0.038439  0.032881 -0.010134 -0.023956   \n",
       "1 -0.015012 -0.008618  0.055333 -0.062544 -0.013444  0.032236 -0.030142   \n",
       "2  0.059726  0.001542  0.020704 -0.074127 -0.044512 -0.041543  0.027740   \n",
       "3  0.047941 -0.030793  0.045149 -0.045497 -0.029377 -0.021335 -0.041732   \n",
       "4  0.019285  0.026527 -0.006441 -0.062933 -0.008385 -0.007794  0.005706   \n",
       "\n",
       "        402       403       404       405       406       407       408  \\\n",
       "0 -0.044026 -0.025120  0.066441 -0.074821  0.005333 -0.003505 -0.005499   \n",
       "1  0.025157 -0.069567 -0.075792 -0.043926 -0.034297 -0.028991 -0.010748   \n",
       "2 -0.050536 -0.024398  0.018480 -0.077055  0.055942  0.037710 -0.036036   \n",
       "3 -0.003356 -0.032710  0.033064 -0.064384  0.007371  0.035646  0.015900   \n",
       "4 -0.055790  0.024424  0.041420 -0.007982 -0.021991  0.043236 -0.040784   \n",
       "\n",
       "        409       410       411       412       413       414       415  \\\n",
       "0  0.027652  0.046678 -0.018353 -0.033452 -0.044611  0.018085  0.050496   \n",
       "1 -0.048759  0.033360  0.016685  0.012548 -0.009353 -0.010127  0.008507   \n",
       "2  0.015827  0.063146  0.003867  0.022773  0.009020  0.027950 -0.026780   \n",
       "3  0.047191  0.000766 -0.054057 -0.037450  0.036423  0.035801 -0.001905   \n",
       "4  0.017503  0.066415 -0.007824 -0.020967  0.033344 -0.015308  0.020027   \n",
       "\n",
       "        416       417       418       419       420       421       422  \\\n",
       "0 -0.062089 -0.048794  0.004885  0.060458 -0.057450 -0.043299 -0.025642   \n",
       "1 -0.044779 -0.009634 -0.021868 -0.031937 -0.046726 -0.027837  0.014695   \n",
       "2 -0.043733 -0.021582  0.041542  0.034917 -0.034974 -0.036343 -0.019457   \n",
       "3  0.018992 -0.010711  0.042425  0.050421 -0.063924 -0.036919 -0.040148   \n",
       "4 -0.037675  0.009981  0.009876  0.044218 -0.049950 -0.041732 -0.047035   \n",
       "\n",
       "        423       424       425       426       427       428       429  \\\n",
       "0 -0.030838 -0.035391  0.010264  0.040862  0.016999  0.005403  0.038329   \n",
       "1 -0.015575 -0.043078 -0.010959 -0.062061 -0.050633 -0.028108 -0.014859   \n",
       "2 -0.019818 -0.042045 -0.008003  0.029776  0.057665 -0.026611 -0.033165   \n",
       "3  0.034210 -0.015544  0.019884  0.046282  0.003292 -0.026894  0.013247   \n",
       "4 -0.033882  0.024838  0.001556  0.018790  0.042439 -0.047982 -0.023436   \n",
       "\n",
       "        430       431       432       433       434       435       436  \\\n",
       "0  0.046733 -0.021436 -0.020737  0.038895  0.040358 -0.006694  0.020653   \n",
       "1  0.007095 -0.055049 -0.041980 -0.025730  0.000140 -0.057979  0.028223   \n",
       "2  0.047233  0.042405 -0.042446 -0.015038 -0.005213 -0.068683  0.068838   \n",
       "3  0.022168  0.043360 -0.034301  0.031476  0.044415 -0.062098 -0.013322   \n",
       "4  0.069657 -0.014867 -0.053325 -0.057066  0.034266 -0.062832  0.002779   \n",
       "\n",
       "        437       438       439       440       441       442       443  \\\n",
       "0 -0.030670  0.009918 -0.033957  0.003464  0.043648 -0.039115 -0.019880   \n",
       "1  0.023330  0.030896 -0.022736  0.001989  0.025023  0.026305 -0.043724   \n",
       "2  0.006120 -0.000853  0.004461  0.007229  0.050998 -0.081806 -0.019911   \n",
       "3  0.035706 -0.012341 -0.028577  0.030244  0.013164 -0.055833 -0.053326   \n",
       "4 -0.009303  0.035855  0.019821  0.006551  0.045036 -0.043001 -0.024296   \n",
       "\n",
       "        444       445       446       447       448       449       450  \\\n",
       "0 -0.043601  0.007612 -0.026493 -0.050176  0.006322  0.036572 -0.058249   \n",
       "1 -0.008223 -0.025230  0.022178 -0.044566 -0.059452 -0.021281  0.026165   \n",
       "2  0.001664  0.037300 -0.004829 -0.054334 -0.029874 -0.010412 -0.060177   \n",
       "3 -0.060028  0.015801 -0.017981 -0.037173  0.014053  0.024343  0.009755   \n",
       "4 -0.041737 -0.001507 -0.028926 -0.013747 -0.004190  0.012009 -0.027855   \n",
       "\n",
       "        451       452       453       454       455       456       457  \\\n",
       "0 -0.002921  0.055421  0.009126 -0.025683 -0.012394 -0.033153  0.017731   \n",
       "1  0.028602 -0.005644  0.018861 -0.022547 -0.025565  0.020375 -0.012497   \n",
       "2  0.051677  0.041015 -0.024855 -0.011949 -0.044116 -0.014593 -0.025919   \n",
       "3  0.010339  0.057975  0.029556 -0.054920 -0.004234 -0.011067  0.045467   \n",
       "4  0.062042  0.057830 -0.017357 -0.002691  0.021538  0.022698  0.020804   \n",
       "\n",
       "        458       459       460       461       462       463       464  \\\n",
       "0 -0.042585 -0.032140  0.038905  0.043402  0.003853  0.033806 -0.032124   \n",
       "1 -0.059997 -0.033381  0.008854 -0.000027 -0.035229  0.033107 -0.043477   \n",
       "2 -0.061265 -0.007169  0.018168  0.030079  0.017743  0.023966 -0.026182   \n",
       "3 -0.025380 -0.012809  0.018434  0.049449  0.054997  0.023123  0.035135   \n",
       "4 -0.002348  0.000068  0.047149  0.051695  0.014969  0.022777  0.011337   \n",
       "\n",
       "        465       466       467       468       469       470       471  \\\n",
       "0  0.018957 -0.014965 -0.030667 -0.010237  0.014229 -0.029927  0.049333   \n",
       "1 -0.016069 -0.022041 -0.032493  0.046335  0.003235 -0.000954  0.012479   \n",
       "2  0.046755  0.006979 -0.052076 -0.026423 -0.017431 -0.026269  0.027149   \n",
       "3 -0.013567 -0.042255 -0.015039 -0.032442  0.012681 -0.017328  0.016732   \n",
       "4  0.066303 -0.016801 -0.061075  0.005618 -0.004727 -0.007571  0.054487   \n",
       "\n",
       "        472       473       474       475       476       477       478  \\\n",
       "0  0.007827 -0.019476  0.062580  0.003257 -0.021132 -0.016794 -0.036559   \n",
       "1 -0.066077 -0.044808 -0.035860  0.002670 -0.060185 -0.070611 -0.056435   \n",
       "2 -0.014962 -0.004338  0.051730 -0.008679 -0.010420 -0.035210  0.005750   \n",
       "3  0.015795  0.031414  0.031735 -0.018684 -0.017713 -0.053451 -0.028899   \n",
       "4  0.051852 -0.027595  0.049119  0.024895  0.039589 -0.005180 -0.030776   \n",
       "\n",
       "        479       480       481       482       483       484       485  \\\n",
       "0 -0.035611  0.011417 -0.001510 -0.016450 -0.021570  0.029265 -0.052247   \n",
       "1  0.007213 -0.062969 -0.057098  0.030148 -0.029601 -0.054524 -0.021627   \n",
       "2 -0.039272  0.002130  0.034935 -0.067422 -0.018009  0.053192 -0.004579   \n",
       "3 -0.029981 -0.012296  0.013980 -0.000314  0.035472  0.017066 -0.007163   \n",
       "4 -0.027065  0.002500  0.002302  0.004549  0.035560  0.008675 -0.016734   \n",
       "\n",
       "        486       487       488       489       490       491       492  \\\n",
       "0 -0.004660 -0.003738 -0.004703 -0.017634 -0.040621 -0.021514 -0.070704   \n",
       "1 -0.054563  0.007108 -0.020683 -0.044945 -0.053670  0.006326 -0.058823   \n",
       "2 -0.067054  0.037026  0.000417 -0.045728 -0.048394 -0.030780 -0.074595   \n",
       "3 -0.037021  0.025341 -0.005736 -0.008843 -0.011976 -0.012992 -0.067666   \n",
       "4 -0.028919  0.031606  0.026708 -0.029746 -0.020197  0.016982 -0.044610   \n",
       "\n",
       "        493       494       495       496       497       498       499  \\\n",
       "0  0.003862  0.026746  0.013707  0.019000  0.026812 -0.069007 -0.070804   \n",
       "1 -0.054704 -0.053761  0.045445 -0.018459  0.016836 -0.005326 -0.064511   \n",
       "2  0.019029  0.002798  0.029871 -0.001668  0.000536 -0.085181 -0.056523   \n",
       "3  0.026549  0.036582  0.063653 -0.016117 -0.031707 -0.070545 -0.028672   \n",
       "4  0.026172 -0.032372  0.040891 -0.008191 -0.015572 -0.084396 -0.016877   \n",
       "\n",
       "        500       501       502       503       504       505       506  \\\n",
       "0  0.033391 -0.022498 -0.055149 -0.056354 -0.015443 -0.007421  0.039084   \n",
       "1 -0.013540 -0.078402  0.033434 -0.012687 -0.063992  0.006111 -0.031982   \n",
       "2 -0.046879  0.015715 -0.024961 -0.012671  0.007758 -0.042361  0.076179   \n",
       "3 -0.016142 -0.037535  0.024257 -0.015881  0.035338 -0.027082  0.034739   \n",
       "4 -0.034199  0.030582  0.038027  0.020157  0.035027 -0.012004  0.024360   \n",
       "\n",
       "        507       508       509       510       511       512       513  \\\n",
       "0 -0.040486 -0.000920 -0.021299  0.027961 -0.000931 -0.020922 -0.032573   \n",
       "1 -0.002981 -0.004428 -0.008878  0.033945 -0.035079 -0.027763 -0.054834   \n",
       "2  0.021334 -0.025536  0.000238 -0.007450 -0.024533 -0.045116 -0.001815   \n",
       "3  0.013127 -0.014858  0.029429  0.023854 -0.044343 -0.014829 -0.006513   \n",
       "4  0.042955  0.031588  0.038404  0.034823 -0.052799 -0.052401 -0.026398   \n",
       "\n",
       "        514       515       516       517       518       519       520  \\\n",
       "0  0.013119  0.033797  0.045542  0.035572 -0.061040  0.029642  0.023532   \n",
       "1 -0.047795 -0.006555 -0.029269 -0.031009 -0.001570  0.031833 -0.009577   \n",
       "2  0.003114  0.048100  0.062544  0.037670 -0.000412  0.071879 -0.035544   \n",
       "3 -0.022683  0.038416  0.041620  0.064459 -0.039872  0.061613 -0.042957   \n",
       "4 -0.056452  0.058365  0.046795  0.051605 -0.042657  0.067626  0.002775   \n",
       "\n",
       "        521       522       523       524       525       526       527  \\\n",
       "0  0.037593  0.031198 -0.063085 -0.054206 -0.026588  0.024658 -0.014704   \n",
       "1  0.002759  0.026573  0.024791 -0.044695  0.006300  0.021884  0.035465   \n",
       "2  0.063520 -0.037774 -0.068834  0.024402 -0.039003 -0.007822 -0.007629   \n",
       "3  0.042632  0.031210 -0.039890 -0.022487 -0.001909 -0.024828 -0.015127   \n",
       "4  0.042008  0.029812 -0.057214 -0.015844 -0.029933 -0.030955  0.032629   \n",
       "\n",
       "        528       529       530       531       532       533       534  \\\n",
       "0 -0.038299 -0.031824 -0.019739 -0.031320 -0.006948  0.012982 -0.055126   \n",
       "1 -0.032914  0.048790  0.029331  0.050386  0.001293 -0.042430 -0.046832   \n",
       "2  0.006040 -0.040305  0.050261 -0.014737 -0.003474  0.008663 -0.045799   \n",
       "3  0.016134 -0.031402  0.022955 -0.018509  0.005166 -0.050349 -0.065345   \n",
       "4 -0.005154 -0.011434  0.018981  0.007277 -0.020914  0.000880 -0.064025   \n",
       "\n",
       "        535       536       537       538       539       540       541  \\\n",
       "0 -0.026795 -0.006064  0.006591  0.021269  0.033001 -0.032250  0.015827   \n",
       "1  0.046444 -0.008148 -0.001692 -0.055412  0.040139 -0.035217 -0.043953   \n",
       "2 -0.036066 -0.021217  0.007143 -0.032458  0.077030 -0.001690  0.021305   \n",
       "3  0.023599  0.028924  0.015552  0.026402  0.066640 -0.012851  0.017554   \n",
       "4 -0.001053  0.025851 -0.015046  0.003338  0.079044 -0.016647  0.002999   \n",
       "\n",
       "        542       543       544       545       546       547       548  \\\n",
       "0  0.016510 -0.014776 -0.037681  0.004595 -0.026294  0.017547  0.022842   \n",
       "1  0.001075 -0.050073 -0.017950  0.012153 -0.012039 -0.059430 -0.046196   \n",
       "2  0.026819 -0.040538 -0.062240 -0.006705 -0.055694 -0.037191  0.017940   \n",
       "3  0.040091  0.003241 -0.052622 -0.006762 -0.039172  0.008828  0.053123   \n",
       "4  0.043950 -0.014483  0.012547 -0.060449 -0.031162 -0.000889  0.028506   \n",
       "\n",
       "        549       550       551       552       553       554       555  \\\n",
       "0 -0.016385  0.013136 -0.068767 -0.045178 -0.014487 -0.078171 -0.023002   \n",
       "1 -0.060626 -0.041724 -0.052979  0.021564 -0.019205 -0.043660 -0.038205   \n",
       "2 -0.013722 -0.060653 -0.066045 -0.015517  0.009460 -0.043011  0.001157   \n",
       "3 -0.021578 -0.052645 -0.068273 -0.022926 -0.015902 -0.041432 -0.040667   \n",
       "4 -0.036522 -0.040901 -0.066498 -0.024918  0.008092 -0.054073 -0.070110   \n",
       "\n",
       "        556       557       558       559       560       561       562  \\\n",
       "0 -0.036315 -0.052431 -0.000520 -0.009770  0.045363  0.026867 -0.047980   \n",
       "1 -0.072295 -0.041036  0.025243 -0.010375 -0.044046 -0.067721  0.017963   \n",
       "2 -0.059165  0.011427  0.000392  0.005337  0.065425  0.003561 -0.040612   \n",
       "3 -0.069364  0.013315 -0.026633 -0.046440  0.051262  0.032154 -0.043774   \n",
       "4 -0.068026 -0.038565 -0.024331 -0.023650  0.048563  0.060248 -0.008320   \n",
       "\n",
       "        563       564       565       566       567       568       569  \\\n",
       "0 -0.027432 -0.027310  0.038483 -0.026820  0.036034  0.037297 -0.031180   \n",
       "1 -0.057155  0.027380 -0.053026  0.006680 -0.001312 -0.003877  0.027125   \n",
       "2 -0.013002  0.005486  0.056963 -0.009979 -0.024970  0.035551  0.061707   \n",
       "3 -0.016167  0.006688  0.052867 -0.007713  0.018891  0.025292 -0.011269   \n",
       "4 -0.020068  0.021731  0.042014 -0.007218  0.017283  0.064126  0.026652   \n",
       "\n",
       "        570       571       572       573       574       575       576  \\\n",
       "0 -0.059132 -0.007731 -0.027518  0.031829  0.008714 -0.032036  0.042813   \n",
       "1 -0.017378  0.009125 -0.055527 -0.041034 -0.024052  0.018046  0.001468   \n",
       "2 -0.076678 -0.024015  0.034280 -0.023932  0.034398  0.003275  0.034987   \n",
       "3 -0.040812  0.018795  0.012903 -0.005320  0.028203  0.003377  0.029359   \n",
       "4 -0.071584 -0.028738  0.017788  0.048558  0.023424 -0.009475  0.047301   \n",
       "\n",
       "        577       578       579       580       581       582       583  \\\n",
       "0 -0.034186 -0.058650 -0.004650 -0.029011 -0.008094  0.025438 -0.016285   \n",
       "1 -0.039330 -0.047653  0.042111  0.017989 -0.015446  0.051372 -0.031258   \n",
       "2 -0.057774 -0.019418 -0.039449  0.002343  0.001089 -0.019458 -0.024801   \n",
       "3 -0.034545 -0.061794 -0.058531 -0.030271 -0.051671  0.033150  0.023626   \n",
       "4 -0.017419 -0.001503 -0.027638  0.031032 -0.022966 -0.025931 -0.017633   \n",
       "\n",
       "        584       585       586       587       588       589       590  \\\n",
       "0 -0.068860 -0.014764 -0.070535 -0.036185 -0.006802  0.023873 -0.037405   \n",
       "1 -0.055496 -0.059705 -0.023875 -0.042165  0.019940  0.027358 -0.048551   \n",
       "2 -0.039026  0.033206 -0.038913 -0.044096  0.027865 -0.014355 -0.009583   \n",
       "3 -0.019329  0.003444 -0.056636 -0.020651  0.009894 -0.019423 -0.030430   \n",
       "4 -0.022676  0.052170 -0.040386  0.019453 -0.015551 -0.045418  0.000249   \n",
       "\n",
       "        591       592       593       594       595       596       597  \\\n",
       "0 -0.008423 -0.009682 -0.013933  0.003593 -0.056361 -0.002160  0.004164   \n",
       "1  0.028619 -0.023517 -0.009893  0.042249 -0.050274  0.028570  0.044214   \n",
       "2 -0.010735  0.041863  0.036662 -0.015503 -0.034403  0.027502 -0.011552   \n",
       "3 -0.038732 -0.024972 -0.013444  0.058141 -0.002052 -0.020304  0.006518   \n",
       "4 -0.022455  0.022014  0.054025 -0.027351  0.029764  0.002437 -0.003595   \n",
       "\n",
       "        598       599       600       601       602       603       604  \\\n",
       "0  0.021140 -0.051472 -0.021516  0.005874 -0.070884 -0.051379  0.000021   \n",
       "1  0.012335 -0.062726  0.024975  0.026510  0.013826  0.031583  0.053254   \n",
       "2  0.007595 -0.029743  0.030767  0.040959 -0.041394 -0.015465 -0.053601   \n",
       "3  0.016561 -0.069910 -0.009482  0.014388  0.011875 -0.040205  0.007239   \n",
       "4 -0.036281 -0.024828  0.003400  0.040060 -0.016915 -0.045029  0.021558   \n",
       "\n",
       "        605       606       607       608       609       610       611  \\\n",
       "0 -0.025921 -0.065846 -0.067918 -0.051060 -0.057959 -0.037975 -0.056151   \n",
       "1 -0.032539  0.014086 -0.006991 -0.027323 -0.027311 -0.049971 -0.063147   \n",
       "2 -0.043798 -0.049690 -0.068626 -0.034806 -0.014614 -0.016748 -0.048023   \n",
       "3 -0.038522 -0.051796 -0.028993 -0.060027 -0.016108 -0.040879 -0.051498   \n",
       "4  0.006657 -0.042960 -0.039728  0.005427 -0.000930  0.011858 -0.007777   \n",
       "\n",
       "        612       613       614       615       616       617       618  \\\n",
       "0 -0.046440  0.023330 -0.016236 -0.022025  0.014357 -0.057889 -0.027978   \n",
       "1 -0.014950  0.058354 -0.032042 -0.030554 -0.007219 -0.058864 -0.033108   \n",
       "2  0.011804 -0.053831  0.035710 -0.040100  0.030246 -0.046397 -0.023452   \n",
       "3 -0.006144 -0.033865 -0.055437 -0.048156 -0.040606 -0.048795 -0.043679   \n",
       "4 -0.024108 -0.023775  0.003627 -0.063811  0.009021 -0.033360 -0.028815   \n",
       "\n",
       "        619       620       621       622       623       624       625  \\\n",
       "0 -0.012089  0.007134 -0.062871 -0.050490  0.018856  0.053673 -0.061114   \n",
       "1  0.011605  0.003425 -0.031755  0.004768 -0.000827 -0.011048 -0.010535   \n",
       "2 -0.045955 -0.039339 -0.027649 -0.012623 -0.005102  0.049829 -0.030375   \n",
       "3 -0.022880 -0.030084 -0.072675 -0.012751 -0.029991  0.061299 -0.056816   \n",
       "4 -0.001102 -0.027291 -0.055037  0.023227 -0.021793  0.036812 -0.042828   \n",
       "\n",
       "        626       627       628       629       630       631       632  \\\n",
       "0  0.034963 -0.024664 -0.010300 -0.041777  0.013548 -0.047875 -0.008419   \n",
       "1 -0.031720 -0.039835  0.048626 -0.008278 -0.008295  0.031869  0.005091   \n",
       "2  0.014987 -0.009591 -0.011586 -0.026485  0.060765 -0.053228  0.016675   \n",
       "3 -0.002892  0.022255  0.001892 -0.036331  0.040686 -0.063263  0.015379   \n",
       "4  0.008904  0.002114 -0.000668 -0.032198  0.025218 -0.035848  0.028674   \n",
       "\n",
       "        633       634       635       636       637       638       639  \\\n",
       "0 -0.005153  0.008514  0.001557 -0.043248  0.051303 -0.046123 -0.010920   \n",
       "1 -0.031928  0.006977 -0.040171  0.004495 -0.023325  0.022757 -0.002867   \n",
       "2 -0.032229  0.023691  0.055550 -0.063425  0.048862 -0.067048 -0.034671   \n",
       "3 -0.048215  0.050425  0.050578 -0.069272  0.044709 -0.052623  0.005587   \n",
       "4 -0.025492  0.002753  0.006651 -0.056653  0.041571 -0.057128  0.041394   \n",
       "\n",
       "        640       641       642       643       644       645       646  \\\n",
       "0 -0.049496 -0.047422 -0.055813 -0.001092 -0.033612  0.045778  0.039050   \n",
       "1 -0.011220 -0.052424 -0.021003 -0.059515  0.031493 -0.025217  0.058204   \n",
       "2 -0.019439 -0.029480  0.007398 -0.032935 -0.000020  0.060325  0.054141   \n",
       "3 -0.042926 -0.051903 -0.032711 -0.054837 -0.022148  0.024751  0.051745   \n",
       "4 -0.052616 -0.043863  0.026294 -0.053773 -0.027309  0.062212  0.045539   \n",
       "\n",
       "        647       648       649       650       651       652       653  \\\n",
       "0 -0.024479 -0.003441 -0.003868 -0.050178  0.023274 -0.006806 -0.007625   \n",
       "1 -0.029380 -0.002297 -0.045685 -0.044427 -0.015865  0.005254 -0.053030   \n",
       "2 -0.016188  0.004541  0.006006  0.025946 -0.010343  0.034404 -0.039990   \n",
       "3 -0.051832 -0.023279 -0.035666 -0.044307 -0.005150  0.011367 -0.042235   \n",
       "4 -0.023669 -0.000307 -0.008739 -0.019718 -0.002310 -0.019890 -0.024340   \n",
       "\n",
       "        654       655       656       657       658       659       660  \\\n",
       "0 -0.038461 -0.008437 -0.042519 -0.052310 -0.022626 -0.001241  0.001597   \n",
       "1 -0.000822  0.027366 -0.051657  0.017930  0.008664 -0.049349 -0.022269   \n",
       "2  0.004940 -0.012243 -0.004247 -0.045913  0.029367 -0.016169  0.026333   \n",
       "3  0.059724 -0.042001 -0.046602 -0.056383  0.034594  0.047642  0.046315   \n",
       "4 -0.011125 -0.022231  0.017658 -0.021805 -0.003099 -0.025080  0.014204   \n",
       "\n",
       "        661       662       663       664       665       666       667  \\\n",
       "0  0.015903 -0.050896  0.046169 -0.050201  0.011978 -0.039249  0.003320   \n",
       "1  0.027957  0.019159  0.016509  0.009524  0.003753 -0.038253 -0.042656   \n",
       "2 -0.010761 -0.004596  0.050391 -0.039973  0.004939 -0.031544  0.012512   \n",
       "3 -0.008817 -0.028448  0.004930 -0.030082 -0.015233 -0.054357 -0.055159   \n",
       "4 -0.062823 -0.007316  0.031108 -0.057578  0.008798 -0.040028 -0.004269   \n",
       "\n",
       "        668       669       670       671       672       673       674  \\\n",
       "0 -0.020891  0.056057  0.032926  0.070106  0.012712  0.004962  0.040885   \n",
       "1  0.025039 -0.050888  0.011036 -0.035658 -0.078400 -0.014437 -0.070811   \n",
       "2 -0.010299  0.031299  0.030247  0.062524 -0.016833  0.023175  0.058165   \n",
       "3  0.043098  0.037228  0.023650  0.054177 -0.017742 -0.004167  0.034210   \n",
       "4 -0.023626  0.038350 -0.013341  0.041302  0.019390  0.046680  0.021299   \n",
       "\n",
       "        675       676       677       678       679       680       681  \\\n",
       "0 -0.052152  0.055809  0.016928  0.004452 -0.042174 -0.056090 -0.049169   \n",
       "1 -0.027193  0.044431 -0.041020 -0.008999 -0.034534  0.019317 -0.074715   \n",
       "2 -0.022931  0.008729 -0.038324 -0.005457 -0.003796 -0.005050 -0.045222   \n",
       "3 -0.050900  0.053110 -0.040369 -0.004762  0.036295 -0.002100 -0.046518   \n",
       "4 -0.036904  0.012153  0.045327 -0.021670 -0.024917 -0.010478 -0.035350   \n",
       "\n",
       "        682       683       684       685       686       687       688  \\\n",
       "0 -0.049363  0.053747  0.032561 -0.028358  0.010958 -0.045892  0.040456   \n",
       "1 -0.019871 -0.050320 -0.032099  0.022733 -0.025217 -0.031488 -0.003999   \n",
       "2 -0.008217  0.061915  0.019607  0.007865  0.004280 -0.047831  0.016658   \n",
       "3 -0.051892  0.026491 -0.016430 -0.031188 -0.056881 -0.058733  0.013560   \n",
       "4 -0.059572  0.042985  0.047083 -0.024677 -0.012715 -0.051293  0.022870   \n",
       "\n",
       "        689       690       691       692       693       694       695  \\\n",
       "0 -0.037873 -0.043589 -0.079459 -0.006884  0.041131 -0.010238  0.029712   \n",
       "1  0.031495 -0.036209 -0.075756 -0.041112  0.025549  0.014089 -0.021566   \n",
       "2 -0.008551 -0.062441 -0.087240  0.018604 -0.017256 -0.033613 -0.011071   \n",
       "3  0.059658 -0.050335 -0.080230 -0.011831  0.000166 -0.073854  0.016977   \n",
       "4  0.041177 -0.023152 -0.077932 -0.014958  0.023975 -0.063354 -0.006642   \n",
       "\n",
       "        696       697       698       699       700       701       702  \\\n",
       "0  0.037579 -0.011273 -0.008118  0.028333 -0.032969 -0.057758 -0.003042   \n",
       "1 -0.004238 -0.012103 -0.023819  0.017329  0.027925 -0.062967 -0.020506   \n",
       "2  0.049141 -0.045517 -0.006953  0.042029 -0.030503  0.031271 -0.040365   \n",
       "3  0.058198 -0.037490 -0.043100  0.051441 -0.047889 -0.054388  0.007461   \n",
       "4 -0.008652 -0.002116 -0.023009  0.041365 -0.059022 -0.039613 -0.032852   \n",
       "\n",
       "        703       704       705       706       707       708       709  \\\n",
       "0  0.017432  0.054796 -0.032331 -0.028657 -0.072479 -0.066265 -0.054768   \n",
       "1 -0.010694 -0.075328  0.006842 -0.042582 -0.017590 -0.027338 -0.007907   \n",
       "2  0.001177  0.034939  0.033515 -0.035650 -0.045078 -0.029359  0.024757   \n",
       "3 -0.021159  0.057263 -0.025591  0.029413 -0.066811 -0.048771 -0.045087   \n",
       "4 -0.027261  0.045291  0.026285 -0.049288 -0.068103 -0.052761 -0.033851   \n",
       "\n",
       "        710       711       712       713       714       715       716  \\\n",
       "0 -0.040680 -0.000351 -0.010429  0.024509 -0.038953  0.038253  0.013392   \n",
       "1 -0.025668 -0.008522  0.020732 -0.003057  0.014741 -0.045396 -0.000815   \n",
       "2  0.023967  0.005200  0.039175  0.008007 -0.035556  0.024410 -0.043322   \n",
       "3  0.009543  0.064123  0.002998 -0.038227 -0.040773 -0.049714  0.022912   \n",
       "4 -0.009923 -0.006535  0.040565  0.026662 -0.063127  0.019787  0.000488   \n",
       "\n",
       "        717       718       719       720       721       722       723  \\\n",
       "0  0.004551 -0.031836 -0.045536 -0.044118  0.003859  0.005132 -0.016759   \n",
       "1  0.033734 -0.023224  0.025429  0.002011  0.007544  0.017693  0.029330   \n",
       "2  0.003650 -0.042928 -0.044471 -0.050518  0.039983 -0.018226  0.018707   \n",
       "3  0.000718 -0.075023 -0.039188 -0.003765  0.040923 -0.057215  0.006998   \n",
       "4 -0.023238 -0.080117 -0.068065 -0.055372 -0.002805 -0.017776 -0.045444   \n",
       "\n",
       "        724       725       726       727       728       729       730  \\\n",
       "0 -0.056777 -0.048227  0.016034 -0.008874 -0.020328  0.036249 -0.004574   \n",
       "1  0.024083 -0.050235  0.008195 -0.060817 -0.081728  0.027943  0.017330   \n",
       "2 -0.075105 -0.043529 -0.005360 -0.001082 -0.058748 -0.019642  0.027507   \n",
       "3 -0.050240 -0.040368 -0.060560  0.010078 -0.064781 -0.017414  0.022735   \n",
       "4 -0.009043 -0.011753  0.028354  0.020486 -0.071744 -0.038359 -0.015980   \n",
       "\n",
       "        731       732       733       734       735       736       737  \\\n",
       "0  0.045056  0.033146  0.010940  0.059654  0.026574 -0.000199  0.053398   \n",
       "1  0.012873 -0.030603  0.025109 -0.044979  0.028983  0.023557 -0.031516   \n",
       "2  0.034679 -0.008890  0.037954  0.009405 -0.038546  0.011280 -0.005038   \n",
       "3  0.030508 -0.035831  0.049904 -0.006664  0.013684  0.039847 -0.017036   \n",
       "4  0.030378  0.071237  0.051422  0.028043  0.031570  0.043080  0.026217   \n",
       "\n",
       "        738       739       740       741       742       743       744  \\\n",
       "0  0.013548 -0.026796 -0.021011  0.057906 -0.002990  0.061430 -0.053808   \n",
       "1 -0.010376  0.022773  0.027381  0.039309 -0.034558 -0.029595  0.023430   \n",
       "2  0.007344 -0.023666  0.069415  0.032804 -0.035571  0.051074 -0.030803   \n",
       "3 -0.015462 -0.043418  0.023723  0.022250  0.025077  0.067034 -0.003046   \n",
       "4 -0.009842 -0.009323  0.045603 -0.035345 -0.039754  0.054821 -0.009467   \n",
       "\n",
       "        745       746       747       748       749       750       751  \\\n",
       "0 -0.064560  0.007055  0.000578  0.045201 -0.028622 -0.028130 -0.006591   \n",
       "1 -0.045198 -0.018821 -0.048212 -0.057645 -0.065396 -0.035478 -0.042057   \n",
       "2 -0.041638  0.019941 -0.015864 -0.046084 -0.065974 -0.043167 -0.016037   \n",
       "3 -0.016186  0.024116 -0.036549 -0.000747 -0.072148 -0.030169  0.005936   \n",
       "4 -0.019029  0.037464 -0.003999 -0.002397 -0.062597  0.003539 -0.004103   \n",
       "\n",
       "        752       753       754       755       756       757       758  \\\n",
       "0 -0.049347 -0.004778  0.004954 -0.020762 -0.050510  0.009578  0.004597   \n",
       "1 -0.012445 -0.039846  0.013547 -0.014138  0.002544  0.003335 -0.044256   \n",
       "2 -0.043990 -0.073447  0.030275 -0.026399 -0.044892 -0.014220  0.020261   \n",
       "3 -0.042314 -0.040760  0.010354 -0.007564 -0.026526  0.010523  0.009558   \n",
       "4 -0.035170 -0.044910  0.028600 -0.052389 -0.027406  0.008259  0.071624   \n",
       "\n",
       "        759       760       761       762       763       764       765  \\\n",
       "0  0.045674  0.016957  0.003439  0.055194 -0.044291  0.034094 -0.038885   \n",
       "1 -0.027914 -0.017669  0.026550  0.047473  0.013402 -0.009568  0.000791   \n",
       "2  0.026414  0.023692  0.006574  0.007752  0.001251  0.003044 -0.043307   \n",
       "3  0.030677  0.000332  0.065556  0.005900  0.003982 -0.000807 -0.029937   \n",
       "4  0.034412  0.007865  0.056166 -0.014121 -0.036367  0.041306  0.005384   \n",
       "\n",
       "        766       767  \n",
       "0 -0.036568 -0.032016  \n",
       "1  0.002250 -0.052819  \n",
       "2 -0.004868  0.001397  \n",
       "3 -0.075138 -0.037401  \n",
       "4 -0.051179 -0.029693  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "bert_embeddings_df_train_labse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_features_train = bert_embeddings_df_train.drop(['suspended'], axis=1)\n",
    "acct_features_train_labse = bert_embeddings_df_train_labse.drop(['suspended'], axis=1).iloc[:,0:bert_embeddings_df_train_labse.drop(['suspended'], axis=1).columns.get_loc('0')]\n",
    "text_feature_train_labse = bert_embeddings_df_train_labse.iloc[:, bert_embeddings_df_train_labse.drop(['suspended'], axis=1).columns.get_loc('0') + 1:]\n",
    "labels_train_labse = bert_embeddings_df_train_labse['suspended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_train_labse = acct_features_train_labse[selected_feat]\n",
    "combined_features_train_labse = pd.concat([acct_features_train_labse, text_feature_train_labse], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_features_valid = bert_embeddings_df_valid.drop(['suspended'], axis=1)\n",
    "acct_features_valid_labse = bert_embeddings_df_valid_labse.drop(['suspended'], axis=1).iloc[:,0:bert_embeddings_df_valid_labse.drop(['suspended'], axis=1).columns.get_loc('0')]\n",
    "text_feature_valid_labse = bert_embeddings_df_valid_labse.iloc[:, bert_embeddings_df_valid_labse.drop(['suspended'], axis=1).columns.get_loc('0') + 1:]\n",
    "labels_valid_labse = bert_embeddings_df_valid_labse['suspended']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_valid_labse = acct_features_valid_labse[selected_feat]\n",
    "combined_features_valid_labse = pd.concat([acct_features_valid_labse, text_feature_valid_labse], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_test_labse = bert_embeddings_df_test_labse.drop(['suspended'], axis=1).iloc[:,0:bert_embeddings_df_test_labse.drop(['suspended'], axis=1).columns.get_loc('0')]\n",
    "text_feature_test_labse = bert_embeddings_df_test_labse.iloc[:, bert_embeddings_df_test_labse.drop(['suspended'], axis=1).columns.get_loc('0') + 1:]\n",
    "labels_test_labse = bert_embeddings_df_test_labse['suspended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct_features_test_labse = acct_features_test_labse[selected_feat]\n",
    "combined_features_test_labse = pd.concat([acct_features_test_labse, text_feature_test_labse], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_fw_labse = data_prep(labels_train_labse)\n",
    "len(labels_train_fw_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [0, 0]\n",
    "for el in labels_train_fw_labse :\n",
    "    class_counts[np.argmax(el)]+=1\n",
    "class_weights_labse = {idx:sum(class_counts)/el for idx, el in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test label_encoding\n",
    "labels_train_labse, fit_train_labse=label_encoding(labels_train_labse)\n",
    "labels_valid_labse, fit_valid_labse=label_encoding(labels_valid_labse)\n",
    "labels_test_labse, fit_test_labse=label_encoding(labels_test_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure numpy array\n",
    "# X\n",
    "train_data_tweettext_labse = np.array(text_feature_train_labse)\n",
    "valid_data_tweettext_labse = np.array(text_feature_valid_labse)\n",
    "test_data_tweettext_labse = np.array(text_feature_test_labse)\n",
    "\n",
    "# Y\n",
    "train_labels_tweettext_labse = labels_train_labse\n",
    "valid_labels_tweettext_labse = labels_valid_labse\n",
    "test_labels_tweettext_labse = labels_test_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input shape for tweet embeddings\n",
    "input_shape_tweettext_labse=train_data_tweettext_labse[0].shape\n",
    "input_shape_tweettext_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "random_seeds(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Tweet_text_labse\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Combined_inputs (InputLayer) [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "normalization_1 (BatchNormal (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 52,114\n",
      "Trainable params: 51,986\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model flow and summary\n",
    "Tweettext_model_labse = model_flow(\"Tweet_text_labse\",num_of_labels, input_shape_tweettext_labse)\n",
    "Tweettext_model_labse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile  model\n",
    "Tweettext_model_labse.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on Tweet Text training data\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3766 - accuracy: 0.4165 - val_loss: 0.6683 - val_accuracy: 0.3980\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3840 - accuracy: 0.4265 - val_loss: 0.6986 - val_accuracy: 0.3980\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.3713 - accuracy: 0.4267 - val_loss: 0.6727 - val_accuracy: 0.3980\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3787 - accuracy: 0.4298 - val_loss: 0.7133 - val_accuracy: 0.3980\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3807 - accuracy: 0.4223 - val_loss: 0.6790 - val_accuracy: 0.3980\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3852 - accuracy: 0.4270 - val_loss: 0.6990 - val_accuracy: 0.3980\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 12s 2ms/step - loss: 1.3780 - accuracy: 0.4311 - val_loss: 0.7001 - val_accuracy: 0.3980\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 13s 2ms/step - loss: 1.3698 - accuracy: 0.4266 - val_loss: 0.7348 - val_accuracy: 0.3362\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3777 - accuracy: 0.4104 - val_loss: 0.7063 - val_accuracy: 0.3886\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3820 - accuracy: 0.4116 - val_loss: 0.6736 - val_accuracy: 0.3980\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on Tweet Text training data\")\n",
    "history_tweettext_labse = Tweettext_model_labse.fit(train_data_tweettext_labse, labels_train_labse, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_tweettext_labse, labels_valid_labse), class_weight = class_weights_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Tweet Text test data\n",
      "2914/2914 [==============================] - 3s 934us/step - loss: 0.6674 - accuracy: 0.4372\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on Tweet Text test data\")\n",
    "Tweettext_model_results_labse = Tweettext_model_labse.evaluate(test_data_tweettext_labse,labels_test_labse, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweettext_model_labse.save('Tweettext_model_labse.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "client = boto3.client('s3')\n",
    "s3.upload_file(Filename='Tweettext_model_labse.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Tweettext_model_labse.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Tweettext_model_labse.h5',\n",
    "                     'Tweettext_model_labse.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Tweettext_model_labse = load_model('Tweettext_model_labse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.42      0.57     26238\n",
      "           1       0.10      0.62      0.18      2901\n",
      "\n",
      "    accuracy                           0.44     29139\n",
      "   macro avg       0.51      0.52      0.38     29139\n",
      "weighted avg       0.83      0.44      0.53     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_tweettext_labse=predict_test(Tweettext_model_labse,test_data_tweettext_labse,labels_test_labse, fit_test_labse)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_tweettext_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext_labse = predict_account(Tweettext_model_labse, train_data_tweettext_labse, bert_embeddings_df_train_labse, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_tweettext.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.78      1440\n",
      "           1       0.09      0.44      0.15       110\n",
      "\n",
      "    accuracy                           0.65      1550\n",
      "   macro avg       0.52      0.55      0.47      1550\n",
      "weighted avg       0.88      0.65      0.74      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_tweettext_labse = classification_report(np.array(train_account_preds_tweettext_labse['suspended_label']), np.array(train_account_preds_tweettext_labse['pred_class']))\n",
    "print(report_tr_tweettext_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext_labse = predict_account(Tweettext_model_labse, valid_data_tweettext_labse, bert_embeddings_df_valid_labse, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_tweettext.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77       523\n",
      "           1       0.05      0.24      0.09        41\n",
      "\n",
      "    accuracy                           0.63       564\n",
      "   macro avg       0.49      0.45      0.43       564\n",
      "weighted avg       0.85      0.63      0.72       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_tweettext_labse = classification_report(np.array(valid_account_preds_tweettext_labse['suspended_label']), np.array(valid_account_preds_tweettext_labse['pred_class']))\n",
    "print(report_v_tweettext_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext_labse = predict_account(Tweettext_model_labse, test_data_tweettext_labse, bert_embeddings_df_test_labse, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_tweettext.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.66      0.77       467\n",
      "           1       0.11      0.45      0.18        44\n",
      "\n",
      "    accuracy                           0.65       511\n",
      "   macro avg       0.52      0.56      0.48       511\n",
      "weighted avg       0.86      0.65      0.72       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_tweettext_labse = classification_report(np.array(test_account_preds_tweettext_labse['suspended_label']), np.array(test_account_preds_tweettext_labse['pred_class']))\n",
    "print(report_te_tweettext_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [0.6673619747161865, 0.437214732170105]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.42      0.57     26238\n",
      "           1       0.10      0.62      0.18      2901\n",
      "\n",
      "    accuracy                           0.44     29139\n",
      "   macro avg       0.51      0.52      0.38     29139\n",
      "weighted avg       0.83      0.44      0.53     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "plot_tweettext=accuracy_plot('Model accuracy of Tweet Text Model', history_tweettext_labse)\n",
    "print(plot_tweettext)\n",
    "\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Tweettext_model_results_labse)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_tweettext)\n",
    "#Classification report\n",
    "print(metrics_report_tweettext_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Account only model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure numpy array\n",
    "train_data_acc = np.array(acct_features_train).astype(np.float32)\n",
    "valid_data_acc = np.array(acct_features_valid).astype(np.float32)\n",
    "test_data_acc = np.array(acct_features_test).astype(np.float32)\n",
    "\n",
    "train_labels = labels_train\n",
    "valid_labels = labels_valid\n",
    "test_labels = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input shape for account features\n",
    "input_shape_acc=train_data_acc[0].shape\n",
    "input_shape_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for model flow. Does not include any parameter tuning\n",
    "def model_flow(model_name, num_countries, input_shape):\n",
    "    inputs = keras.Input(shape=(input_shape), name=\"Combined_inputs\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(16, activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(0.15)(x)\n",
    "    outputs = layers.Dense(num_of_labels, activation=\"softmax\",name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Account_Info\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Combined_inputs (InputLayer) [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1984      \n",
      "_________________________________________________________________\n",
      "normalization_1 (BatchNormal (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 4,882\n",
      "Trainable params: 4,754\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model flow and summary\n",
    "Acc_model = model_flow(\"Account_Info\",num_of_labels, input_shape_acc)\n",
    "Acc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile  model\n",
    "Acc_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on Account Information features training data\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.3078 - accuracy: 0.5324 - val_loss: nan - val_accuracy: 0.12290s - loss: 1.3077 - accuracy: 0.53\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 13s 2ms/step - loss: 1.2927 - accuracy: 0.5574 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 13s 2ms/step - loss: 1.3037 - accuracy: 0.5310 - val_loss: nan - val_accuracy: 0.1745\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.2871 - accuracy: 0.5359 - val_loss: nan - val_accuracy: 0.1318\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.3007 - accuracy: 0.5138 - val_loss: nan - val_accuracy: 0.1271\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.2833 - accuracy: 0.5594 - val_loss: nan - val_accuracy: 0.1946\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2791 - accuracy: 0.5683 - val_loss: nan - val_accuracy: 0.9058\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 12s 2ms/step - loss: 1.2848 - accuracy: 0.5086 - val_loss: nan - val_accuracy: 0.2509\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.2897 - accuracy: 0.5372 - val_loss: nan - val_accuracy: 0.5812\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 18s 3ms/step - loss: 1.2830 - accuracy: 0.5231 - val_loss: nan - val_accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on Account Information features training data\")\n",
    "history_acc = Acc_model.fit(train_data_acc, train_labels, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_acc, valid_labels), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Account Information features test data\n",
      "2914/2914 [==============================] - 3s 972us/step - loss: nan - accuracy: 0.9004\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on Account Information features test data\")\n",
    "Acc_model_results = Acc_model.evaluate(test_data_acc, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_model.save('Acc_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='Acc_model.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Acc_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load back in model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Acc_model.h5',\n",
    "                     'Acc_model.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Acc_model = load_model('Acc_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     26238\n",
      "           1       0.00      0.00      0.00      2901\n",
      "\n",
      "    accuracy                           0.90     29139\n",
      "   macro avg       0.45      0.50      0.47     29139\n",
      "weighted avg       0.81      0.90      0.85     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_acc=predict_test(Acc_model,test_data_acc,test_labels, fit_test)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_acc = predict_account(Acc_model, train_data_acc, bert_embeddings_df_train, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_acc.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_acc.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1440\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.93      1550\n",
      "   macro avg       0.46      0.50      0.48      1550\n",
      "weighted avg       0.86      0.93      0.89      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_acc = classification_report(np.array(train_account_preds_acc['suspended_label']), np.array(train_account_preds_acc['pred_class']))\n",
    "print(report_tr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_acc = predict_account(Acc_model, valid_data_acc, bert_embeddings_df_valid, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_acc.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_acc.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       523\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.93       564\n",
      "   macro avg       0.46      0.50      0.48       564\n",
      "weighted avg       0.86      0.93      0.89       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_acc = classification_report(np.array(valid_account_preds_acc['suspended_label']), np.array(valid_account_preds_acc['pred_class']))\n",
    "print(report_v_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_acc = predict_account(Acc_model, test_data_acc, bert_embeddings_df_test, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_acc.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_acc.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.96       467\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.91       511\n",
      "   macro avg       0.46      0.50      0.48       511\n",
      "weighted avg       0.84      0.91      0.87       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_acc = classification_report(np.array(test_account_preds_acc['suspended_label']), np.array(test_account_preds_acc['pred_class']))\n",
    "print(report_te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Account Information Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABCpElEQVR4nO3deXxcdbn48c+Tfe+Spi1JlxRa6EK3EIoiSBWUXQRBqIpUVCxeF9yQ63XBi/zUC9frVREu2rJrQUAEbxEEWVTw2pS26U4XoC1N2jSlbdJmm+T5/fE9k07SLJN0Ts4sz/v1yisz55w555kzM+c53+Wcr6gqxhhjUlda0AEYY4wJliUCY4xJcZYIjDEmxVkiMMaYFGeJwBhjUpwlAmOMSXGWCLoRkXIRURHJiGLZhSLyt6GIK1GJyBgReVlEGkTkP4OOJ96IyA9EZK+I1AYdS3ciMkFEGkUkPehYDIjIzSLyYJTLvigin4l23QmdCETkTRFpFZFR3aav8g7m5QGFZo64DtgLFKnq13pbyPuSq4jMG7rQBiaak4QB/ljHA18Dpqvq2FjFOVje7+mc8HNV3a6qBara7sO2VEQOeYmmUUT2x2idk2MQ3rHGMd+L5fFu02d7018MKLReJXQi8LwBLAg/EZGZQG5w4cSHaEo0Q2QisF77uHJRRAS4GtgHXDNUgcWBiUC9qu4Z6Avj6PM9FrO9RFOgqsODDibGJZ864HQRKY6Ydg3wegy3ETPJkAgeAD4Z8fwa4P7IBURkmIjcLyJ1IvKWiHxbRNK8eekicrtXPN8GXNjDaxeLSI2IvO0V5aP6wojI70SkVkQOeNUjMyLm5YrIf3rxHBCRv4lIrjfvDBF5RUT2i8gOEVnoTe9S3OteNeWdbfyLiGwGNnvT/ttbx0ERWSEiZ0Ysny4i3xKRrV7VzQoRGS8id3SvxhGRp0Tkhl7e5+kistx7H8tF5HRv+r3e53Gjd9Z3Tk+vB84ESoEvA1eJSNYx7qe+Pu8uZ+zdz/K9fXyLiPzd2yfPypES58ve//3e+3l3L+8nct+oiCwSkc0i8o63b8XbF38GSr113est/yERWee9pxdFZFrEut4UkW+KSDVwSEQme+v/lPf+3/G2daqIVHvr+EXE608Qkb+ISL33fX9IRIZ78x4AJgBPefHc2MO+KRWRJ0Vkn4hsEZHPRqz7ZhF5xNvvDd57qOxv//Swv0pF5DHvs3tDRL4UMW+eiLzqva8aEflF+LsiIuHPZrUX/5XSQ9WtRJQaROReEblTRJaJyCHgfVFsv0rcb2m3iPykj7fSCjwBXOW9Nh34KPBQt3h6/O148yaJyEve/vwz0L3m410R3//VIjI/qp3cE1VN2D/gTeAcYBMwDUgHduDOtBQo95a7H/gDUAiU47Lyp715i4CNwHhgJPCC99oMb/4TwP8A+cBo4J/A57x5C4G/9RHftd42s4GfAqsi5t0BvAiUeXGf7i03AWjAlXIygWJgjveaF4HPRKyjy/a9uP/svY9cb9onvHVk4KohaoEcb943gDXASYAAs71l5wG7gDRvuVHAYWBMD+9xJPAO7ow+w4v7HaDYm38v8IN+PsfFwCPe+60HLjvG/dTX530z8GDE+su7fd4vAluBE3ElyxeBH/W0bC/vpfv6FfgjMNyLuQ44z5s3H9gZseyJwCHgA957uhHYAmRFfN9X4b6ruRHx3AXkAB8EmnHf2dHePtsDnOW9frK37mygBJfYftr999THvnkJ+KW3rTneezk74n03Axd4n9MPgX/0sZ8UmNxtWhqwAvgukAUcD2wDzvXmnwK8C/c9Kwc2ADf0tk56+H1GLoP7bh4A3uNtO6+f7b8KXO09LgDe1ct7mw/sxH1X/8+bdgHwDPAZ4MUofzuvAj/xPq/34r7vD3rzynC/lQu82D/gPS/p6VjR77F0qA/esfzjSCL4tvfFOw93IMzwPvBy70vZgquHDb/ucxEfxl+ARRHzPhj+8gNjvNfmRsxfALzQ2xetj1iHe+sd5n1wTbiicffl/hX4fS/r6PLhdt++t/739xPHO+Ht4hLoJb0stwH4gPf4C8CyXpa7Gvhnt2mvAgsjfmy9JgLcj+8g8GHv+f8Af/AeD3g/RfF530z/ieDbEfM/D/ypp2V7eT/d16/AGRHPHwFu8h7Pp2si+A7wSMTzNOBtYH7E9/3aHmIvi5hWD1wZ8fwxIg6W3WL9MLCy+++pp32DSz7tQGHE/B8C90a87+ci5k0HmvrYT+p97vu9v58BpwHbe/ic7+llHTdEfgcYXCK4P2Jen9vHJc7vA6P6+Y11fq64kvlJwFLg43RNBL3+dnAnDSEgP2LebziSCL4JPNDttc8A10R8j6NOBMlQzwiueuhlYBLdqoVwZ7NZwFsR097CZVRwVRI7us0Lm4g7M6sRkfC0tG7L98grCt4KXIE7++qIiCcbd1a1tYeXju9lerS6xCYiX8N9+UpxP4IijhQx+9rWfbjSxJ+9///dy3KldN1n0HX/9udS3Bd+mff8IeA5ESnBlVIGup/6+7yjEdmD5zDu7O9YRLu+LvtSVTtEZAddY+/pu7c74nFTD88LAERkNO6AeyautJSGOzGIRimwT1UbIqa9BURW/3R/nzkikqGqoV7WWaGqW8JPROSjuKqy/RHLpAN/9eafiDtDrsSdQGTgzuCPReT+nNjX9oFPA/8ObBSRN4Dvq+of+1n/A7gTqffhagg+FjGvr99OKfCOqh7qNm98RKxXiMjFEfMzcTUaA5YMbQSo6lu4RuMLgMe7zd4LtOF2XNgE3JkWQA1Hdm54XtgO3NnlKFUd7v0VqeoM+vcx4BJciWUY7uwK3MFtL64YfUIPr9vRy3Rw1QZ5Ec976mmi4Qfi2gO+iaubHKGuQe6AF0N/23oQuEREZuOq3Z7oZblddN230HX/9uca3IFqu7gulL/DfaEXMLj91N/nHc0+7I32v8gx6bIvxZ19jKfrvjyWGH7ovX6WqhbhErxEzO9r3buAkSJSGDFtIJ9zNHYAb0T81oaraqGqXuDNvxNXjTvFi/9b3eLvrstnLSJ9/l76276qblbVBbhqtx8Dj4pIfj/v6QFcqXKZqh7uNq+v304NMKLb+rsfmx7oFmu+qv6on3h6lBSJwPNpXLVIZAZFXde3R4BbRaRQRCYCX8Ud6PDmfUlExonICOCmiNfWAM8C/ykiRSKS5jW4nRVFPIW4JFKP+zL+v4j1dgBLgJ94jVPpIvJuEcnGnRGfIyIfFZEMESkWkTneS1cBl4lIntfg9ekoYgjh6nIzROS7uBJB2K+BW0RkijizxOvloKo7geW4L/JjqtrUyzaWASeKyMe8eK/EVQv0d6aEiJQBZwMX4eqc5+DaKX6MK+IOeD9F8XmvAt4rro/8MFzRP1p1uJLd8QN4zUA8AlwoImeLSCauTacFeCVG6y8EGnGN3WW4NqJIu+nlvanqDi+OH4pIjojMwn3/Hupp+UH6J3BQXIN4rvd5nywip0bEfxBoFJGpwPX9xL8amCEic0QkB1d9Nejti8gnRKTE+17u917TZ9daVX0DOAv4tx5m9/rb8U5uq4Dvi0iWiJwBRJ79PwhcLCLnenHmiOu2Oq6f99ijpEkEqrpVVat6mf1F3NnBNuBvuLq2Jd68X+Hq1lYDr3F0ieKTuKqG9bhi9KPAcVGEdD+uKPe299p/dJv/dVxD7XJct8kf4xpnt+NKNl/zpq/CHRwB/gvXG2E3ruqmvx/hM8DTuMbSt3Bn15FF4Z/gDj7P4n5gi+na9fY+YCYuGfRIVetxB/Kv4ZLejcBFqrq3n9jA1ZGuUtVnVbU2/IervpglIiczuP3U6+etqn8GHgaqcdUK/SasiPd6GFfd93evp8a7on1tlOvfhDtL/zmuZHMxcLGqtsZoE98HKnClwv/l6O/6D4Fve+/t6z28fgGuZLsL+D3wPW9/xoSXxC/GnRC8gdsHv8aVqMF9Fz6GazT9Fe5zjHQzcJ8X/0dV9XVcVc5zuLr6Pi/+jGL75wHrRKQRV1V6lao2R/G+/qaqu3qY3t9v52O4dot9wPeIqPb2EvMluFJRHe53/Q0GeUwXr2HBmKOIyHtxZx7l3lmQMSYJJU2JwMSWVzXxZeDXlgSMSW6WCMxRxF3EtB9XBfbTQIMxxvjOqoaMMSbFWYnAGGNSXMJdUDZq1CgtLy8POgxjjEkoK1as2KuqJT3NS7hEUF5eTlVVb71EjTHG9EREul/F3MmqhowxJsVZIjDGmBRnicAYY1KcJQJjjElxlgiMMSbFWSIwxpgUZ4nAGGNSnCUCY4xJBC/+CN78uy+r9jURiMh5IrJJRLaIyE09zB8hIr8XkWoR+ad3/3ljjDGR6rfCiz+Et2I1RlFXviUCb8zeO4DzcaPuLBCR6d0W+xZuYJJZuAFgehsX1xgzVPZugWe/DR19Dr5lhtKKeyAtAyqu9mX1fpYI5gFbVHWbN8LSUtyIOpGmA88DqOpGoFxExvgYkzGmP6segld+DnvWBx2JAWhrhpUPwdQLoXAgQ2xHz89EUEbXYRF3etMirQYuAxCRebiBnAc15qYxJkZqq93/t18LNg7jbHgSmvZB5bW+bcLPRCA9TOs++MGPgBEisgo3zuxK3GDrXVckcp2IVIlIVV1dXcwDNcZEqF3j/u+yRBAXqpbAyBOg/L2+bcLPu4/uBMZHPB+HG/S6k6oeBD4FICKCGzD6je4rUtW7gbsBKisrbSQdY/zSsBsad7vHViII3u51sP1V+OAPIM2/83Y/SwTLgSkiMklEsoCrgCcjFxCR4d48gM8AL3vJwRgThHC10PjTXBtBW3Ow8aS6qnsgPRvmfNzXzfiWCFQ1BHwBeAbYADyiqutEZJGILPIWmwasE5GNuN5FX/YrHmNMFMKJoOIa6AgdqSYyQ6+lEVYvhRmXQt5IXzfl68A0qroMWNZt2l0Rj18FpvgZgzFmAGqqYfhEOH6+e75rJYw/NdCQUtbax6C1wddG4jC7stgYc0RtNYydCUWlkD/aGoyDogpVi2H0DBg/z/fNWSIwxjgtDbBvGxw3G0SgrMIajIOy6zWoWQ2nXus+C59ZIjDGOLVr3f+xs9z/0grY+7pLEGZoVS2BzHyY+dEh2ZwlAmOME24oHjvT/S+rABR2rQoqotTU9A6seQxmXQE5RUOySUsExhinthryil37ALgSAVg7wVBb/TCEmoakkTjMEoExxqmpdtVC4Trp/GIYPsH1HDJDQ9VVC5VVuraaIWKJwBgDoVbYs+FItVBY6VxrMB5Kb70CezcNaWkALBEYY8AdfDrajj4LLa2A/W/Bofpg4ko1VYshZ5i7iGwIWSIwxrhqITjSYyisLNxOYNVDvmusg/VPuttJZOUN6aYtERhjXENxZh4Un9B1+nFzALEG46Gw6kFXKjvlU0O+aUsExhh3T6ExMyAtvev0nCIYNcXaCfzW0eFuMFd+JpScOOSbt0RgTKrr6HCJoHu1UFhphVUN+W3rX1xbTOXQlwbAEoExZv+b0HLw6B5DYaVzobEWDu7qeb45dlVLIL8Epl4cyOYtERiT6sK3mj6ulxJBuMHYqof8ceBteP1pmHs1ZGT1v7wPLBEYk+pqqkHSYfT0nuePnQlpGdZg7JfX7nMXkp1yTWAhWCIwJtXVVsOoEyEzt+f5mbkwepqVCPzQ3gYr7oPJ58CI8sDC8DURiMh5IrJJRLaIyE09zB8mIk+JyGoRWSciwbSUGJPKatf0Xi0UFm4wVhsyPKZe/5Nrfzn104GG4VsiEJF04A7cEJTTgQUi0r3s+S/AelWdDcwH/jNiDGNjjN8a66ChpvceQ2FlFdC8H955Y0jCShlVS6BoHEz5YKBh+FkimAdsUdVtqtoKLAUu6baMAoUiIkABsA8I+RiTMSZS7Wr3v7ceQ2Glc91/qx6KnfqtrtvoKdccff3GEPMzEZQBOyKe7/SmRfoFbgD7XcAa4Muq2uFjTMaYSOEeQ/0lgtHTISPHrieIpRX3ukb6ik8GHYmviaCn8dW6VzCeC6wCSoE5wC9E5KiRGETkOhGpEpGqurq6WMdpTOqqqYZhEyBvZN/LpWe6ZGElgtgItcDKB2HqhVA4NuhofE0EO4HxEc/H4c78I30KeFydLcAbwNTuK1LVu1W1UlUrS0pKfAvYmJQTHqw+GqUVbhzdjnZ/Y0oF65+Epn1Dfrvp3viZCJYDU0RkktcAfBXwZLdltgNnA4jIGOAkYJuPMRljwloaXT11fz2GwsoqoO0Q1G3yN65UULUYRh4Pk84KOhLAx0SgqiHgC8AzwAbgEVVdJyKLRGSRt9gtwOkisgZ4Hvimqu71KyZjTITd6wDtv8dQmA1dGRu718P2V11pIC0+LuXK8HPlqroMWNZt2l0Rj3cBwfabMiZVdR+svj/FkyGr0DUYz/2Ef3EluxX3QHo2zP5Y0JF0io90ZIwZerXVkDsCho2Lbvm0NCidYw3Gx6L1EKxeCjM+7MaEjhOWCIxJVd0Hq49G6VzYvdaNcWwGbs2j7k6vcdJIHGaJwJhU1N4Ge9ZHXy0UVlYB7a0uGZiBq1oCo2fA+NOCjqQLSwTGpKK9r7sDevfB6vtjDcaD9/ZrULPKDT4zkFLYELBEYEwq6m2w+v4MnwB5xfC2XWE8YFWLITMfZl0ZdCRHsURgTCqqrXa3jCiePLDXidjQlYPRtB/WPAYzL3fjQMcZSwTGpKLwYPXpg+hBXjoX6ja4HjAmOtUPQ6gp8NtN98YSgTGpRtW7tcQAq4XCyipAO45UL5m+qbpG4rJTBt4mM0QsERiTava/Bc0HBt5jKMwajAfmrVegbmPcdRmNZInAmFTTOVj9IM9OC8dAUZldWBatqiWQMwxmXBZ0JL2yRGBMqqmpBknrfbD6aJTOtRJBNBrrYP0f3O0ksvKCjqZXlgiMSTW1a6B4yrEdmMoqYN82aHondnElo1UPQUebu3YgjlkiMCbV1FZHf+vp3oSHrty16pjDSVodHe4GcxPPgJKTgo6mT5YIjEklh+rh4NuD7zEU1pkIrHqoV9v+Au+8CafGbyNxmCUCY1LJQG893ZvcEW5gFWsw7l3VPZA3CqZeHHQk/bJEYEwqCSeCWPRntyuMe3fgbdj0NFRcDRlZQUfTL18TgYicJyKbRGSLiNzUw/xviMgq72+tiLSLSD+jaBtjBq2mGorG9T9YfTTKKlw1U8PuY19XsnntfnfR3SkLg44kKr4lAhFJB+4AzgemAwtEpEt/NVW9TVXnqOoc4F+Bl1R1n18xGZPyatcce7VQmF1Y1rP2ELx2H0w+B0aUBx1NVPwsEcwDtqjqNlVtBZYCl/Sx/ALgtz7GY0xqaz0M9ZuPvcdQ2HGz3PUIVj3U1et/goaauL6SuDs/E0EZsCPi+U5v2lFEJA84D3jMx3iMSW2717nqimPtMRSWlQ8lU63BuLuqxe7K6ymJMxy7n4mgp5EXtJdlLwb+3lu1kIhcJyJVIlJVV1cXswCNSSmx6jEUqbTCVQ1pbz/tFLNvG2z9C1RcM7g7uwbEz0SwExgf8XwcsKuXZa+ij2ohVb1bVStVtbKkpCSGIRqTQmqrIWe4G1wmVsrmwuF62L89dutMZCvuBUmHik8GHcmA+JkIlgNTRGSSiGThDvZPdl9IRIYBZwF/8DEWY0xNtSsNxHKYRGswPiLUAisfhKkXQNFxQUczIL4lAlUNAV8AngE2AI+o6joRWSQiiyIWvRR4VlVtlAtj/NIe8garj1H7QNiYkyE9y9oJANY/6UpHCdRIHOZrJZaqLgOWdZt2V7fn9wL3+hmHMSmvfjOEmmPXYygsI8slA+s55G43PfJ4mDQ/6EgGzK4sNiYV1PjQUBxWOhdqVrubrKWqPRtg+ytwyqcgLfEOq4kXsTFm4GqrIT0bRp0Y+3WXVUDLQajfEvt1J4qqe1wV2ZyPBx3JoFgiMCYV1FbDmOmQnhn7dad6g3HrIVj9W5j+YcgvDjqaQbFEYEyyUz3SY8gPJSdBZn7qNhivfcyViE79dNCRDJolAmOS3YGd0Lw/9j2GwtLS3d1MU7VEULXEDfs5/rSgIxk0SwTGJLtY3nq6N2UV7oZ27W3+bSMevf2a6zFVeW1sr88YYpYIjEl2NdWAHNtg9f0pneu6p+7Z4N824lHVEsjMg1kfDTqSY2KJwJhkV7sGiidDdoF/20jFoSub9rv2gZlXQM6woKM5JpYIjEl2sRisvj8jj3f3MUqlBuPqR6DtMFR+KuhIjpklAmOS2eF9cGCHfz2GwkRcqSBVSgSq7nbTpRVHSkMJzBKBMcmsdo3771ePoUhlFbB7PbQ1+b+toG1/Feo2JnSX0UiWCIxJZp1jEAxBIiitAG0/knySWdUSyB4GMy4LOpKYsERgTDKrqYbC46BgCMbx6GwwTvIb0B3aC+v/AHMWQFZe0NHEhCUCY5JZ7ZqhKQ0AFJVCwZjkbzBe+SC0t7obzCUJSwTGJKu2Jtj7uv89hsJEjgxdmaw6OmDFPTDxDBg9NehoYsYSgTHJavd6V2fvd4+hSGUVsHczNB8cum0OpW0vwDtvJkWX0Ui+JgIROU9ENonIFhG5qZdl5ovIKhFZJyIv+RmPMSllKBuKw0orAIWaVUO3zaFUtQTyRsG0i4OOJKZ8SwQikg7cAZwPTAcWiMj0bssMB34JfEhVZwBX+BWPMSmnthqyi2BE+dBtM9xgnIztBAd3waanYe4nICM76Ghiys8SwTxgi6puU9VWYClwSbdlPgY8rqrbAVR1j4/xGJNa/Bisvj/5xTB8QnL2HHrtftAOOGVh0JHEnJ+JoAzYEfF8pzct0onACBF5UURWiMgnfYzHmNTR0Q671w1ttVBYMjYYt4dgxX0w+WwYOSnoaGKu30QgIheJyGASRk+nIdrteQZwCnAhcC7wHRE5aiw9EblORKpEpKqurm4QoRiTYuq3QKhp6HoMRSqrgP3bXX/7ZPH6n6Bhl7vddBKK5gB/FbBZRP5DRKYNYN07gfERz8cBu3pY5k+qekhV9wIvA0fdNF1V71bVSlWtLCkZggtjjEl0fg5W35/OoSuTqHqoagkUlcGUc4OOxBf9JgJV/QQwF9gK3CMir3pn6IX9vHQ5MEVEJolIFi6hPNltmT8AZ4pIhojkAacBKXZDc2N8UFvtBlMvCaCve+kcQJKnwXjfG7D1eai4BtIzgo7GF1FV+ajqQeAxXIPvccClwGsi8sU+XhMCvgA8gzu4P6Kq60RkkYgs8pbZAPwJqAb+CfxaVdcew/sxxoBLBKOn+TNYfX+yC2HUicnTTrDiXpB0qLg66Eh80296E5GLgWuBE4AHgHmqusc7g98A/Ly316rqMmBZt2l3dXt+G3DbwEM3xvQoPFj91AuCi6GsArb+xcWSwEM4EmqBlQ/ASee7W2gkqWhKBFcA/6Wqs1T1tnAXT1U9jEsQxph4cnAXNO2DsT6OUdyf0rnQuNvFksjWPwmH65PmdtO9iSYRfA9XbQOAiOSKSDmAqj7vU1zGmMHqHKw+gB5DYZ0NxglePVS12I2+Nml+0JH4KppE8DugI+J5uzfNGBOPatcAAmNmBBfD2JmQlpHYDca717sBaE75FKQl923Zonl3Gd6VwQB4j7P8C8kYc0xqVruz2Oz+Ovb5KDMHRk9P7BJB1RJIz3a3lEhy0SSCOhH5UPiJiFwCJNGVIsYkmaEYrD4aZRXuWgLtfh1pAmhphNVLYcalkDcy6Gh8F02n2EXAQyLyC9zVwjsAuxWEMTHQ3qFs3tPA6h37WbVjP5tqG0hPE3Iy08nJTCc3/Jd15HlOZlqX57nhZbPSyOtoZNr+7Ryc8Qk6DreSk5lOdkYaEkTPndIK1/Vy3zYoPmHot38s1vwOWhuSvpE4rN9EoKpbgXeJSAEgqtrgf1jGJKfdB5tZud0d9FfteIc1Ow9wqLUdgKKcDKaXFiEIjS0h6hpaaG5rp6mtnabWdppDHbSGOvpc/7vS1rM0C774QjsvPf9nwPXezMlwySQ3M53szLTeE0xmOjlZ6RTlZHJCST6TRxcysTiPzPRB1JFHDl2ZSIlA1TUSjzkZxp0adDRDIqrL5ETkQmAGkBM+s1DVf/cxrqTQ1NrO9n2H2b7vMG/VH/L+H2Z/UxsF2e7HVpiTQWG3/0U9TCvMySA7Iz3ot2QG4FBLiDVvH3AH/e37Wb1zPzUHmgHISBOmlxbxkVPGMWf8cGaPH86k4nzS0vo+c2/v0C7JoSXUTlNrh3ve1s7otWugGj58/nnMTxtBU1s7zV4SaWpt71zOTWvnUEuIvY2tbp3eNLfeIwknM12YNCqfKaMLmTy6gBPHFDJlTAHlxflkZfSRIEZPg4wc12A88/KY7NMh8fYK1+B+4U8S+xqIAYjmgrK7gDzgfcCvgcuJ6E6aylSV+kOt7mBf7w7yb+07xA7vgL+noaXL8oXZGUwozmNkfhaHWkLsOdhCQ3OIhua2zrPCvmRnpFGYk+kliq5JordkUtRtmT5/uGbQulfxrNy+n9d3N9DhVY9PGJlHZflI5owfzpzxw5lRWkRO5sATe3qakJ+dQX52Lz/ddW9CwRguPbNi8G8GONwaYuueQ2ze08DruxvZsqeBtbsOsGxtTWeVf3qaUF6cx5TRLjGEk8SkUfnuvaVnurufJlqD8fLFkFVAaMbl7D3QzJ6GZkId2vmbKsjOIC8rPZjqNp9EUyI4XVVniUi1qn5fRP4TeNzvwOJFqL2DXfubeWvfId6qP9x5kH9rn3vc2BLqsvzYohwmFOdx1oklTCzOY/zIPCYW5zNxZB7D8zJ7/fK0dyiNzSEONrd1JoeG5hANLW0cbDry/GDkvOY2dh9sPqZkUpCTQUF2BgXZLmEUeAeZgpwMCrO9ed708PwCK530W8Uze/xwPjh9DHMmDGf2uOEUFwzRQCY11TG59XReVgYzxw1j5rhhXaY3t7Wzta6RLXsa2by7kdd3N/D67gaeXV/bmfTSBCYW5zN5dAGfC5Uzp+4pNu6o54Qxw8nNCv5709zWTl1DC3samtlzsIU9EY8P7a/jv95+lCflfdx4y996bedOE7zfxJETrfDzAu95kZc0Iud1XzZeTsyiSQTN3v/DIlIK1ANJdUPuQy0h3qo/7FXjHIp4fJi332ki1HHk25CVnsb4kblMGJnHaZNGMmFkHhOL3d+4EXmDOssDd3Y1LC+TYXmDvzdMZDI5klC6Jo7uyeRQS4j6xsM0NIdobHF/7R399/LISk8jPzvdSxKZLmlEJIrIJJKfndFlfqH3moKcDPIy0/utDglaX1U8menCtOOOVPHMGT+c8iiqeHzR1gx1G+Gk83zbRE5mOjNKhzGjtGuCaAm188beQ2ze3cjmPa4EsXl3I7/dN4rKjCa+9stHeJ0JjB+Rx5TRBUweU+BKEqNdSaLXEk6UVJXGlpA7qB90B3Z3sG9hz8Fm72DvHh9sDh31+vQ0YVRBFp/NeJpsWtk1+Sq+OHoKowuzGV2YTWZ6Ggeb22hsOfKbagz/vlrc87rGFrbtPdQ5vbW97/YcCJ+YHSlp9JU0CnMyOGlsISeOiX234Gj2/lPekJK3Aa/hxhT4Vcwj8dnB5jZer23ocjYfrrff29jaZdlhuZlMLM5jZtkwLpp1HBNG5jFhZD4Ti/MYW5QTtweuWCQTVaW5rYOGFvdFb2wJuS92i0sa4R9CePqR523saWhmW92RaS39NGyCq4LNzUwnL8s1XOZlZnQ2anZOywo3bLoieZ7XwBl+nJuVcdTyeZkZ5GSlkZU+sB4zQ1XF44u6DUM/WL0nOyOdqWOLmDq2qMv01t1j4c5fctvp7byQeyKb9zSwZU8jf928t8uBsmx4LlPGFDBltEsQk72qpsLsDPYfbuty1t75uKGFuoMt7PamN7UdXSLOykjrPJhPLing9BOKvec5lBRldz4emZ9FOgq/uBGKT+PLH7/smPdJc1v7kd9H+OSr83lb5+8ofGIWXtadmLllG1tCXUol188/gW+eF/s7yvaZCLwBaZ5X1f3AYyLyRyBHVQ/EPBKfvbBxD19eugpwB5/SYbmMH5nL2VPHMME7o584Mp8JI/OO6UCa6ETEHYiz0hl9jCcebe0dHIpMHBFJpdFLHg3NIQ63tnO4tZ2m1hBNbeHH7dQcaKPZe37Ym9fWPrA+6elpQp7XOyYyweRlZXRJJpnpaWze0xA/VTyD0TkGQRxcQ+DJKpkC2UXMkm3MOmdK5/RQewdv7TvMZq/9YbNX1fTq1vqjGqp7+szzs9IZXZRDSWE2M8uGMXpqDqMjDuyji7IZU5hDUW5G9CcC216CfVvhrG8e8/sGOrsAjzqG70xHh3K4rb2zBFKY48+xqc9EoKodXpvAu73nLUBLX6+JV+8+oZh7Fp7KhOI8xo3ITfk67qGQmZ7G8LwshufF7kL0tvaOzh4z4YTR1BaZTNojkombHrn84dZ2L7mE2NvYcqRrZls75aPyB9yLJ67UVkNWIYyIo5rbtDQ4bvZRDcYZ6WmcUFLACSUFwNjO6e0dys53XIJ4fU8DB5tClBRmd57Vjy7KYXRh9jFXJfVo+WLIHQnTuw+tHpy0NPHa8DJgWP/LD1Y0e/NZEfkIbpD5BLxE0BldmMPoqTlBh2GOUWZ6GpnpaRT5dGaU0GrXwNiT4+++OGUV8Oov3S2dM/o+O05PE9e5ojifc6aPGaIAgYM1sPF/4d3/4m6PkWKi+cZ8FXeTuRYROSgiDSJy0Oe4jDED0dEOtWvjqlqoU2kFdLTB7jgec+q1+137SuWngo4kENEMVVmoqmmqmqWqRd7zov5eByAi54nIJhHZIiI39TB/vogcEJFV3t93B/MmjEl5+7ZB26H4uMdQd2XeNQ3xeifS9pC7FcYJ73c360tB0VxQ9t6epqvqy/28Lh24A/gAbpD65SLypKqu77boX1X1oijjNcb0JDwGQRCD1fdn2HjIGxW/g9lvfgYadsEFqTtQYjRtBN+IeJwDzANWAO/v53XzgC2qug1ARJYClwDdE4Ex5ljVVENaJpRMCzqSo4m4+w7FayJYvhiKyuBE/66/iHfRVA1dHPH3AeBkYHcU6y7D3ak0bKc3rbt3i8hqEXlaRAIcScOYBFZbDaOnQkacDhVSVuEudms9FHQkXe3bBlufh4prIN2HnkgJYjDdC3bikkF/eup3173X0WvARFWdDfwceKLHFYlcJyJVIlJVV1c3kFiNSX7hwerjsaE4rLQCtMMNmhNPqu4BSYeKq4OOJFDRtBH8nCMH8DRgDhDNp7kTGB/xfBzQZSRrVT0Y8XiZiPxSREap6t5uy90N3A1QWVmZsF1YjfFFQy0c3hvfiSCywXji6cHGEtbWDCsfhKkXQFFp0NEEKpqyUFXE4xDwW1X9exSvWw5MEZFJwNvAVcDHIhcQkbHAblVVEZmHSzT1UUVujHHiYbD6/hSMhqJx8XUn0g1PQtM+qEyNwWf6Ek0ieBRoVtV2cL2BRCRPVQ/39SJVDYnIF4BngHRgiaquE5FF3vy7cLe0vl5EQkATcFUiX7RmTCDCiWBMNDW2ASqbG19dSJcvhpEnwKSzgo4kcNEkgueBc4BG73ku8CzQb/lOVZcBy7pNuyvi8S+AX0QbrDGmBzXV7rYSOVFd3hOc0grY8BQc3hf8OMC718GOf8AHb42/K7EDEM0eyFHVcBLAe5znX0jGmAGJl8Hq+xMeurJmVaBhAK40kJ4Ncz7W/7IpIJpEcEhEOoc7EpFTcNU4xpigNR+Ad96MzwvJugsngqCrh1oaoPphOPmy4EsmcSKaqqEbgN+JSLjHz3HAlb5FZIyJXq13/56xs4ONIxq5w12dfNAXlq35HbQ2WiNxhH4TgaouF5GpwEm4awM2qmqb75EZY/qXCD2GIpVVwJvRdDr0iSosX+JKUOMqg4sjzvRbNSQi/wLkq+paVV0DFIjI5/0PzRjTr9o1kF8CBUN4y+ZjUVrh7uvTUBvM9ncuh91rXGkgiQafP1bRtBF81huhDABVfQf4rG8RGWOiF76iOFEOakHfiXT5Yjd4z8wrgtl+nIomEaRJxFhv3l1F4/SGJsakkFCLG6c4ERqKw8bOBEkLpp3g8D5Y93uYfSVkFwz99uNYNI3FzwCPiMhduFtNLAKe9jUqY0z/6jZCRyhx2gcAsvLdHVKDuMJ41UPQ3mKNxD2IJhF8E7gOuB7XWLwS13PIGBOkzsHqE6DHUKSyubBxmWu4HaoqrY4OqFoCE94NY6YPzTYTSDS3oe4A/gFsAyqBs4ENPsdljOlPbTVk5ifeqFqlFe4eP/vfGrptvvGiu+V05bVDt80E0muJQEROxN0obgHuRnAPA6jq+4YmNGNMn+J1sPr+RDYYjygfmm0uXwx5xTD9kqHZXoLp6xu0EXf2f7GqnqGqPwfahyYsY0yfOjq8RJBA7QNho2dAetbQtRMc3AWbnoa5n4CM7KHZZoLpKxF8BKgFXhCRX4nI2fQ82IwxZqi984a7OjaRegyFZWS5O6XuWjU023vtfjcozimfGprtJaBeE4Gq/l5VrwSmAi8CXwHGiMidIvLBIYrPGNOTRLuiuLuyCpcIOjr83U57CFbcB5PPhpGT/N1WAoumsfiQqj6kqhfhRhlbBdzkd2DGmD7UVENaRnwOVh+N0gpobYD6zf5u5/Wn3ZXM1kjcpwG1MqnqPlX9H1V9v18BGWOiUFsNo06CzJygIxmcobrCePliKCqDKef6u50E52t3AxE5T0Q2icgWEem1FCEip4pIu4hc7mc8xiSN2jWJWy0EMOpE1/XVzwbj+q2w7QU4ZSGkR3PJVOryLRF4t6K4AzgfmA4sEJGjruTwlvsx7gpmY0x/GnZD4+7E7DEUlpYOpXP8LRGsuAckHSo+6d82koSfJYJ5wBZV3aaqrcBSoKdOvF8EHgP2+BiLMckj3FCciD2GIpXOdSWbdh/uat/WDCsfgqkXQuHY2K8/yfiZCMqAHRHPd3rTOolIGXApcBfGmOgkUyJob4E962O/7vVPuKuXT7X7CkXDz0TQ0zUH2u35T4FvqmqfF6qJyHUiUiUiVXV1dbGKz5jEVFMNwye6Eb8SmZ8NxssXQ/FkmHRW7NedhPxMBDuB8RHPxwG7ui1TCSwVkTeBy4FfisiHu69IVe9W1UpVrSwpKfEpXGMSRO2axC8NAIyYBLkjYt9gXLsGdv7TdRlNlHEaAuZnIlgOTBGRSSKShbtv0ZORC6jqJFUtV9Vy4FHg86r6hI8xGZPYWhpg31Y4LsHuONoTEVc99HaMxyaoWgIZOTB7QWzXm8R8SwSqGgK+gOsNtAF4RFXXicgiEVnk13aNSWqdg9UncI+hSKUVro2g9XBs1tfSANWPwIzLIG9kbNaZAnztXKuqy4Bl3ab12DCsqgv9jMWYpFC7xv1PhqohcO0E2u7e14TTjn191Q+7ezBZI/GAJNj9a41JcbWr3e2Ui0qDjiQ2Sue6/7EYulIVli9xpaWyU459fSnEEoExiSTRBqvvT1EpFIyNTYPxjn/CnnWuNJAs+2eIWCIwJlGEWt04xclSLRRWVhGbLqRViyG7CE62O9UMlCUCYxLF3k3Q3pocPYYilVa4u5A2Hxj8Og7Vw7rfw6wrIbsgdrGlCEsExiSKzsHqk6THUFhZuJ1g1eDXsepBlyStkXhQLBEYkyhq10BmHhSfEHQksVXqXWE82HaCjg6ougcmnA6jE3R8hoBZIjAmUdRWw5gZ7s6dySRvpLtlxmB7Dm17wQ3daaWBQbNEYEwiSOTB6qNRVjH4K4yrlkDeKJh2cWxjSiGWCIxJBPvfgpaDyddjKKy0Ag5sh0N7B/a6A2/DpmUw9xOQke1PbCnAEoExiSDRB6vvz2DvRPrafe5CsspPxT6mFGKJwJhEUFPtRtsafdQgf8nhuNmADKzBuL0NVtwHk8+BEeV+RZYSLBEYkwhq13jj/OYGHYk/sguh5KSBlQg2PQ2NtdZIHAOWCIxJBLXVyVstFFY61/Uc0u7jV/WiajEUjYMpH/Q3rhRgicCYeNdYBw01ydtjKKy0Ag7tgYNv979s/VbY9iKcsjD5utMGwBKBMfEuWcYo7s9AGoyrlkBaBlR80t+YUoQlAmPiXaokgjEnu4N7fw3GbU2w6iGYehEUjhma2JKcr4lARM4TkU0iskVEbuph/iUiUi0iq7zB6c/wMx5jElJNNQybkPwjbmXmuCun+ysRrHsCmt6xRuIY8i0RiEg6cAdwPjAdWCAi3fu+PQ/MVtU5wLXAr/2Kx5iElSyD1UejtMLdfK6jo/dlqhZD8RQoP3PIwkp2fpYI5gFbVHWbqrYCS4FLIhdQ1UbVzi4C+UCU3QWMSREtjVC/Jfl7DIWVzoWWA7BvW8/za6ph53KovNYGn4khPxNBGbAj4vlOb1oXInKpiGwE/hdXKjDGhO1eB2jy9xgKCzcY93YDuqrFkJELcxYMXUwpwM9E0FO6PuqMX1V/r6pTgQ8Dt/S4IpHrvDaEqrq6uthGaUw8S5WG4rCSae5A31ODcfNBqP4dnPwRyB0x9LElMT8TwU5gfMTzccCu3hZW1ZeBE0RkVA/z7lbVSlWtLCkpiX2kxsSr2mp30Bs2LuhIhkZ6hqsG66nBuPphaDvkqoVMTPmZCJYDU0RkkohkAVcBT0YuICKTRVxFn4hUAFlAvY8xGZNYkm2w+miUVkDNamgPHZmm6q4dOG72keojEzO+JQJVDQFfAJ4BNgCPqOo6EVkkIou8xT4CrBWRVbgeRldGNB4bk9ra22DPhtSpFgorq4BQE9RtPDJt+z9gz3qo/HRqJcUhkuHnylV1GbCs27S7Ih7/GPixnzEYk7D2vg7tLck3WH1/IoeuHHuye1y1GLKHwczLg4sridmVxcbEq2QdrL4/I4+H7KIjPYcO7YX1f4DZV0FWfrCxJSlLBMbEq9o1kJEDxZODjmRopaVB6ZwjDcYrH4T2Vht8xkeWCIyJV+HB6tN9rcGNT6UV7hqKtiZYcQ9MfA+MnhZ0VEnLEoEx8UjVJYJUqxYKK6uAjjZ45efwzpvWZdRnlgiMiUf7t0PzgdTrMRQWbjB++XbIL4FpHwo2niRnicCYeNQ5WH2K9RgKGzbOJYD2Fph7NWRkBR1RUrNEYEw8qqkGSUvewer7I+JuQIe4UciMr1KwFcqYBFC7xt1qOSsv6EiCc8ZX4cRzYcTEoCNJepYIjIlHtdUw8fSgowjWxHe7P+M7qxoyJt4cqncDuKdqQ7EZcpYIjIk3tSl6RbEJjFUNGRNvUigRtLW1sXPnTpqbm4MOJWnk5OQwbtw4MjMzo36NJQJj4k1NNRSVQX5x0JH4bufOnRQWFlJeXo7YXUWPmapSX1/Pzp07mTRpUtSvs6ohY+JN7ZqUKA0ANDc3U1xcbEkgRkSE4uLiAZewLBEYE09aD0P95tQZrB4sCcTYYPanJQJj4sme9aAd1mPIDClfE4GInCcim0Rki4jc1MP8j4tItff3ioik6PX0xnhqVrv/KVI1FLT6+nrmzJnDnDlzGDt2LGVlZZ3PW1tb+3xtVVUVX/rSl4YoUn/51lgsIum44Sc/gBvIfrmIPKmq6yMWewM4S1XfEZHzgbuB0/yKyZi4V1sNOcNg+ISgI0kJxcXFrFq1CoCbb76ZgoICvv71r3fOD4VCZGT0fJisrKyksrJyKML0nZ+9huYBW1R1G4CILAUuAToTgaq+ErH8P4BxPsZjTPwLNxSnYL35959ax/pdB2O6zumlRXzv4hkDes3ChQsZOXIkK1eupKKigiuvvJIbbriBpqYmcnNzueeeezjppJN48cUXuf322/njH//IzTffzPbt29m2bRvbt2/nhhtuSKjSgp+JoAzYEfF8J32f7X8aeNrHeIyJb+0hNxhL5aeDjiTlvf766zz33HOkp6dz8OBBXn75ZTIyMnjuuef41re+xWOPPXbUazZu3MgLL7xAQ0MDJ510Etdff/2A+vIHyc9E0NMpjfa4oMj7cIngjF7mXwdcBzBhghWZTRLqaIeV90OoOaV6DEUa6Jm7n6644grS09MBOHDgANdccw2bN29GRGhra+vxNRdeeCHZ2dlkZ2czevRodu/ezbhxiVHJ4Wdj8U5gfMTzccCu7guJyCzg18Alqlrf04pU9W5VrVTVypKSEl+CNSYQHe1Q/QjccRr88SswZiZM+WDQUaW8/Pz8zsff+c53eN/73sfatWt56qmneu2jn52d3fk4PT2dUCjke5yx4meJYDkwRUQmAW8DVwEfi1xARCYAjwNXq+rrPsZiTHzpaIe1j8FL/+GuGxg9Az56P0y92A3ebuLGgQMHKCsrA+Dee+8NNhif+JYIVDUkIl8AngHSgSWquk5EFnnz7wK+CxQDv/QuggipanI0wxvTk84E8GOo3wJjToaPPgBTL7IEEKduvPFGrrnmGn7yk5/w/ve/P+hwfCGqPVbbx63KykqtqqoKOgxjBqY95BLAy/9xJAGc9c2UTwAbNmxg2rRpQYeRdHraryKyorcTbbvpXKpRhZ1VsOohaD0EJ50PUz4A2YVBR5acekoAVgIwccYSQapo3AOrl8LKB2HvJsjMg8xcWPMIpGfD8fNh2kVw0gWQPyroaBNfewjWPuraAPZtdQngygfhpAstAZi4Y4kgmbW3weZn3cH/9WdA22HcPLj4ZzDjUsjKhx3/Bxv+CBufgs3PgHwZJrzbnbFOu8iucB2ooxLATEsAJu5ZIkhGezbCqgddCeBQHeSPhtO/AHM+ASUndl124unu79xb3VWtG//oEsMz/+r+xs6CaRe7xDB6Wkpe8RqV9hCs+Z2rAtq3zUsAD7kSliUAE+csESSL5gOw9nF39v92FaRlwInnwdxPwORzIL2fKxxF3IVMx82C930L6rfCxv91ieGF/wcv3Aojj/dKChdDWaUd4ODoBDDWEoBJPJYIEllHB7z1d3fwX/8HCDVByVT44K0w60ooOIaL74pPgPd8yf017IZN/+tKCv+4E175GRSMhakXuMRQfiZkZMXufSWCnhLAVb9xCcBKTSbB2ClLIjqwE166DX4+F+67CDYtg9lXwWf+Ap//h6sGOpYk0F3hGKi8Fq5+HL6xBS77NUw4DVY/DA9eBrdNhsc+65JR66HYbTcetYdg1W/gjlPhiUWuneWq38Dn/gpTL7QkkGDmz5/PM88802XaT3/6Uz7/+c/3uny4+/oFF1zA/v37j1rm5ptv5vbbb+9zu0888QTr1x+5EfN3v/tdnnvuuQFGHztWIkgUbc3urHzlg7D1BUBh0nvhff/mzsqz8oYmjtzhMOsK99fWBNtedCWFTctcD6SMHDjh/S6mk86HvJFDE5ff2kPu/b30H/DOG67t5KrfuvdoB/+EtWDBApYuXcq5557bOW3p0qXcdttt/b522bJlg97uE088wUUXXcT06dMB+Pd///dBrysWLBHEu5rV7uBf/Qg074dh4+GsG2HOx2BEebCxZea6A+FJ57sD5fZXjzQ2b1oGku4aoqdd7M6WhyXGDbi6aA9B9cPw8m2WAPz29E2uw0IsjZ0J5/+o19mXX3453/72t2lpaSE7O5s333yTXbt28Zvf/IavfOUrNDU1cfnll/P973//qNeWl5dTVVXFqFGjuPXWW7n//vsZP348JSUlnHLKKQD86le/4u6776a1tZXJkyfzwAMPsGrVKp588kleeuklfvCDH/DYY49xyy23cNFFF3H55Zfz/PPP8/Wvf51QKMSpp57KnXfeSXZ2NuXl5VxzzTU89dRTtLW18bvf/Y6pU6fGZDdZ1VA8OrwP/nEX3HkG/M97YcV9rsH36ifgy9WuMTfoJNBdegZMOhPO/zF8ZS1c9yKc8RXXa+npG+G/ZsDd8+Hl26FuU9DR9q89BCsfgl9Uwh8+7y64u+q38LmXXduIJYGkUFxczLx58/jTn/4EuNLAlVdeya233kpVVRXV1dW89NJLVFdX97qOFStWsHTpUlauXMnjjz/O8uXLO+dddtllLF++nNWrVzNt2jQWL17M6aefzoc+9CFuu+02Vq1axQknnNC5fHNzMwsXLuThhx9mzZo1hEIh7rzzzs75o0aN4rXXXuP666/vt/ppIKxEEC862l2Vz8oH3Nl0eyscNwcuuB1mXg65I4KOMHoiUDrX/Z39Hdi7xV2nsOGP8Jdb3F/xFHedwtSLYcx0d1FbPPSyaW+LKAG8CcfNhgVLXQ8sO/j7q48zdz+Fq4cuueQSli5dypIlS3jkkUe4++67CYVC1NTUsH79embN6vn24H/961+59NJLyctz1bMf+tCHOuetXbuWb3/72+zfv5/GxsYuVVA92bRpE5MmTeLEE10372uuuYY77riDG264AXCJBeCUU07h8ccfP9a33skSQdDqt7rGx1W/gYZdkDsSTv0MzPk4jD056OhiY9RkVzo44ytwcNeRbqmv/Bz+9l9HlkvPhswcyMjt9t/7y8zt4X921+V6ndfLOsMHd0sAKevDH/4wX/3qV3nttddoampixIgR3H777SxfvpwRI0awcOHCXm89HSa9fEcWLlzIE088wezZs7n33nt58cUX+1xPf/d+C9/qOta3ubZEEITWQ66HzcoHXfdPSXNVP+f/CE48P7m7YhaVwrzPur+md2Dzn10vqFCz+2trdt1gu/xvhpYGOLS3h3lNoB2DjyecEFSh5YArhS14GE481xJAiigoKGD+/Plce+21LFiwgIMHD5Kfn8+wYcPYvXs3Tz/9NPPnz+/19e9973tZuHAhN910E6FQiKeeeorPfe5zADQ0NHDcccfR1tbGQw891Hk768LCQhoaGo5a19SpU3nzzTfZsmVLZ5vCWWed5cv7jpQ6iWDLc/DMvwUdhXNgJ7Q2wsgT4Ozvua6fRaVBRzX0ckfArI8e2zpU3dl8qAlCLa4nU6i5l/8tPSeStmboaHNn/1YCSEkLFizgsssuY+nSpUydOpW5c+cyY8YMjj/+eN7znvf0+drwuMZz5sxh4sSJnHnmmZ3zbrnlFk477TQmTpzIzJkzOw/+V111FZ/97Gf52c9+xqOPPtq5fE5ODvfccw9XXHFFZ2PxokWL/HnTEVLnNtQ7/gmv/iL2AQ1GXjHM/ChMeJcddExKs9tQ+8NuQ92b8fNg/P1BR2GMMXHH124aInKeiGwSkS0iclMP86eKyKsi0iIiX/czFmOMMT3zrUQgIunAHcAHcAPZLxeRJ1V1fcRi+4AvAR/2Kw5jTHxT1V573ZiBG0x1v58lgnnAFlXdpqqtwFLgksgFVHWPqi4H2nyMwxgTp3Jycqivrx/UwcscTVWpr68nJydnQK/zs42gDNgR8XwncJqP2zPGJJhx48axc+dO6urqgg4laeTk5DBu3MBu5+JnIuiprDeotC8i1wHXAUyYYCNmGZMsMjMzmTRpUtBhpDw/q4Z2AuMjno8Ddg1mRap6t6pWqmplSUkMb69sjDHG10SwHJgiIpNEJAu4CnjSx+0ZY4wZBN+qhlQ1JCJfAJ4B0oElqrpORBZ58+8SkbFAFVAEdIjIDcB0VT3oV1zGGGO6Srgri0WkDnhrkC8fBeyNYTiJzvZHV7Y/jrB90VUy7I+Jqtpj3XrCJYJjISJVvV1inYpsf3Rl++MI2xddJfv+iIMbwBtjjAmSJQJjjElxqZYI7g46gDhj+6Mr2x9H2L7oKqn3R0q1ERhjjDlaqpUIjDHGdGOJwBhjUlzKJIL+xkZIJSIyXkReEJENIrJORL4cdExBE5F0EVkpIn8MOpagichwEXlURDZ635F3Bx1TUETkK95vZK2I/FZEBnZbzwSREokgYmyE84HpwAIRmR5sVIEKAV9T1WnAu4B/SfH9AfBlYEPQQcSJ/wb+pKpTgdmk6H4RkTLceCmVqnoy7g4JVwUblT9SIhEQxdgIqURVa1T1Ne9xA+6HXhZsVMERkXHAhcCvg44laCJSBLwXWAygqq2quj/QoIKVAeSKSAaQxyBvnBnvUiUR9DQ2Qsoe+CKJSDkwF/i/gEMJ0k+BG4GOgOOIB8cDdcA9XlXZr0UkP+iggqCqbwO3A9uBGuCAqj4bbFT+SJVEELOxEZKJiBQAjwE3pOqN/kTkImCPqq4IOpY4kQFUAHeq6lzgEJCSbWoiMgJXczAJKAXyReQTwUblj1RJBDEbGyFZiEgmLgk8pKqPBx1PgN4DfEhE3sRVGb5fRB4MNqRA7QR2qmq4hPgoLjGkonOAN1S1TlXbgMeB0wOOyRepkghsbIQI4kYKXwxsUNWfBB1PkFT1X1V1nKqW474Xf1HVpDzri4aq1gI7ROQkb9LZwPoAQwrSduBdIpLn/WbOJkkbzv0cqjJu9DY2QsBhBek9wNXAGhFZ5U37lqouCy4kE0e+CDzknTRtAz4VcDyBUNX/E5FHgddwPe1WkqS3mrBbTBhjTIpLlaohY4wxvbBEYIwxKc4SgTHGpDhLBMYYk+IsERhjTIqzRGBMNyLSLiKrIv5idmWtiJSLyNpYrc+YWEiJ6wiMGaAmVZ0TdBDGDBUrERgTJRF5U0R+LCL/9P4me9MnisjzIlLt/Z/gTR8jIr8XkdXeX/j2BOki8ivvPvfPikhuYG/KGCwRGNOT3G5VQ1dGzDuoqvOAX+DuWor3+H5VnQU8BPzMm/4z4CVVnY27X0/4avYpwB2qOgPYD3zE13djTD/symJjuhGRRlUt6GH6m8D7VXWbd9O+WlUtFpG9wHGq2uZNr1HVUSJSB4xT1ZaIdZQDf1bVKd7zbwKZqvqDIXhrxvTISgTGDIz28ri3ZXrSEvG4HWurMwGzRGDMwFwZ8f9V7/ErHBnC8OPA37zHzwPXQ+eYyEVDFaQxA2FnIsYcLTfirqzgxu8NdyHNFpH/w51ELfCmfQlYIiLfwI3uFb5b55eBu0Xk07gz/+txI10ZE1esjcCYKHltBJWqujfoWIyJJasaMsaYFGclAmOMSXFWIjDGmBRnicAYY1KcJQJjjElxlgiMMSbFWSIwxpgU9/8Bj588GXFB/YsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [nan, 0.9004427194595337]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     26238\n",
      "           1       0.00      0.00      0.00      2901\n",
      "\n",
      "    accuracy                           0.90     29139\n",
      "   macro avg       0.45      0.50      0.47     29139\n",
      "weighted avg       0.81      0.90      0.85     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "plot_acc=accuracy_plot('Model accuracy of Account Information Features Model', history_acc)\n",
    "print(plot_acc)\n",
    "\n",
    "#train vs validation accuracy plot\n",
    "print(plot_acc)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Acc_model_results)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_acc)\n",
    "#Classification report\n",
    "print(metrics_report_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure numpy array\n",
    "train_data = np.array(combined_features_train).astype(np.float32)\n",
    "valid_data = np.array(combined_features_valid).astype(np.float32)\n",
    "test_data = np.array(combined_features_test).astype(np.float32)\n",
    "\n",
    "train_labels = labels_train\n",
    "valid_labels = labels_valid\n",
    "test_labels = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets shape of the data for the model\n",
    "input_shape_combined=train_data[0].shape\n",
    "input_shape_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798,)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "random_seeds(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model_flow\n",
    "Combined_model = model_flow(\"Combined\",11, input_shape_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Combined\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Combined_inputs (InputLayer) [(None, 798)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                51136     \n",
      "_________________________________________________________________\n",
      "normalization_1 (BatchNormal (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 54,034\n",
      "Trainable params: 53,906\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model summary\n",
    "Combined_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model diagram\n",
    "tf.keras.utils.plot_model(Combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile  model\n",
    "Combined_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3084 - accuracy: 0.5314 - val_loss: nan - val_accuracy: 0.1303\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2899 - accuracy: 0.5580 - val_loss: nan - val_accuracy: 0.9130\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3036 - accuracy: 0.5379 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 13s 2ms/step - loss: 1.2857 - accuracy: 0.5501 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 12s 2ms/step - loss: 1.3021 - accuracy: 0.5189 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 11s 2ms/step - loss: 1.2816 - accuracy: 0.5624 - val_loss: nan - val_accuracy: 0.2643\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 12s 2ms/step - loss: 1.2801 - accuracy: 0.5774 - val_loss: nan - val_accuracy: 0.9001\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 10s 1ms/step - loss: 1.2817 - accuracy: 0.5351 - val_loss: nan - val_accuracy: 0.6669\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 12s 2ms/step - loss: 1.2857 - accuracy: 0.5553 - val_loss: nan - val_accuracy: 0.9130\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.2797 - accuracy: 0.5367 - val_loss: nan - val_accuracy: 0.9105\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on training data\")\n",
    "history_combined = Combined_model.fit(train_data, train_labels, epochs=10, batch_size=15,\n",
    "                                      validation_data=(valid_data, valid_labels), class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "2914/2914 [==============================] - 2s 846us/step - loss: nan - accuracy: 0.8985\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on test data\")\n",
    "model_results_comb = Combined_model.evaluate(test_data, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_model.save('Combined_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='Combined_model.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Combined_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Combined_model.h5',\n",
    "                     'Combined_model.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Combined_model = load_model('Combined_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     26238\n",
      "           1       0.11      0.00      0.01      2901\n",
      "\n",
      "    accuracy                           0.90     29139\n",
      "   macro avg       0.51      0.50      0.48     29139\n",
      "weighted avg       0.82      0.90      0.85     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_comb=predict_test(Combined_model,test_data,test_labels, fit_test)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb = predict_account(Combined_model, train_data, bert_embeddings_df_train, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_comb.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96      1440\n",
      "           1       0.14      0.04      0.06       110\n",
      "\n",
      "    accuracy                           0.92      1550\n",
      "   macro avg       0.53      0.51      0.51      1550\n",
      "weighted avg       0.87      0.92      0.89      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_comb = classification_report(np.array(train_account_preds_comb['suspended_label']), np.array(train_account_preds_comb['pred_class']))\n",
    "print(report_tr_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb = predict_account(Combined_model, valid_data, bert_embeddings_df_valid, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_comb.csv', index=False, encoding = \"utf_8_sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       523\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.92       564\n",
      "   macro avg       0.46      0.50      0.48       564\n",
      "weighted avg       0.86      0.92      0.89       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_comb = classification_report(np.array(valid_account_preds_comb['suspended_label']), np.array(valid_account_preds_comb['pred_class']))\n",
    "print(report_v_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb = predict_account(Combined_model, test_data, bert_embeddings_df_test, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_comb.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.96       467\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.91       511\n",
      "   macro avg       0.46      0.50      0.48       511\n",
      "weighted avg       0.84      0.91      0.87       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_comb = classification_report(np.array(test_account_preds_comb['suspended_label']), np.array(test_account_preds_comb['pred_class']))\n",
    "print(report_te_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [nan, 0.8984865546226501]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     26238\n",
      "           1       0.11      0.00      0.01      2901\n",
      "\n",
      "    accuracy                           0.90     29139\n",
      "   macro avg       0.51      0.50      0.48     29139\n",
      "weighted avg       0.82      0.90      0.85     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "plot_combined=accuracy_plot('Model accuracy of Combined Model', history_combined)\n",
    "print(plot_combined)\n",
    "\n",
    "#train vs validation accuracy plot\n",
    "print(plot_combined)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", model_results_comb)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries)\n",
    "#Classification report\n",
    "print(metrics_report_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure numpy array\n",
    "train_data_labse = np.array(combined_features_train_labse).astype(np.float32)\n",
    "valid_data_labse = np.array(combined_features_valid_labse).astype(np.float32)\n",
    "test_data_labse = np.array(combined_features_test_labse).astype(np.float32)\n",
    "\n",
    "train_labels_labse = labels_train_labse\n",
    "valid_labels_labse = labels_valid_labse\n",
    "test_labels_labse = labels_test_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets shape of the data for the model\n",
    "input_shape_combined_labse=valid_data_labse[0].shape\n",
    "input_shape_combined_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "random_seeds(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model_flow\n",
    "Combined_model_labse = model_flow(\"Combined\",11, input_shape_combined_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Combined\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Combined_inputs (InputLayer) [(None, 798)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                51136     \n",
      "_________________________________________________________________\n",
      "normalization_1 (BatchNormal (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 54,034\n",
      "Trainable params: 53,906\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model summary\n",
    "Combined_model_labse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile  model\n",
    "Combined_model_labse.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: nan - accuracy: 0.8969 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: nan - accuracy: 0.8942 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8963 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: nan - accuracy: 0.8947 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: nan - accuracy: 0.8944 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: nan - accuracy: 0.8941 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: nan - accuracy: 0.8950 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: nan - accuracy: 0.8964 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8950 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8943 - val_loss: nan - val_accuracy: 0.9118\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on training data\")\n",
    "history_combined_labse = Combined_model_labse.fit(train_data_labse, train_labels_labse, epochs=10, batch_size=15,\n",
    "                                      validation_data=(valid_data_labse, valid_labels_labse), class_weight = class_weights_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "2914/2914 [==============================] - 3s 896us/step - loss: nan - accuracy: 0.9004\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on test data\")\n",
    "model_results_comb_labse = Combined_model_labse.evaluate(test_data_labse, test_labels_labse, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_model_labse.save('Combined_model_labse.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='Combined_model_labse.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Combined_model_labse.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Combined_model_labse.h5',\n",
    "                     'Combined_model_labse.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Combined_model_labse = load_model('Combined_model_labse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     26238\n",
      "           1       0.00      0.00      0.00      2901\n",
      "\n",
      "    accuracy                           0.90     29139\n",
      "   macro avg       0.45      0.50      0.47     29139\n",
      "weighted avg       0.81      0.90      0.85     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_comb_labse=predict_test(Combined_model_labse,test_data_labse,test_labels_labse, fit_test_labse)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_comb_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb_labse = predict_account(Combined_model_labse, train_data_labse, bert_embeddings_df_train_labse, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_comb_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1440\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "    accuracy                           0.93      1550\n",
      "   macro avg       0.46      0.50      0.48      1550\n",
      "weighted avg       0.86      0.93      0.89      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_comb_labse = classification_report(np.array(train_account_preds_comb_labse['suspended_label']), np.array(train_account_preds_comb_labse['pred_class']))\n",
    "print(report_tr_comb_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb_labse = predict_account(Combined_model_labse, valid_data_labse, bert_embeddings_df_valid_labse, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_comb_labse.csv', index=False, encoding = \"utf_8_sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       523\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.92       564\n",
      "   macro avg       0.46      0.50      0.48       564\n",
      "weighted avg       0.86      0.92      0.89       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_comb_labse_labse = classification_report(np.array(valid_account_preds_comb_labse['suspended_label']), np.array(valid_account_preds_comb_labse['pred_class']))\n",
    "print(report_v_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb_labse = predict_account(Combined_model_labse, test_data_labse, bert_embeddings_df_test_labse, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_comb_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.96       467\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.91       511\n",
      "   macro avg       0.46      0.50      0.48       511\n",
      "weighted avg       0.84      0.91      0.87       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_comb_labse = classification_report(np.array(test_account_preds_comb_labse['suspended_label']), np.array(test_account_preds_comb_labse['pred_class']))\n",
    "print(report_te_comb_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [nan, 0.9004427194595337]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     26238\n",
      "           1       0.00      0.00      0.00      2901\n",
      "\n",
      "    accuracy                           0.90     29139\n",
      "   macro avg       0.45      0.50      0.47     29139\n",
      "weighted avg       0.81      0.90      0.85     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "plot_combined=accuracy_plot('Model accuracy of Combined Model', history_combined_labse)\n",
    "print(plot_combined)\n",
    "\n",
    "#train vs validation accuracy plot\n",
    "print(plot_combined)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", model_results_comb_labse)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries)\n",
    "#Classification report\n",
    "print(metrics_report_comb_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine Tuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Only Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape_tweettext=train_data_tweettext[0].shape\n",
    "input_shape_tweettext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner_builder(hp):\n",
    "    inputs = keras.Input(input_shape_tweettext, name=\"Tuned_Tweet_Text_Inputs\")\n",
    "    x = layers.Dense(hp.Int('units', 50, 200, step = 20), activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(hp.Int('units1', 100, 200, step = 50), activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(hp.Int('units2', 20, 100, step = 20), activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(hp.Float('dropout',0.0,0.50, step=0.10, default=0.10))(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\",name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Tuned_Tweet_Text_Model\")\n",
    "    #Compile  model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "      hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner settings \n",
    "TweetText_tuner = kt.Hyperband(\n",
    "    tuner_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3, # \n",
    "    directory = 'Trial_run_tweettext', # Reduction factor for the number of epochs and number of models for each bracket.\n",
    "    project_name = 'Parameters_trials_tweettext_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks\n",
    "callback1=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clears training output\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thsis might take a while - I need top doublecheck how many parameters it is going through. I'm gueesing 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 54s]\n",
      "val_accuracy: 0.9127399921417236\n",
      "\n",
      "Best val_accuracy So Far: 0.9130434989929199\n",
      "Total elapsed time: 00h 42m 18s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#run through tuner\n",
    "TweetText_tuner.search(train_data_tweettext, train_labels_tweettext,validation_data=(valid_data_tweettext, valid_labels_tweettext),\n",
    "             callbacks=[callback1,ClearTrainingOutput()])\n",
    "\n",
    "# INFO:tensorflow:Oracle triggered exit- Is that normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x1a5e754410>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets best parameters\n",
    "best_hyper_TweetText = TweetText_tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper_TweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 1st Dense layer is 150\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 1st Dense layer is', best_hyper_TweetText.get('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 2nd Dense layer is 200\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 2nd Dense layer is', best_hyper_TweetText.get('units1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 3rd Dense layer is 80\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 3rd Dense layer is', best_hyper_TweetText.get('units2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Dropout layer is 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Dropout layer is', best_hyper_TweetText.get('dropout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate for the ADAM is 0.0012918827423762096\n"
     ]
    }
   ],
   "source": [
    "print('Best learning rate for the ADAM is', best_hyper_TweetText.get('learning_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies to tuning to model\n",
    "TweetText_model_tuned= TweetText_tuner.hypermodel.build(best_hyper_TweetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on Tuned Tweet Text training data\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5047: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 19s 3ms/step - loss: 1.3931 - accuracy: 0.5687 - val_loss: 0.6551 - val_accuracy: 0.6597\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.3043 - accuracy: 0.6606 - val_loss: 0.7608 - val_accuracy: 0.5476\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 20s 3ms/step - loss: 1.2846 - accuracy: 0.6654 - val_loss: 0.7800 - val_accuracy: 0.4541\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.2503 - accuracy: 0.6839 - val_loss: 0.6998 - val_accuracy: 0.5468\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.2509 - accuracy: 0.6743 - val_loss: 0.6350 - val_accuracy: 0.6692\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.2198 - accuracy: 0.7014 - val_loss: 0.6476 - val_accuracy: 0.6540\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.2092 - accuracy: 0.7083 - val_loss: 0.6418 - val_accuracy: 0.6624\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.1952 - accuracy: 0.6977 - val_loss: 0.6907 - val_accuracy: 0.6144\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.1874 - accuracy: 0.7022 - val_loss: 0.6276 - val_accuracy: 0.7154\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.1768 - accuracy: 0.7226 - val_loss: 0.6957 - val_accuracy: 0.6191\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on Tuned Tweet Text training data\")\n",
    "history_tweettext_tuned = TweetText_model_tuned.fit(train_data_tweettext, train_labels_tweettext, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_tweettext, valid_labels_tweettext), class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tweettext_tuned=accuracy_plot('Model accuracy of Tuned Tweet Text Model', history_tweettext_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Tuned Tweet Text test data\n",
      "2914/2914 [==============================] - 3s 1ms/step - loss: 0.6992 - accuracy: 0.6275\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on Tuned Tweet Text test data\")\n",
    "Tweettext_model_tuned_results = TweetText_model_tuned.evaluate(test_data_tweettext,test_labels_tweettext, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "TweetText_model_tuned.save('TweetText_model_tuned.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='TweetText_model_tuned.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/TweetText_model_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/TweetText_model_tuned.h5',\n",
    "                     'TweetText_model_tuned.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "TweetText_model_tuned = load_model('TweetText_model_tuned.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76     26238\n",
      "           1       0.11      0.39      0.17      2901\n",
      "\n",
      "    accuracy                           0.63     29139\n",
      "   macro avg       0.51      0.52      0.47     29139\n",
      "weighted avg       0.83      0.63      0.70     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_tweettext_tuned=predict_test(TweetText_model_tuned,test_data_tweettext,test_labels_tweettext, fit_test)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_tweettext_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>suspended_label</th>\n",
       "      <th>total_pre_prob</th>\n",
       "      <th>mean_pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03BMsU2SEkt8d42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909805</td>\n",
       "      <td>0.454903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06ESWEE9eDlnyuD</td>\n",
       "      <td>1</td>\n",
       "      <td>90.654739</td>\n",
       "      <td>0.686778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0TnE5IV0T2ELXU1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.873597</td>\n",
       "      <td>0.572295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10261026chch</td>\n",
       "      <td>1</td>\n",
       "      <td>4.992465</td>\n",
       "      <td>0.499247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1GEsShFS3GJoxoh</td>\n",
       "      <td>0</td>\n",
       "      <td>1.865694</td>\n",
       "      <td>0.621898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>zoezhan94328375</td>\n",
       "      <td>0</td>\n",
       "      <td>12.710810</td>\n",
       "      <td>0.385176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>zsN4FULPvhcM1IZ</td>\n",
       "      <td>0</td>\n",
       "      <td>2.431527</td>\n",
       "      <td>0.607882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>zubaira60720536</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671181</td>\n",
       "      <td>0.335591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>zurLgxI4tqnHSVF</td>\n",
       "      <td>0</td>\n",
       "      <td>124.237175</td>\n",
       "      <td>0.575172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>zydeng1984</td>\n",
       "      <td>0</td>\n",
       "      <td>32.304863</td>\n",
       "      <td>0.448679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1550 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user.screen_name  suspended_label  total_pre_prob  mean_pred_prob  \\\n",
       "0     03BMsU2SEkt8d42                0        0.909805        0.454903   \n",
       "1     06ESWEE9eDlnyuD                1       90.654739        0.686778   \n",
       "2     0TnE5IV0T2ELXU1                0       10.873597        0.572295   \n",
       "3        10261026chch                1        4.992465        0.499247   \n",
       "4     1GEsShFS3GJoxoh                0        1.865694        0.621898   \n",
       "...               ...              ...             ...             ...   \n",
       "1545  zoezhan94328375                0       12.710810        0.385176   \n",
       "1546  zsN4FULPvhcM1IZ                0        2.431527        0.607882   \n",
       "1547  zubaira60720536                0        0.671181        0.335591   \n",
       "1548  zurLgxI4tqnHSVF                0      124.237175        0.575172   \n",
       "1549       zydeng1984                0       32.304863        0.448679   \n",
       "\n",
       "      pred_class  \n",
       "0              0  \n",
       "1              1  \n",
       "2              1  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "1545           0  \n",
       "1546           1  \n",
       "1547           0  \n",
       "1548           1  \n",
       "1549           0  \n",
       "\n",
       "[1550 rows x 5 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_account_preds_tweettext_tuned = predict_account(TweetText_model_tuned, train_data_tweettext, bert_embeddings_df_train, df_train_f)\n",
    "train_account_preds_tweettext_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_tweettext_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.80      1440\n",
      "           1       0.16      0.81      0.27       110\n",
      "\n",
      "    accuracy                           0.69      1550\n",
      "   macro avg       0.57      0.74      0.54      1550\n",
      "weighted avg       0.92      0.69      0.77      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_tweettext_tuned = classification_report(np.array(train_account_preds_tweettext_tuned['suspended_label']), np.array(train_account_preds_tweettext_tuned['pred_class']))\n",
    "print(report_tr_tweettext_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext_tuned = predict_account(TweetText_model_tuned, valid_data_tweettext, bert_embeddings_df_valid, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_tweettext_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.66      0.77       523\n",
      "           1       0.05      0.22      0.08        41\n",
      "\n",
      "    accuracy                           0.63       564\n",
      "   macro avg       0.48      0.44      0.42       564\n",
      "weighted avg       0.85      0.63      0.72       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_tweettext_tuned = classification_report(np.array(valid_account_preds_tweettext_tuned['suspended_label']), np.array(valid_account_preds_tweettext_tuned['pred_class']))\n",
    "print(report_v_tweettext_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext_tuned = predict_account(TweetText_model_tuned, test_data_tweettext, bert_embeddings_df_test, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_tweettext_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.65      0.76       467\n",
      "           1       0.10      0.43      0.17        44\n",
      "\n",
      "    accuracy                           0.63       511\n",
      "   macro avg       0.51      0.54      0.46       511\n",
      "weighted avg       0.85      0.63      0.71       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_tweettext_tuned = classification_report(np.array(test_account_preds_tweettext_tuned['suspended_label']), np.array(test_account_preds_tweettext_tuned['pred_class']))\n",
    "print(report_te_tweettext_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet Text Model with Fine Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [0.6992205381393433, 0.6275438666343689]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76     26238\n",
      "           1       0.11      0.39      0.17      2901\n",
      "\n",
      "    accuracy                           0.63     29139\n",
      "   macro avg       0.51      0.52      0.47     29139\n",
      "weighted avg       0.83      0.63      0.70     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "print(plot_tweettext_tuned)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Tweettext_model_tuned_results)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_tweettext_tuned)\n",
    "#Classification report\n",
    "print(metrics_report_tweettext_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABSE for Tweets only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape_tweettext_labse=train_data_tweettext_labse[0].shape\n",
    "input_shape_tweettext_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner_builder(hp):\n",
    "    inputs = keras.Input(input_shape_tweettext_labse, name=\"Tuned_Tweet_Text_Inputs\")\n",
    "    x = layers.Dense(hp.Int('units', 50, 200, step = 20), activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(hp.Int('units1', 100, 200, step = 50), activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(hp.Int('units2', 20, 100, step = 20), activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(hp.Float('dropout',0.0,0.50, step=0.10, default=0.10))(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\",name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Tuned_Tweet_Text_Model\")\n",
    "    #Compile  model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "      hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner settings \n",
    "TweetText_tuner = kt.Hyperband(\n",
    "    tuner_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3, # \n",
    "    directory = 'Trial_run_tweettext', # Reduction factor for the number of epochs and number of models for each bracket.\n",
    "    project_name = 'Parameters_trials_tweettext_labse_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 29s]\n",
      "val_accuracy: 0.9117730855941772\n",
      "\n",
      "Best val_accuracy So Far: 0.9117730855941772\n",
      "Total elapsed time: 00h 41m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#run through tuner\n",
    "TweetText_tuner.search(train_data_tweettext_labse, train_labels_tweettext_labse,validation_data=(valid_data_tweettext_labse, valid_labels_tweettext_labse),\n",
    "             callbacks=[callback1,ClearTrainingOutput()])\n",
    "\n",
    "# INFO:tensorflow:Oracle triggered exit- Is that normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x1a31d7c610>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets best parameters\n",
    "best_hyper_TweetText_labse = TweetText_tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper_TweetText_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 1st Dense layer is 130\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 1st Dense layer is', best_hyper_TweetText_labse.get('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 2nd Dense layer is 150\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 2nd Dense layer is', best_hyper_TweetText_labse.get('units1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 3rd Dense layer is 60\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 3rd Dense layer is', best_hyper_TweetText_labse.get('units2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Dropout layer is 0.1\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Dropout layer is', best_hyper_TweetText_labse.get('dropout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate for the ADAM is 0.00010944719053745696\n"
     ]
    }
   ],
   "source": [
    "print('Best learning rate for the ADAM is', best_hyper_TweetText_labse.get('learning_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies to tuning to model\n",
    "TweetText_model_tuned_labse= TweetText_tuner.hypermodel.build(best_hyper_TweetText_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on Tuned Tweet Text training data\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 19s 2ms/step - loss: 1.3861 - accuracy: 0.4522 - val_loss: 0.6376 - val_accuracy: 0.9118\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3855 - accuracy: 0.4362 - val_loss: 0.7025 - val_accuracy: 0.3926\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.3721 - accuracy: 0.4517 - val_loss: 0.6653 - val_accuracy: 0.3980\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.3788 - accuracy: 0.4517 - val_loss: 0.7140 - val_accuracy: 0.3980\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.3808 - accuracy: 0.4180 - val_loss: 0.6680 - val_accuracy: 0.3980\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3847 - accuracy: 0.4218 - val_loss: 0.7040 - val_accuracy: 0.3882\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.3772 - accuracy: 0.4241 - val_loss: 0.6802 - val_accuracy: 0.3828\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3690 - accuracy: 0.4221 - val_loss: 0.7259 - val_accuracy: 0.3600\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.3770 - accuracy: 0.4191 - val_loss: 0.7104 - val_accuracy: 0.3934\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3812 - accuracy: 0.4186 - val_loss: 0.6782 - val_accuracy: 0.3980\n"
     ]
    }
   ],
   "source": [
    "#Fitting on training and validation data\n",
    "print(\"Fit model on Tuned Tweet Text training data\")\n",
    "history_tweettext_tuned_labse = TweetText_model_tuned_labse.fit(train_data_tweettext_labse, train_labels_tweettext_labse, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_tweettext_labse, valid_labels_tweettext_labse), class_weight = class_weights_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tweettext_tuned_labse=accuracy_plot('Model accuracy of Tuned Tweet Text Model', history_tweettext_tuned_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Tuned Tweet Text test data\n",
      "2914/2914 [==============================] - 3s 986us/step - loss: 0.6716 - accuracy: 0.4372\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluate on Tuned Tweet Text test data\")\n",
    "Tweettext_model_tuned_results_labse = TweetText_model_tuned_labse.evaluate(test_data_tweettext_labse,test_labels_tweettext_labse, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "TweetText_model_tuned_labse.save('TweetText_model_tuned_labse.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='TweetText_model_tuned_labse.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/TweetText_model_tuned_labse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/TweetText_model_tuned_labse.h5',\n",
    "                     'TweetText_model_tuned_labse.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "TweetText_model_tuned_labse = load_model('TweetText_model_tuned_labse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.42      0.57     26238\n",
      "           1       0.10      0.62      0.18      2901\n",
      "\n",
      "    accuracy                           0.44     29139\n",
      "   macro avg       0.51      0.52      0.38     29139\n",
      "weighted avg       0.83      0.44      0.53     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_tweettext_tuned_labse=predict_test(TweetText_model_tuned_labse,test_data_tweettext_labse,test_labels_tweettext_labse, fit_test_labse)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_tweettext_tuned_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext_tuned_labse = predict_account(TweetText_model_tuned_labse, train_data_tweettext_labse, bert_embeddings_df_train_labse, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_tweettext_tuned_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_tweettext_tuned_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.65      0.77      1440\n",
      "           1       0.09      0.46      0.15       110\n",
      "\n",
      "    accuracy                           0.64      1550\n",
      "   macro avg       0.52      0.56      0.46      1550\n",
      "weighted avg       0.88      0.64      0.73      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_tweettext_tuned_labse = classification_report(np.array(train_account_preds_tweettext_tuned_labse['suspended_label']), np.array(train_account_preds_tweettext_tuned_labse['pred_class']))\n",
    "print(report_tr_tweettext_tuned_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext_tuned_labse = predict_account(TweetText_model_tuned_labse, valid_data_tweettext_labse, bert_embeddings_df_valid_labse, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_tweettext_tuned_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_tweettext_tuned_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.65      0.76       523\n",
      "           1       0.06      0.29      0.10        41\n",
      "\n",
      "    accuracy                           0.62       564\n",
      "   macro avg       0.49      0.47      0.43       564\n",
      "weighted avg       0.86      0.62      0.71       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_tweettext_tuned_labse_labse = classification_report(np.array(valid_account_preds_tweettext_tuned_labse['suspended_label']), np.array(valid_account_preds_tweettext_tuned_labse['pred_class']))\n",
    "print(report_v_tweettext_tuned_labse_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext_tuned_labse = predict_account(TweetText_model_tuned_labse, test_data_tweettext_labse, bert_embeddings_df_test_labse, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_tweettext_tuned_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_tweettext_tuned_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.63      0.75       467\n",
      "           1       0.10      0.45      0.17        44\n",
      "\n",
      "    accuracy                           0.62       511\n",
      "   macro avg       0.51      0.54      0.46       511\n",
      "weighted avg       0.85      0.62      0.70       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_tweettext_tuned_labse = classification_report(np.array(test_account_preds_tweettext_tuned_labse['suspended_label']), np.array(test_account_preds_tweettext_tuned_labse['pred_class']))\n",
    "print(report_te_tweettext_tuned_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [0.6715512275695801, 0.437214732170105]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.42      0.57     26238\n",
      "           1       0.10      0.62      0.18      2901\n",
      "\n",
      "    accuracy                           0.44     29139\n",
      "   macro avg       0.51      0.52      0.38     29139\n",
      "weighted avg       0.83      0.44      0.53     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "print(plot_tweettext_tuned_labse)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Tweettext_model_tuned_results_labse)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_tweettext_tuned)\n",
    "#Classification report\n",
    "print(metrics_report_tweettext_tuned_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Account Only Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input shape for account features\n",
    "input_shape_acc=train_data_acc[0].shape\n",
    "input_shape_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner_builder1(hp):\n",
    "    inputs = keras.Input(shape=input_shape_acc, name=\"Tuned_Account_Info_Inputs\")\n",
    "    x = layers.Dense(hp.Int('units', 50, 200, step = 20), activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(hp.Int('units1', 100, 200, step = 50), activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(hp.Int('units2', 20, 100, step = 20), activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(hp.Float('dropout',0.0,0.50, step=0.10, default=0.10))(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\",name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Tuned_Account_Info_Model\")\n",
    "    #Compile  model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "        hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner settings \n",
    "Acc_info_tuner = kt.Hyperband(\n",
    "    tuner_builder1,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory = 'Trial_run_acc_info2',\n",
    "    project_name = 'Parameters_trials_acc_info2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 28s]\n",
      "val_accuracy: 0.9130434989929199\n",
      "\n",
      "Best val_accuracy So Far: 0.9130434989929199\n",
      "Total elapsed time: 00h 38m 40s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#run through tuner\n",
    "Acc_info_tuner.search(train_data_acc, train_labels,validation_data=(valid_data_acc, valid_labels),callbacks=[callback1,ClearTrainingOutput()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x1a5da0c050>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets best parameters\n",
    "best_hyper_Acc_info = Acc_info_tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper_Acc_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 1st Dense layer is 150\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 1st Dense layer is', best_hyper_Acc_info.get('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 2nd Dense layer is 150\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 2nd Dense layer is', best_hyper_Acc_info.get('units1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 3rd Dense layer is 40\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 3rd Dense layer is', best_hyper_Acc_info.get('units2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Dropout layer is 0.1\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Dropout layer is', best_hyper_Acc_info.get('dropout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate for the ADAM is 0.0016275322152575935\n"
     ]
    }
   ],
   "source": [
    "print('Best learning rate for the ADAM is', best_hyper_Acc_info.get('learning_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies to tuning to model\n",
    "Acc_info_model_tuned= Acc_info_tuner.hypermodel.build(best_hyper_Acc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data with tuned Account Information model\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3137 - accuracy: 0.5284 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 14s 2ms/step - loss: 1.2931 - accuracy: 0.5281 - val_loss: nan - val_accuracy: 0.3997\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 18s 3ms/step - loss: 1.3043 - accuracy: 0.5427 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 21s 3ms/step - loss: 1.2874 - accuracy: 0.5421 - val_loss: nan - val_accuracy: 0.5421\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 18s 3ms/step - loss: 1.3033 - accuracy: 0.4827 - val_loss: nan - val_accuracy: 0.4365\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 19s 3ms/step - loss: 1.2852 - accuracy: 0.5624 - val_loss: nan - val_accuracy: 0.6530\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.2813 - accuracy: 0.5688 - val_loss: nan - val_accuracy: 0.7443\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.2851 - accuracy: 0.5162 - val_loss: nan - val_accuracy: 0.5349\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 23s 3ms/step - loss: 1.2891 - accuracy: 0.5406 - val_loss: nan - val_accuracy: 0.5864\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.2843 - accuracy: 0.5082 - val_loss: nan - val_accuracy: 0.6957\n"
     ]
    }
   ],
   "source": [
    "#trials using newly tuned model\n",
    "#Fitting on training and validation data based on selected tuning parameters\n",
    "print(\"Fit model on training data with tuned Account Information model\")\n",
    "history_acc_info_tuned = Acc_info_model_tuned.fit(train_data_acc, train_labels, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_acc, valid_labels), class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc_info_tuned=accuracy_plot('Model accuracy of Tuned Account Information Model', history_acc_info_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate tuned Account Information model on test data\n",
      "2914/2914 [==============================] - 3s 991us/step - loss: nan - accuracy: 0.6559\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the tuned model on the test data \n",
    "print(\"Evaluate tuned Account Information model on test data\")\n",
    "acc_info_tuned_results = Acc_info_model_tuned.evaluate(test_data_acc, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_info_model_tuned.save('Acc_info_model_tuned.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='Acc_info_model_tuned.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Acc_info_model_tuned.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Acc_info_model_tuned.h5',\n",
    "                     'Acc_info_model_tuned.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Tweettext_model = load_model('Acc_info_model_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.68      0.78     26238\n",
      "           1       0.14      0.48      0.22      2901\n",
      "\n",
      "    accuracy                           0.66     29139\n",
      "   macro avg       0.53      0.58      0.50     29139\n",
      "weighted avg       0.84      0.66      0.72     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_acc_tuned=predict_test(Acc_info_model_tuned,test_data_acc,test_labels, fit_test)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_acc_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_acc_tuned = predict_account(Acc_info_model_tuned, train_data_acc, bert_embeddings_df_train, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_acc_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_acc_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.46      0.62      1440\n",
      "           1       0.07      0.56      0.13       110\n",
      "\n",
      "    accuracy                           0.47      1550\n",
      "   macro avg       0.50      0.51      0.37      1550\n",
      "weighted avg       0.87      0.47      0.58      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_acc_tuned = classification_report(np.array(train_account_preds_acc_tuned['suspended_label']), np.array(train_account_preds_acc_tuned['pred_class']))\n",
    "print(report_tr_acc_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>suspended_label</th>\n",
       "      <th>total_pre_prob</th>\n",
       "      <th>mean_pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0PbG8K7S2ioNjL4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.027080</td>\n",
       "      <td>0.256770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1s11370977</td>\n",
       "      <td>0</td>\n",
       "      <td>7.050694</td>\n",
       "      <td>0.783410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nk96A8nKuoTNMO</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2wuajzMCkrXhVJR</td>\n",
       "      <td>0</td>\n",
       "      <td>2.155278</td>\n",
       "      <td>0.718426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3P7EJm4JEW4rzyr</td>\n",
       "      <td>0</td>\n",
       "      <td>16.812952</td>\n",
       "      <td>0.700540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>zhangta50319667</td>\n",
       "      <td>0</td>\n",
       "      <td>8.892814</td>\n",
       "      <td>0.635201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>zhutnut</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>zoub98237444</td>\n",
       "      <td>0</td>\n",
       "      <td>6.069143</td>\n",
       "      <td>0.758643</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>zuoye0510</td>\n",
       "      <td>0</td>\n",
       "      <td>2.501173</td>\n",
       "      <td>0.039701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>zzzzzzh15</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637598</td>\n",
       "      <td>0.545866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user.screen_name  suspended_label  total_pre_prob  mean_pred_prob  \\\n",
       "0    0PbG8K7S2ioNjL4                0        1.027080        0.256770   \n",
       "1         1s11370977                0        7.050694        0.783410   \n",
       "2    2nk96A8nKuoTNMO                0        0.000000             NaN   \n",
       "3    2wuajzMCkrXhVJR                0        2.155278        0.718426   \n",
       "4    3P7EJm4JEW4rzyr                0       16.812952        0.700540   \n",
       "..               ...              ...             ...             ...   \n",
       "559  zhangta50319667                0        8.892814        0.635201   \n",
       "560          zhutnut                0        0.000000             NaN   \n",
       "561     zoub98237444                0        6.069143        0.758643   \n",
       "562        zuoye0510                0        2.501173        0.039701   \n",
       "563        zzzzzzh15                0        1.637598        0.545866   \n",
       "\n",
       "     pred_class  \n",
       "0             0  \n",
       "1             1  \n",
       "2             0  \n",
       "3             1  \n",
       "4             1  \n",
       "..          ...  \n",
       "559           1  \n",
       "560           0  \n",
       "561           1  \n",
       "562           0  \n",
       "563           1  \n",
       "\n",
       "[564 rows x 5 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_account_preds_acc_tuned = predict_account(Acc_info_model_tuned, valid_data_acc, bert_embeddings_df_valid, df_valid_f)\n",
    "valid_account_preds_acc_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_acc_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_acc_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.67      0.78       523\n",
      "           1       0.08      0.39      0.14        41\n",
      "\n",
      "    accuracy                           0.65       564\n",
      "   macro avg       0.51      0.53      0.46       564\n",
      "weighted avg       0.87      0.65      0.73       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_acc_tuned = classification_report(np.array(valid_account_preds_acc_tuned['suspended_label']), np.array(valid_account_preds_acc_tuned['pred_class']))\n",
    "print(report_v_acc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>suspended_label</th>\n",
       "      <th>total_pre_prob</th>\n",
       "      <th>mean_pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0zGrNrwl6bRjIv9</td>\n",
       "      <td>0</td>\n",
       "      <td>6.001352</td>\n",
       "      <td>0.545577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1hs6oMXjD9toV8o</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2IKcY5haEk7adWs</td>\n",
       "      <td>0</td>\n",
       "      <td>62.913994</td>\n",
       "      <td>0.827816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2R0fErCwAWXV2mG</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ebRkaTgSVMcGrl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>zh94004641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>zhong_yuming</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485596</td>\n",
       "      <td>0.485596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>zhongshusheren</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>zhuguli10808518</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338965</td>\n",
       "      <td>0.021185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>zyb19990107</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user.screen_name  suspended_label  total_pre_prob  mean_pred_prob  \\\n",
       "0    0zGrNrwl6bRjIv9                0        6.001352        0.545577   \n",
       "1    1hs6oMXjD9toV8o                0        0.000000             NaN   \n",
       "2    2IKcY5haEk7adWs                0       62.913994        0.827816   \n",
       "3    2R0fErCwAWXV2mG                0        0.000000             NaN   \n",
       "4    2ebRkaTgSVMcGrl                0        0.000000             NaN   \n",
       "..               ...              ...             ...             ...   \n",
       "506       zh94004641                0        0.000000             NaN   \n",
       "507     zhong_yuming                0        0.485596        0.485596   \n",
       "508   zhongshusheren                0        0.000000             NaN   \n",
       "509  zhuguli10808518                0        0.338965        0.021185   \n",
       "510      zyb19990107                1        0.000000             NaN   \n",
       "\n",
       "     pred_class  \n",
       "0             1  \n",
       "1             0  \n",
       "2             1  \n",
       "3             0  \n",
       "4             0  \n",
       "..          ...  \n",
       "506           0  \n",
       "507           0  \n",
       "508           0  \n",
       "509           0  \n",
       "510           0  \n",
       "\n",
       "[511 rows x 5 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_account_preds_acc_tuned = predict_account(Acc_info_model_tuned, test_data_acc, bert_embeddings_df_test, df_test_f)\n",
    "test_account_preds_acc_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_acc_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_acc_tuned.csv', index=False, encoding = \"utf_8_sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.69      0.79       467\n",
      "           1       0.11      0.39      0.17        44\n",
      "\n",
      "    accuracy                           0.67       511\n",
      "   macro avg       0.51      0.54      0.48       511\n",
      "weighted avg       0.85      0.67      0.74       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_acc_tuned_tuned = classification_report(np.array(test_account_preds_acc_tuned['suspended_label']), np.array(test_account_preds_acc_tuned['pred_class']))\n",
    "print(report_te_acc_tuned_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Account Information Model with Fine Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [nan, 0.655890703201294]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.68      0.78     26238\n",
      "           1       0.14      0.48      0.22      2901\n",
      "\n",
      "    accuracy                           0.66     29139\n",
      "   macro avg       0.53      0.58      0.50     29139\n",
      "weighted avg       0.84      0.66      0.72     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "print(plot_acc_info_tuned)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", acc_info_tuned_results)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_acc_info_tuned)\n",
    "#Classification report\n",
    "print(metrics_report_acc_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets shape of the data for the model\n",
    "input_shape_combined=train_data[0].shape\n",
    "input_shape_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to tune to the model\n",
    "def tuner_builder(hp):\n",
    "    inputs = keras.Input(shape=input_shape_combined, name=\"Tuned_Combined_Inputs\") # They had 3333 (w CV) -> I hard coded my model shape. \n",
    "    x = layers.Dense(hp.Int('units', 50, 200, step = 20), activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(hp.Int('units1', 100, 200, step = 50), activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(hp.Int('units2', 20, 100, step = 20), activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(hp.Float('dropout',0.0,0.50, step=0.10, default=0.10))(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\",name=\"predictions\")(x) # I get error now, but with full data there will be 2\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Tuned_Combined_Model\")\n",
    "    #Compile  model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "        hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner settings \n",
    "Combined_tuner = kt.Hyperband(\n",
    "    tuner_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory = 'Trial_run_combined2',\n",
    "    project_name = 'Parameters_trials_combined_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 36s]\n",
      "val_accuracy: 0.9130434989929199\n",
      "\n",
      "Best val_accuracy So Far: 0.9130434989929199\n",
      "Total elapsed time: 00h 41m 15s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#run through tuner\n",
    "Combined_tuner.search(train_data, train_labels,validation_data=(valid_data, valid_labels),\n",
    "             callbacks=[callback1,ClearTrainingOutput()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x1a3da8b790>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets best parameters\n",
    "best_hyper_combined = Combined_tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 1st Dense layer is 90\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 1st Dense layer is', best_hyper_combined.get('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 2nd Dense layer is 150\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 2nd Dense layer is', best_hyper_combined.get('units1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 3rd Dense layer is 20\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 3rd Dense layer is', best_hyper_combined.get('units2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Dropout layer is 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Dropout layer is', best_hyper_combined.get('dropout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate for the ADAM is 0.0001336992122024675\n"
     ]
    }
   ],
   "source": [
    "print('Best learning rate for the ADAM is', best_hyper_combined.get('learning_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies to tuning to model\n",
    "Combined_model_tuned= Combined_tuner.hypermodel.build(best_hyper_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data with tuned model\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.3144 - accuracy: 0.4982 - val_loss: nan - val_accuracy: 0.1981\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.2970 - accuracy: 0.5303 - val_loss: nan - val_accuracy: 0.8600\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3069 - accuracy: 0.5218 - val_loss: nan - val_accuracy: 0.1306\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.2883 - accuracy: 0.5401 - val_loss: nan - val_accuracy: 0.1532\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.3057 - accuracy: 0.5130 - val_loss: nan - val_accuracy: 0.4063\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.2851 - accuracy: 0.5293 - val_loss: nan - val_accuracy: 0.1229\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: 1.2808 - accuracy: 0.5391 - val_loss: nan - val_accuracy: 0.9130\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: 1.2850 - accuracy: 0.5278 - val_loss: nan - val_accuracy: 0.8277\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: 1.2890 - accuracy: 0.5323 - val_loss: nan - val_accuracy: 0.5910\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 18s 2ms/step - loss: 1.2854 - accuracy: 0.5252 - val_loss: nan - val_accuracy: 0.5065\n"
     ]
    }
   ],
   "source": [
    "#trials using newly tuned model\n",
    "#Fitting on training and validation data based on selected tuning parameters\n",
    "print(\"Fit model on training data with tuned model\")\n",
    "history_combined_tuned = Combined_model_tuned.fit(train_data, train_labels, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data, valid_labels), class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_combined_tuned=accuracy_plot('Model accuracy of Tuned Combined Model', history_combined_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate tuned model on test data\n",
      "2914/2914 [==============================] - 3s 996us/step - loss: nan - accuracy: 0.3743\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the tuned model on the test data \n",
    "print(\"Evaluate tuned model on test data\")\n",
    "Combined_model_tuned_results = Combined_model_tuned.evaluate(test_data, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_model_tuned.save('Combined_model_tuned.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='Combined_model_tuned.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Combined_model_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Combined_model_tuned.h5',\n",
    "                     'Combined_model_tuned.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Combined_model_tuned = load_model('Combined_model_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.35      0.50     26238\n",
      "           1       0.09      0.56      0.15      2901\n",
      "\n",
      "    accuracy                           0.37     29139\n",
      "   macro avg       0.48      0.46      0.33     29139\n",
      "weighted avg       0.80      0.37      0.47     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_comb_tuned=predict_test(Combined_model_tuned,test_data,test_labels, fit_test)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_comb_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb_tuned = predict_account(Combined_model_tuned, train_data, bert_embeddings_df_train, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_comb_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.20      0.33      1440\n",
      "           1       0.08      0.86      0.14       110\n",
      "\n",
      "    accuracy                           0.25      1550\n",
      "   macro avg       0.51      0.53      0.24      1550\n",
      "weighted avg       0.89      0.25      0.32      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_comb_tuned = classification_report(np.array(train_account_preds_comb_tuned['suspended_label']), np.array(train_account_preds_comb_tuned['pred_class']))\n",
    "print(report_tr_comb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb_tuned = predict_account(Combined_model_tuned, valid_data, bert_embeddings_df_valid, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_comb_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.47      0.63       523\n",
      "           1       0.09      0.71      0.17        41\n",
      "\n",
      "    accuracy                           0.49       564\n",
      "   macro avg       0.52      0.59      0.40       564\n",
      "weighted avg       0.89      0.49      0.59       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_comb_tuned = classification_report(np.array(valid_account_preds_comb_tuned['suspended_label']), np.array(valid_account_preds_comb_tuned['pred_class']))\n",
    "print(report_v_comb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb_tuned = predict_account(Combined_model_tuned, test_data, bert_embeddings_df_test, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb_tuned.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_comb_tuned.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.47      0.62       467\n",
      "           1       0.08      0.50      0.14        44\n",
      "\n",
      "    accuracy                           0.47       511\n",
      "   macro avg       0.49      0.48      0.38       511\n",
      "weighted avg       0.84      0.47      0.58       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_comb = classification_report(np.array(test_account_preds_comb_tuned['suspended_label']), np.array(test_account_preds_comb_tuned['pred_class']))\n",
    "print(report_te_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Model with Fine Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [nan, 0.3742750287055969]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.35      0.50     26238\n",
      "           1       0.09      0.56      0.15      2901\n",
      "\n",
      "    accuracy                           0.37     29139\n",
      "   macro avg       0.48      0.46      0.33     29139\n",
      "weighted avg       0.80      0.37      0.47     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "print(plot_combined_tuned)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Combined_model_tuned_results)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_tuned)\n",
    "#Classification report\n",
    "print(metrics_report_comb_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Model with LAbse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798,)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets shape of the data for the model\n",
    "input_shape_combined_labse=train_data_labse[0].shape\n",
    "input_shape_combined_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to tune to the model\n",
    "def tuner_builder(hp):\n",
    "    inputs = keras.Input(shape=input_shape_combined_labse, name=\"Tuned_Combined_Inputs\") # They had 3333 (w CV) -> I hard coded my model shape. \n",
    "    x = layers.Dense(hp.Int('units', 50, 200, step = 20), activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.BatchNormalization(name=\"normalization_1\")(x)\n",
    "    x = layers.Dense(hp.Int('units1', 100, 200, step = 50), activation=\"relu\",name=\"dense_2\")(x)\n",
    "    x = layers.Dense(hp.Int('units2', 20, 100, step = 20), activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"dense_3\")(x)\n",
    "    x = layers.Dropout(hp.Float('dropout',0.0,0.50, step=0.10, default=0.10))(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\",name=\"predictions\")(x) # I get error now, but with full data there will be 2\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Tuned_Combined_Model\")\n",
    "    #Compile  model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "        hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner settings \n",
    "Combined_tuner = kt.Hyperband(\n",
    "    tuner_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory = 'Trial_run_combined2',\n",
    "    project_name = 'Parameters_trials_combined_labse_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 32s]\n",
      "val_accuracy: 0.9117730855941772\n",
      "\n",
      "Best val_accuracy So Far: 0.9117730855941772\n",
      "Total elapsed time: 00h 42m 08s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#run through tuner\n",
    "Combined_tuner.search(train_data_labse, train_labels_labse,validation_data=(valid_data_labse, valid_labels_labse),\n",
    "             callbacks=[callback1,ClearTrainingOutput()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x1a3db2b690>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets best parameters\n",
    "best_hyper_combined_labse = Combined_tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper_combined_labse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 1st Dense layer is 170\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 1st Dense layer is', best_hyper_combined_labse.get('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 2nd Dense layer is 150\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 2nd Dense layer is', best_hyper_combined_labse.get('units1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for 3rd Dense layer is 40\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for 3rd Dense layer is', best_hyper_combined_labse.get('units2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Dropout layer is 0.4\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Dropout layer is', best_hyper_combined_labse.get('dropout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate for the ADAM is 0.0017507470678438822\n"
     ]
    }
   ],
   "source": [
    "print('Best learning rate for the ADAM is', best_hyper_combined_labse.get('learning_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies to tuning to model\n",
    "Combined_model_tuned_labse= Combined_tuner.hypermodel.build(best_hyper_combined_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data with tuned model\n",
      "Epoch 1/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 2/10\n",
      "7159/7159 [==============================] - 15s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 3/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 4/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 5/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 6/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 7/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 8/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 9/10\n",
      "7159/7159 [==============================] - 16s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n",
      "Epoch 10/10\n",
      "7159/7159 [==============================] - 17s 2ms/step - loss: nan - accuracy: 0.8948 - val_loss: nan - val_accuracy: 0.9118\n"
     ]
    }
   ],
   "source": [
    "#trials using newly tuned model\n",
    "#Fitting on training and validation data based on selected tuning parameters\n",
    "print(\"Fit model on training data with tuned model\")\n",
    "history_combined_tuned_labse = Combined_model_tuned.fit(train_data_labse, train_labels_labse, epochs=10, batch_size=15,\n",
    "                   validation_data=(valid_data_labse, valid_labels_labse), class_weight = class_weights_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_combined_tuned_labse=accuracy_plot('Model accuracy of Tuned Combined Model', history_combined_tuned_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate tuned model on test data\n",
      "2914/2914 [==============================] - 4s 1ms/step - loss: nan - accuracy: 0.1831\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the tuned model on the test data \n",
    "print(\"Evaluate tuned model on test data\")\n",
    "Combined_model_tuned_results_labse = Combined_model_tuned_labse.evaluate(test_data_labse, test_labels_labse, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_model_tuned_labse.save('Combined_model_tuned_labse.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "s3.upload_file(Filename='Combined_model_tuned_labse.h5',\n",
    "                  Bucket=import_bucket,\n",
    "                  Key='modeling/model_output/Combined_model_tuned_labse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(import_bucket,\n",
    "                     'modeling/model_output/Combined_model_tuned_labse.h5',\n",
    "                     'Combined_model_tuned_labse.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "Combined_model_tuned_labse = load_model('Combined_model_tuned_labse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.06      0.11     26238\n",
      "           1       0.09      0.80      0.16      2901\n",
      "\n",
      "    accuracy                           0.13     29139\n",
      "   macro avg       0.41      0.43      0.13     29139\n",
      "weighted avg       0.67      0.13      0.12     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test predict_test\n",
    "metrics_report_comb_tuned_labse=predict_test(Combined_model_tuned_labse,test_data_labse,test_labels_labse, fit_test_labse)\n",
    "\n",
    "# predicted_susp_tweettext\n",
    "print(metrics_report_comb_tuned_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb_tuned_labse = predict_account(Combined_model_tuned_labse, train_data_labse, bert_embeddings_df_train_labse, df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_account_preds_comb_tuned_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/train_account_preds_comb_tuned_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.27      0.42      1440\n",
      "           1       0.08      0.81      0.14       110\n",
      "\n",
      "    accuracy                           0.31      1550\n",
      "   macro avg       0.51      0.54      0.28      1550\n",
      "weighted avg       0.89      0.31      0.40      1550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_tr_comb_tuned_labse = classification_report(np.array(train_account_preds_comb_tuned_labse['suspended_label']), np.array(train_account_preds_comb_tuned_labse['pred_class']))\n",
    "print(report_tr_comb_tuned_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb_tuned_labse = predict_account(Combined_model_tuned_labse, valid_data_labse, bert_embeddings_df_valid_labse, df_valid_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_account_preds_comb_tuned_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/valid_account_preds_comb_tuned_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.31      0.46       523\n",
      "           1       0.08      0.80      0.15        41\n",
      "\n",
      "    accuracy                           0.34       564\n",
      "   macro avg       0.52      0.56      0.31       564\n",
      "weighted avg       0.89      0.34      0.44       564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_v_comb_tuned_labse = classification_report(np.array(valid_account_preds_comb_tuned_labse['suspended_label']), np.array(valid_account_preds_comb_tuned_labse['pred_class']))\n",
    "print(report_v_comb_tuned_labse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb_tuned_labse = predict_account(Combined_model_tuned_labse, test_data_labse, bert_embeddings_df_test_labse, df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_account_preds_comb_tuned_labse.to_csv('s3://joe-exotic-2020/modeling/account_level_results/test_account_preds_comb_tuned_labse.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.31      0.46       467\n",
      "           1       0.09      0.73      0.16        44\n",
      "\n",
      "    accuracy                           0.34       511\n",
      "   macro avg       0.51      0.52      0.31       511\n",
      "weighted avg       0.85      0.34      0.44       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#report = classification_report(np.array(test_labels), predictions)\n",
    "report_te_comb_labse = classification_report(np.array(test_account_preds_comb_tuned_labse['suspended_label']), np.array(test_account_preds_comb_tuned_labse['pred_class']))\n",
    "print(report_te_comb_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'matplotlib.pyplot' from '/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>\n",
      "test loss, test accuracy: [nan, 0.13487079739570618]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.06      0.11     26238\n",
      "           1       0.09      0.80      0.16      2901\n",
      "\n",
      "    accuracy                           0.13     29139\n",
      "   macro avg       0.41      0.43      0.13     29139\n",
      "weighted avg       0.67      0.13      0.12     29139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train vs validation accuracy plot\n",
    "print(plot_combined_tuned_labse)\n",
    "#test accuracy\n",
    "print(\"test loss, test accuracy:\", Combined_model_tuned_results_labse)\n",
    "#Countries labels predicted\n",
    "#print(predicted_countries_tuned)\n",
    "#Classification report\n",
    "print(metrics_report_comb_tuned_labse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 1. Load Packages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "import emoji\n",
    "import random \n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "########## 2. Set Parameters \n",
    "\n",
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "filepath = \"data/\"\n",
    "\n",
    "import_bucket = \"joe-exotic-2020\"\n",
    "\n",
    "key = 'processed' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "results_bucket = 'full_clean' # already created on S3\n",
    "\n",
    "######### 3. Import Data from S3 bucket\n",
    "# Import Flattened Data\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"processed.csv\",obj.key)]\n",
    "\n",
    "filelist = filelist[:2]\n",
    "\n",
    "def import_data(filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe. Concatenates multiple files if necessary. \n",
    "    Inputs: Directory path, number of rows to skip\n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "    # Identify if directory or file path was provided\n",
    "    if type(filelist) == list:\n",
    "        dataframes = []\n",
    "        # Iterate through files of the directory\n",
    "        for filename in filelist:\n",
    "            object_key = filename.split('/', 1)[1]\n",
    "            csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "            body = csv_obj['Body']\n",
    "            csv_string = body.read().decode('utf-8')\n",
    "            dataframe = pd.read_csv(StringIO(csv_string))\n",
    "            #Subset vars - Keep only those that are relevant. \n",
    "            dataframe = dataframe[['created_at', 'text', 'source', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive',\n",
    "                      'withheld_in_countries', 'place.country', 'quoted_status_id', 'user.created_at',\n",
    "                      'user.description', 'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
    "                       'user.geo_enabled', 'user.has_extended_profile', 'user.lang', 'user.listed_count',\n",
    "                       'user.location', 'user.name', 'user.protected', 'user.screen_name',\n",
    "                       'user.time_zone', 'user.verified', 'user.protected', 'user.default_profile',\n",
    "                       'is_quote_status', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count',\n",
    "                       'retweeted_status.user.followers_count', 'retweeted_status.user.friends_count', 'user.url',\n",
    "                       'in_reply_to_status_id', 'id_str', 'user.id', 'suspended', 'user.statuses_count', 'id']]\n",
    "            dataframes.append(dataframe)\n",
    "        df = pd.concat(dataframes, ignore_index=True, sort=False) # axis=0?\n",
    "    else:\n",
    "        # Read in single file\n",
    "        object_key = filelist.rsplit('/', 1)[1]\n",
    "        csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "        body = csv_obj['Body']\n",
    "        csv_string = body.read().decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(csv_string))\n",
    "    rows = len(df)\n",
    "    dups = len(df) - len(df.drop_duplicates())\n",
    "    \n",
    "    # Check for text_column\n",
    "    try:\n",
    "        if len(df[text_column]) > 1:  \n",
    "            pass\n",
    "    except:\n",
    "        print(\"Cannot find text column. Please confirm that the text_column and skiprow parameters are updated.\")\n",
    "    \n",
    "    # Clean up rows and drop duplicates based on tweet id string. \n",
    "    df = df.drop_duplicates(subset=['id_str'])\n",
    "    df = df[df['created_at'] != \"False\"]\n",
    "    df = df[df['created_at'] != \"created_at\"]\n",
    "    #df['user.id'] = df['user.id'].astype('float')\n",
    "    df['user.id'] = pd.to_numeric(df['user.id'], errors='coerce')\n",
    "    df = df[df['user.id'].isnull() != True]\n",
    "    \n",
    "    # Format dates \n",
    "    df[\"user.created_at\"] = pd.to_datetime(df[\"user.created_at\"], format='%a %b %d %H:%M:%S %z %Y')\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], format='%Y-%m-%d %H:%M:%S').dt.tz_localize('UTC')\n",
    "\n",
    "    return df\n",
    "\n",
    "df = import_data(filelist)\n",
    "\n",
    "######## 4. Split into Train vs. Valid vs. Test (Note particular method due to panel data)\n",
    "#- Test: 20%\n",
    "#- Train: 60%\n",
    "#- Validation: 20%\n",
    "\n",
    "# https://towardsdatascience.com/assigning-panel-data-to-training-testing-and-validation-groups-for-machine-learning-models-7017350ab86e\n",
    "# Here is a few lines of python code to that ensure that your training, testing and validation groups are independent.\n",
    "# Get a Unique List of All IDs (machines).\n",
    "pd_id=df.drop_duplicates(subset='user.id')\n",
    "pd_id=pd_id[['user.id']]\n",
    "\n",
    "# Create a new variable with a random number between 0 and .\n",
    "np.random.seed(42)\n",
    "pd_id['wookie'] = (np.random.randint(0, 10000, pd_id.shape[0]))/10000\n",
    "pd_id=pd_id[['user.id', 'wookie']]\n",
    "\n",
    "#Give each machine a 20% chance of being in the validation, \n",
    "#a 20% chance of being in the testing and a \n",
    "# 60% chance of being in the training data set.\n",
    "# Split into Train vs. Valid vs. Test\n",
    "#- Test: 20%\n",
    "#- Train: 60%\n",
    "#- Validation: 20%\n",
    "pd_id['MODELING_GROUP'] = np.where(((pd_id.wookie <= 0.60)), 'TRAINING', np.where(((pd_id.wookie <= 0.80)), 'VALIDATION', 'TESTING'))\n",
    "\n",
    "tips_summed = pd_id.groupby(['MODELING_GROUP'])['wookie'].count()\n",
    "\n",
    "# Append the Group of each id to each individual record.\n",
    "df=df.sort_values(by=['user.id'], ascending=[True])\n",
    "\n",
    "pd_id=pd_id.sort_values(by=['user.id'], ascending=[True])\n",
    "df = df.merge(pd_id, on=['user.id'], how='inner')\n",
    "\n",
    "#Train\n",
    "X_train = df[df['MODELING_GROUP'] == 'TRAINING']\n",
    "X_train = X_train.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_train = df[df['MODELING_GROUP'] == 'TRAINING'].suspended\n",
    "\n",
    "# Validation\n",
    "X_valid = df[df['MODELING_GROUP'] == 'VALIDATION']\n",
    "X_valid = X_valid.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_valid = df[df['MODELING_GROUP'] == 'VALIDATION'].suspended\n",
    "\n",
    "# Test\n",
    "X_test = df[df['MODELING_GROUP'] == 'TESTING']\n",
    "X_test = X_test.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_test = df[df['MODELING_GROUP'] == 'TESTING'].suspended\n",
    "\n",
    "########## 5. Preprocessing pipeline\n",
    "\n",
    "# Create functions for pipeline\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tweets_per_minute = True): # no *args or **kargs\n",
    "        self.tweets_per_minute = tweets_per_minute\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        X_train_2 = X.copy()\n",
    "        # User Age (tweet created_at - account created_at)\n",
    "        X_train_2[\"user_age\"] = (X_train_2[\"created_at\"] - X_train_2[\"user.created_at\"]).dt.days\n",
    "        # Tweets per day \n",
    "        X_train_2['tweets_per_day'] = X_train_2['user.statuses_count'].astype(float)/X_train_2[\"user_age\"] \n",
    "        # Calculate time since last tweet\n",
    "        X_train_2[\"since_last_tweet_mins\"] = X_train_2.sort_values(['user.id','created_at']).groupby('user.id')['created_at'].diff().dt.seconds.div(60)\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].min().to_frame('since_last_tweet_mins_min'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].max().to_frame('since_last_tweet_mins_max'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].mean().to_frame('since_last_tweet_mins_mean'), on = [\"user.id\"])        \n",
    "        \n",
    "        X_train_2['date'] = X_train_2['created_at'].dt.date\n",
    "        X_train_2['hour'] = X_train_2['created_at'].dt.hour\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2[['user.id', 'date', 'hour', 'id']].groupby(['user.id', 'date', 'hour']).count().groupby('user.id', sort=False)[\"id\"].mean().reset_index(name ='avg_tweets_per_hr'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2[['user.id', 'date', 'id']].groupby(['user.id', 'date']).count().groupby('user.id', sort=False)[\"id\"].mean().reset_index(name ='avg_tweets_per_day'), on = [\"user.id\"])\n",
    "\n",
    "        X_train_2.loc[X_train_2['quoted_status_id'].notna(), 'quoted_status_id'] = 1\n",
    "        X_train_2.loc[X_train_2['quoted_status_id'].isna(), 'quoted_status_id'] = 0\n",
    "        X_train_2['no_hashtags'] = X_train_2['text'].apply(lambda x: len(re.findall(r\"#(\\w+)\", x)))\n",
    "        X_train_2['no_mentions'] = X_train_2['text'].apply(lambda x: len(re.findall(\"@(\\w{1,15})\", x)))\n",
    "        X_train_2['no_urls'] = X_train_2['text'].apply(lambda x: len(re.findall(\"(?P<url>https?://[^\\s]+)\", x)))\n",
    "        X_train_2['tw_len'] = X_train_2['text'].apply(lambda x: len(x))\n",
    "        X_train_2['followers_per_followees'] = X_train_2['user.followers_count'].astype('float')/X_train_2['user.friends_count'].astype('float')\n",
    "\n",
    "        # URLs (percent of tweets with them)\n",
    "        X_train_2[\"containsURL\"] = (X_train_2['no_urls']  > 0).astype(int)\n",
    "        url_counts = X_train_2.groupby('user.id').agg({'created_at':'count', \n",
    "                         'containsURL':'sum'})\n",
    "        url_counts['user.urls_per_tweet'] = url_counts['containsURL']/url_counts['created_at']\n",
    "        X_train_2 = pd.merge(X_train_2, url_counts[['user.urls_per_tweet']], on = [\"user.id\"])  \n",
    "\n",
    "        # Hashtags, Mentions, and URLS\n",
    "        url_counts = X_train_2.groupby('user.id').agg({'created_at':'count', \n",
    "                 'no_hashtags':'sum', 'no_mentions':'sum', 'no_urls':'sum'})\n",
    "        url_counts['no_hashtags_per_tweet'] = url_counts['no_hashtags']/url_counts['created_at']\n",
    "        url_counts['no_mentions_per_tweet'] = url_counts['no_mentions']/url_counts['created_at']\n",
    "        url_counts['no_urls_per_tweet'] = url_counts['no_urls']/url_counts['created_at']\n",
    "        X_train_2 = pd.merge(X_train_2, url_counts[['no_hashtags_per_tweet', 'no_mentions_per_tweet', 'no_urls_per_tweet']], on = [\"user.id\"])  \n",
    "\n",
    "        X_train_2['user.followers_count'] = X_train_2['user.followers_count'].astype('float')\n",
    "        X_train_2['user.friends_count'] = X_train_2['user.friends_count'].astype('float')\n",
    "\n",
    "        # Pace of follower and friend add-on during collected time period \n",
    "        avg_friends_per_day = X_train_2.groupby(['user.id', 'date'], as_index=True).mean()[['user.followers_count', 'user.friends_count']]\n",
    "        avg_friends_change = avg_friends_per_day.sort_values(['user.id','date']).groupby('user.id').diff().rename(columns={'user.followers_count':'user.followers_countdailychange','user.friends_count' : 'user.friends_countdailychange'})\n",
    "        X_train_2 = pd.merge(X_train_2, avg_friends_change.groupby(['user.id'], as_index=True)[['user.followers_countdailychange', 'user.friends_countdailychange']].mean(), on = [\"user.id\"]) \n",
    "\n",
    "        # Pace of follower and friend add-on overall \n",
    "        X_train_2['user.friend_rate'] = X_train_2['user.friends_count']/X_train_2['user_age']\n",
    "        X_train_2['user.followers_rate'] = X_train_2['user.followers_count']/X_train_2['user_age']\n",
    "\n",
    "        X_train_2['user.has_url'] = (X_train_2['user.url'].fillna(False) != False).astype(int)\n",
    "        X_train_2['user.has_location'] = (X_train_2['user.location'].fillna(False) != False).astype(int)\n",
    "        X_train_2['user.screen_name.digit_length'] = X_train_2['user.screen_name'].apply(lambda x: len(re.sub(\"[^0-9]\", \"\", x)) if pd.notnull(x) else x)\n",
    "        X_train_2['user.screen_name.length'] = X_train_2['user.screen_name'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in tweet\n",
    "        X_train_2['text'] = X_train_2['text'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "        X_train_2['text'] = X_train_2['text'].apply(lambda x: preprocess_text(x) if pd.notnull(x) else x)\n",
    "        X_train_2['text'] = X_train_2['text'].apply(lambda x: remove_tags(x) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in bio \n",
    "        X_train_2['user.description'] = X_train_2['user.description'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in name\n",
    "        X_train_2['user.name'] = X_train_2['user.name'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Create binary for whether it is reply or not \n",
    "        X_train_2['is_reply'] = (X_train_2['in_reply_to_status_id'].fillna(False) != False).astype(int)\n",
    "\n",
    "        X_train_2 = X_train_2[['id','created_at', 'text', 'source', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive',\n",
    "                              'withheld_in_countries', 'place.country', 'quoted_status_id', 'user.id', 'user.created_at',\n",
    "                              'user.description', 'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
    "                               'user.geo_enabled', 'user.has_extended_profile', 'user.lang', 'user.listed_count',\n",
    "                               'user.location', 'user.name', 'user.protected', 'user.screen_name', 'user.statuses_count',\n",
    "                               'user.time_zone', 'user.verified', 'user.protected', 'user.default_profile',\n",
    "                               'is_quote_status', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count',\n",
    "                               'retweeted_status.user.followers_count', 'retweeted_status.user.friends_count', 'user_age', \n",
    "                              'tweets_per_day', 'since_last_tweet_mins', 'since_last_tweet_mins_min',\n",
    "                               'since_last_tweet_mins_max', 'since_last_tweet_mins_mean', 'avg_tweets_per_hr', \n",
    "                               'avg_tweets_per_day', 'no_hashtags', 'no_mentions', 'no_urls', 'tw_len',\n",
    "                              'followers_per_followees', 'containsURL',  'user.urls_per_tweet', 'no_hashtags_per_tweet',\n",
    "                              'no_mentions_per_tweet', 'no_urls_per_tweet', 'user.followers_countdailychange', \n",
    "                               'user.friends_countdailychange', 'user.friend_rate', 'user.followers_rate',\n",
    "                              'user.has_url', 'user.has_location', 'user.screen_name.digit_length', \n",
    "                               'user.screen_name.length', 'is_reply', 'suspended']]\n",
    "        return X_train_2\n",
    "    \n",
    "####### 6. Implement Pipeline and Save Test, Train, Valid Files to S3 bucket\n",
    "test_pipeline = Pipeline([\n",
    "        #('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder())\n",
    "        #('std_scaler', StandardScaler()), # feature scaling\n",
    "    ])\n",
    "\n",
    "# Train\n",
    "X_train_tr = test_pipeline.fit_transform(X_train)\n",
    "\n",
    "X_train_tr.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + \"x_train.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "y_train.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + \"y_train.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Test\n",
    "X_train_tr = test_pipeline.fit_transform(X_test)\n",
    "X_train_tr.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + \"x_test.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "y_test.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + \"y_test.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Valid \n",
    "X_train_tr = test_pipeline.fit_transform(X_valid)\n",
    "X_train_tr.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + \"x_validation.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "y_valid.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + \"y_validation.csv\").put(Body=csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

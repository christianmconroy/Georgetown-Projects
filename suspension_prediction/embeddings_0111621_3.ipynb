{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying BERT Multilingual Classifier to Predict Account Suspension \n",
    "\n",
    "Guidance from: https://github.com/kacossio/TeamPython/blob/master/Bert%20Multilingual%20Embedding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Load Packages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "import emoji\n",
    "import random \n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters \n",
    "########## Set Parameters\n",
    "\n",
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "filepath = \"data/\"\n",
    "\n",
    "import_bucket = \"joe-exotic-2020\"\n",
    "\n",
    "embedding_bucket = \"modeling/embeddings\"\n",
    "\n",
    "key = 'full_clean' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "results_bucket = 'full_clean' # already created on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load in Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe.\n",
    "    Inputs: Filelist \n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "\n",
    "    # Read in single file\n",
    "    object_key = filelist[0].split('/', 1)[1]\n",
    "    csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_string), error_bad_lines=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load in data from S3\n",
    "\n",
    "# Import Train and Measure Balance\n",
    "# Import Flattened Data\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"train_updated\",obj.key)]\n",
    "\n",
    "df_train = import_data(filelist)\n",
    "\n",
    "df_train['suspended'] = pd.to_numeric(df_train['suspended'], errors='coerce')\n",
    "df_train = df_train[df_train['suspended'].notna()]\n",
    "\n",
    "# Import Test and Measure Balance\n",
    "\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"test_updated\",obj.key)]\n",
    "\n",
    "df_test = import_data(filelist)\n",
    "\n",
    "df_test['suspended'] = pd.to_numeric(df_test['suspended'], errors='coerce')\n",
    "df_test = df_test[df_test['suspended'].notna()]\n",
    "\n",
    "# Import Validation and Measure Balance\n",
    "# Import Flattened Data\n",
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"valid_updated\",obj.key)]\n",
    "\n",
    "df_valid = import_data(filelist)\n",
    "\n",
    "df_valid['suspended'] = pd.to_numeric(df_valid['suspended'], errors='coerce')\n",
    "df_valid = df_valid[df_valid['suspended'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that Target Variable is Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['suspended'] = df_train['suspended'].astype(int)\n",
    "df_valid['suspended'] = df_valid['suspended'].astype(int)\n",
    "df_test['suspended'] = df_test['suspended'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['id', 'created_at', 'text'])\n",
    "df_valid = df_valid.drop_duplicates(subset=['id', 'created_at', 'text'])\n",
    "df_test = df_test.drop_duplicates(subset=['id', 'created_at', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure binary possibly_sensitive vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['possibly_sensitive'][df_train['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan\n",
    "df_valid['possibly_sensitive'][df_valid['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan\n",
    "df_test['possibly_sensitive'][df_test['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages \n",
    "from translate import Translator\n",
    "import spacy\n",
    "import langid\n",
    "import keras_bert\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime as dt\n",
    "import pytz\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Dates to Unix Epoch Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to convert dates into float (Unix Epoch Times )\n",
    "def convert_dates_float(df):\n",
    "    '''\n",
    "    Convert key input data variables to numeric format for tensors. Uses unix epoch time in seconds. \n",
    "    '''\n",
    "    # created_at (tweet)\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    my_datetime = dt.datetime(1970,1,1) \n",
    "    good_dt = pytz.timezone('UTC').localize(my_datetime)\n",
    "    df['created_at'] = (df['created_at'] - good_dt).dt.total_seconds()\n",
    "\n",
    "    # User.created_at (account)\n",
    "    df['user.created_at'] = pd.to_datetime(df['user.created_at'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    my_datetime = dt.datetime(1970,1,1) \n",
    "    good_dt = pytz.timezone('UTC').localize(my_datetime)\n",
    "    df['user.created_at'] = (df['user.created_at'] - good_dt).dt.total_seconds()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert binary and categorical variables to one-hot encoded (not sure this is best or not)\n",
    "\n",
    "Options \n",
    "\n",
    "- Integer Encoding: Where each unique label is mapped to an integer.\n",
    "- One Hot Encoding: Where each label is mapped to a binary vector.\n",
    "- Learned Embedding: Where a distributed representation of the categories is learned.\n",
    "\n",
    "We use one hot encoding below. \n",
    "\n",
    "#### We use get_dummies below instead of one_hot_encoder as get dummies knows how to deal with missingness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One Hote Encoding (Unix Epoch Times )\n",
    "def one_hot(df_train, df_valid, df_test): \n",
    "    '''\n",
    "    One hot encoding requires the full dataset in order to ensure that there end up the same amount of columns for test, validation and train.\n",
    "    We therefore combine train, valid, and test, fill nas with 0 where necessary, and one hot encode categorical vars. \n",
    "    '''\n",
    "    df_train['split'] = \"train\"\n",
    "    df_valid['split'] = \"valid\"\n",
    "    df_test['split'] = \"test\"\n",
    "    df = pd.concat([df_train, df_test, df_valid], ignore_index=True, sort=False)\n",
    "    df = convert_dates_float(df)\n",
    "    # Extra layer of Processing \n",
    "    df = df[df['retweet_count'] != \"False\"] \n",
    "    df['quoted_status.user.followers_count'] = df['quoted_status.user.followers_count'].fillna(0) \n",
    "    df['quoted_status.user.friends_count'] = df['quoted_status.user.friends_count'].fillna(0) \n",
    "    df['retweeted_status.user.followers_count'] = df['retweeted_status.user.followers_count'].fillna(0) \n",
    "    df['retweeted_status.user.friends_count'] = df['retweeted_status.user.friends_count'].fillna(0) \n",
    "    # One-hot\n",
    "    df = df.drop([\"user.protected.1\", \"user.protected.2\", \"user.protected.3\"], axis=1)\n",
    "    df = pd.get_dummies(df, columns=[\"source\", \"lang\", \"possibly_sensitive\", \"withheld_in_countries\", \"place.country\", \n",
    "                                         \"user.geo_enabled\", \"user.lang\", \"user.verified\", \"user.has_extended_profile\",\n",
    "                                        \"user.lang\", \"user.protected\", \"user.time_zone\", \"user.verified\", \"user.default_profile\",\n",
    "                                        \"is_quote_status\"])\n",
    "    return df\n",
    "# Tp get rid of: Text, user.protected.1, user.protected.2, user.protected.3, \n",
    "# To concat (or get rid of): user.description, user.location, user.name, user.screen_name\n",
    "# to potentially take out entirely - user.id (This would explain everything)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split one-hot encoded df back apart into train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot(df_train, df_valid, df_test)\n",
    "df_train_f = df[df['split'] == \"train\"]\n",
    "df_valid_f = df[df['split'] == \"valid\"]\n",
    "df_test_f = df[df['split'] == \"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove other text fields (may concatanate with tweets in future iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_f.drop(['user.description', \"user.location\", \"user.name\", \"user.screen_name\", \"split\"], axis=1)\n",
    "df_valid = df_valid_f.drop(['user.description', \"user.location\", \"user.name\", \"user.screen_name\", \"split\"], axis=1)\n",
    "df_test = df_test_f.drop(['user.description', \"user.location\", \"user.name\", \"user.screen_name\", \"split\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>...</th>\n",
       "      <th>user.geo_enabled_False</th>\n",
       "      <th>user.geo_enabled_True</th>\n",
       "      <th>user.verified_False</th>\n",
       "      <th>user.has_extended_profile_False</th>\n",
       "      <th>user.has_extended_profile_True</th>\n",
       "      <th>user.protected_False</th>\n",
       "      <th>user.verified_False</th>\n",
       "      <th>user.default_profile_True</th>\n",
       "      <th>is_quote_status_False</th>\n",
       "      <th>is_quote_status_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304799e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>containcontrast you are as worse as nazi germ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>RT bcelyj nevernever maryann CCP CCCP CCCP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>RT maryann CCP CCP https co EAFQGqFQ</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304795e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>https co oBzs zO https co Si btzc U</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.304794e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>https co oBzs zO Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  \\\n",
       "1  1.304799e+18  1.599923e+09   \n",
       "2  1.304796e+18  1.599923e+09   \n",
       "3  1.304796e+18  1.599923e+09   \n",
       "4  1.304795e+18  1.599922e+09   \n",
       "5  1.304794e+18  1.599922e+09   \n",
       "\n",
       "                                                text retweet_count  \\\n",
       "1   containcontrast you are as worse as nazi germ...           0.0   \n",
       "2        RT bcelyj nevernever maryann CCP CCCP CCCP            1.0   \n",
       "3               RT maryann CCP CCP https co EAFQGqFQ         802.0   \n",
       "4                https co oBzs zO https co Si btzc U           0.0   \n",
       "5                                 https co oBzs zO Z           0.0   \n",
       "\n",
       "  favorite_count  quoted_status_id       user.id  user.created_at  \\\n",
       "1            1.0               0.0  1.278120e+18     1.593563e+09   \n",
       "2            0.0               0.0  1.278120e+18     1.593563e+09   \n",
       "3            0.0               0.0  1.278120e+18     1.593563e+09   \n",
       "4           15.0               1.0  1.278120e+18     1.593563e+09   \n",
       "5           10.0               1.0  1.278120e+18     1.593563e+09   \n",
       "\n",
       "   user.favourites_count  user.followers_count  ...  user.geo_enabled_False  \\\n",
       "1                25355.0                 377.0  ...                       1   \n",
       "2                25355.0                 377.0  ...                       1   \n",
       "3                25355.0                 377.0  ...                       1   \n",
       "4                25355.0                 377.0  ...                       1   \n",
       "5                25355.0                 377.0  ...                       1   \n",
       "\n",
       "   user.geo_enabled_True  user.verified_False  \\\n",
       "1                      0                    1   \n",
       "2                      0                    1   \n",
       "3                      0                    1   \n",
       "4                      0                    1   \n",
       "5                      0                    1   \n",
       "\n",
       "   user.has_extended_profile_False  user.has_extended_profile_True  \\\n",
       "1                                0                               1   \n",
       "2                                0                               1   \n",
       "3                                0                               1   \n",
       "4                                0                               1   \n",
       "5                                0                               1   \n",
       "\n",
       "   user.protected_False  user.verified_False  user.default_profile_True  \\\n",
       "1                     1                    1                          1   \n",
       "2                     1                    1                          1   \n",
       "3                     1                    1                          1   \n",
       "4                     1                    1                          1   \n",
       "5                     1                    1                          1   \n",
       "\n",
       "   is_quote_status_False  is_quote_status_True  \n",
       "1                      1                     0  \n",
       "2                      1                     0  \n",
       "3                      1                     0  \n",
       "4                      0                     1  \n",
       "5                      0                     1  \n",
       "\n",
       "[5 rows x 162 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embeddings functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweets from dataframe\n",
    "def get_tweets_list(df):\n",
    "    '''\n",
    "    Convert panda series of tweets into list of tweets and output id_list for tweets. \n",
    "    '''\n",
    "    all_tweets = df['text'].tolist()\n",
    "    id_list = df['id'].tolist()\n",
    "    return all_tweets,id_list\n",
    "\n",
    "#input list of tweets with structure as [[tweet, screen_name],[tweet, screen_name],[tweet, screen_name],...]\n",
    "def get_bert_embeddings(tweet_list):\n",
    "    '''\n",
    "    Extracy embeddings using tf hub supplied model path and your tweet list.\n",
    "    '''\n",
    "    model_path = \"multi_cased_L-12_H-768_A-12\"\n",
    "    embeddings = keras_bert.extract_embeddings(model_path, tweet_list)\n",
    "    print('embeddings complete')\n",
    "    return(embeddings)\n",
    "\n",
    "#mean pool the embeddings to return 768 embeddings per sentence\n",
    "def avg_pooling(embed_array):\n",
    "    '''\n",
    "    Mean pool the embeddings to return 768 embeddings per sentence. \n",
    "    '''\n",
    "    embeddings_pooled = []\n",
    "    for sentence in embed_array:\n",
    "        sentence = np.expand_dims((sentence),axis = 0)\n",
    "        sentence = tf.keras.layers.GlobalAveragePooling1D()(sentence)\n",
    "        embeddings_pooled.append(np.squeeze(sentence))\n",
    "    return(embeddings_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only the below on Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import cuda \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla K80\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing pre-trained BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-multilingual-cased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "#model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we’ll tokenize and process all sentences together as a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x, pl):\n",
    "    return int(math.ceil(x / pl)) * pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_bert_embeddings(df, file_save):\n",
    "    full_range = range(0,roundup(len(df), 1000),1000)\n",
    "    # First iteration crashed GPU memory after 45,000 (44), so restarted kernel and reran starting at 46,000 (45)\n",
    "    # Had to rerun 46,000-47,000 in smaller chunks given larger tensors evidently. \n",
    "    # Started over at 46 (47,000)\n",
    "    # Same thing happened when we got to 63,000\n",
    "    b = range(0,len(full_range))\n",
    "    for a in b:\n",
    "        try: \n",
    "            model = model_class.from_pretrained(pretrained_weights)\n",
    "            model = model.to('cuda')\n",
    "            if a < b[-1]:\n",
    "                df_train_for_merge = df[full_range[a]:full_range[a+1]]\n",
    "                tokenized = df_train_for_merge['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, truncation = True)))\n",
    "                # Padding \n",
    "                # The dataset is currently a list (or pandas Series/DataFrame) of lists. Before BERT can process this as input, \n",
    "                # we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0. \n",
    "                # You can refer to the notebook for the padding step, it’s basic python string and array manipulation.\n",
    "                max_len = 0\n",
    "                for i in tokenized.values:\n",
    "                    if len(i) > max_len:\n",
    "                        max_len = len(i)\n",
    "\n",
    "                padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "                # After the padding, we have a matrix/tensor that is ready to be passed to BERT:\n",
    "                # Mask\n",
    "                attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "                # Input IDs and Mask \n",
    "                # We now create an input tensor out of the padded token matrix, and send that to BERT\n",
    "                input_ids = torch.tensor(padded)  \n",
    "\n",
    "                attention_mask = torch.tensor(attention_mask)\n",
    "            else: \n",
    "                df_train_for_merge = df[full_range[a]:]\n",
    "                tokenized = df_train_for_merge['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, truncation = True)))\n",
    "\n",
    "                # Padding \n",
    "                # The dataset is currently a list (or pandas Series/DataFrame) of lists. Before BERT can process this as input, \n",
    "                # we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0. \n",
    "                # You can refer to the notebook for the padding step, it’s basic python string and array manipulation.\n",
    "                max_len = 0\n",
    "                for i in tokenized.values:\n",
    "                    if len(i) > max_len:\n",
    "                        max_len = len(i)\n",
    "\n",
    "                padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "                # After the padding, we have a matrix/tensor that is ready to be passed to BERT:\n",
    "                # Mask\n",
    "                attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "                # Input IDs and Mask \n",
    "                # We now create an input tensor out of the padded token matrix, and send that to BERT\n",
    "                input_ids = torch.tensor(padded)  \n",
    "\n",
    "                attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "            input_ids = input_ids.to('cuda')\n",
    "            attention_mask = attention_mask.to('cuda')\n",
    "\n",
    "\n",
    "            # Disabling gradient calculation is useful for inference, when you are sure that you will not call \n",
    "            # Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "            with torch.no_grad():\n",
    "                last_hidden_states = model(input_ids, attention_mask)\n",
    "\n",
    "            # Are there options to set the batch size, etc. and stuff here though? \n",
    "\n",
    "            # Takes a total of 24 seconds with the pytorch implementation on CPU (faster than Keras bert)\n",
    "            # Note if we run the same 300 sample with everythin on Cuda, it takes .035-.044 (GPU is a big deal)\n",
    "            # If we put just the model on cuda and not the inputs, we get an error as they must be on the same device. \n",
    "\n",
    "            bert_pooled_train = last_hidden_states[0][:,0,:].cpu().numpy()\n",
    "\n",
    "            id_tweet_train = df_train_for_merge['id'].tolist()\n",
    "\n",
    "            #bert embedding\n",
    "            bert_embeddings_df_train = pd.DataFrame(list(zip(id_tweet_train, bert_pooled_train)),columns=['id', 'Bert_embeddings'])\n",
    "            bert_embeddings_df_train = pd.concat([bert_embeddings_df_train['id'], bert_embeddings_df_train['Bert_embeddings'].apply(pd.Series)], axis = 1)\n",
    "            bert_embeddings_df_train = pd.merge(df_train_for_merge.drop(['text'], axis=1), bert_embeddings_df_train, left_on = [\"id\"], right_on = [\"id\"])\n",
    "\n",
    "            bert_embeddings_df_train.to_csv(('s3://joe-exotic-2020/modeling/embeddings/' + file_save + \"_\" + str(full_range[a]) + \".csv\"), index=False, encoding = \"utf_8_sig\")\n",
    "            print(\"Embeddings Extracted for Slice\" + \" \" + str(full_range[a]))\n",
    "            del input_ids, attention_mask, model, df_train_for_merge\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except: \n",
    "            c = range(full_range[a], full_range[a+1]+50, 50)\n",
    "            d = range(0,len(c))\n",
    "            for e in d:\n",
    "                model = model_class.from_pretrained(pretrained_weights)\n",
    "                model = model.to('cuda')\n",
    "                try: \n",
    "                    df_train_for_merge = df[c[e]:c[e+1]]\n",
    "                    tokenized = df_train_for_merge['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, truncation = True)))\n",
    "                    # Padding \n",
    "                    # The dataset is currently a list (or pandas Series/DataFrame) of lists. Before BERT can process this as input, \n",
    "                    # we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0. \n",
    "                    # You can refer to the notebook for the padding step, it’s basic python string and array manipulation.\n",
    "                    max_len = 0\n",
    "                    for i in tokenized.values:\n",
    "                        if len(i) > max_len:\n",
    "                            max_len = len(i)\n",
    "\n",
    "                    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "                    # After the padding, we have a matrix/tensor that is ready to be passed to BERT:\n",
    "                    # Mask\n",
    "                    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "                    # Input IDs and Mask \n",
    "                    # We now create an input tensor out of the padded token matrix, and send that to BERT\n",
    "                    input_ids = torch.tensor(padded)  \n",
    "\n",
    "                    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "                    input_ids = input_ids.to('cuda')\n",
    "                    attention_mask = attention_mask.to('cuda')\n",
    "                    # Disabling gradient calculation is useful for inference, when you are sure that you will not call \n",
    "                    # Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "                    with torch.no_grad():\n",
    "                        last_hidden_states = model(input_ids, attention_mask)\n",
    "\n",
    "                    # Are there options to set the batch size, etc. and stuff here though? \n",
    "\n",
    "                    # Takes a total of 24 seconds with the pytorch implementation on CPU (faster than Keras bert)\n",
    "                    # Note if we run the same 300 sample with everythin on Cuda, it takes .035-.044 (GPU is a big deal)\n",
    "                    # If we put just the model on cuda and not the inputs, we get an error as they must be on the same device. \n",
    "\n",
    "                    bert_pooled_train = last_hidden_states[0][:,0,:].cpu().numpy()\n",
    "\n",
    "                    id_tweet_train = df_train_for_merge['id'].tolist()\n",
    "\n",
    "                    #bert embedding\n",
    "                    bert_embeddings_df_train = pd.DataFrame(list(zip(id_tweet_train, bert_pooled_train)),columns=['id', 'Bert_embeddings'])\n",
    "                    bert_embeddings_df_train = pd.concat([bert_embeddings_df_train['id'], bert_embeddings_df_train['Bert_embeddings'].apply(pd.Series)], axis = 1)\n",
    "                    bert_embeddings_df_train = pd.merge(df_train_for_merge.drop(['text'], axis=1), bert_embeddings_df_train, left_on = [\"id\"], right_on = [\"id\"])\n",
    "\n",
    "                    bert_embeddings_df_train.to_csv(('s3://joe-exotic-2020/modeling/embeddings/' + file_save + \"_\" + str(c[e]) + \"_\" + str(c[e+1]) + \".csv\"), index=False, encoding = \"utf_8_sig\")\n",
    "                    print(\"Embeddings Extracted for Slice\" + \" \" + str(c[e]) + \"_\" + str(c[e+1]))\n",
    "                    del input_ids, attention_mask, model, df_train_for_merge\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start= time.time()\n",
    "file_save =  'ml_train_embeddings_df'\n",
    "pytorch_bert_embeddings(df_train, file_save)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Extracted for Slice 32000\n",
      "Embeddings Extracted for Slice 33000\n",
      "Embeddings Extracted for Slice 34000_34050\n",
      "Embeddings Extracted for Slice 34050_34100\n",
      "Embeddings Extracted for Slice 34100_34150\n",
      "Embeddings Extracted for Slice 34150_34200\n",
      "Embeddings Extracted for Slice 34200_34250\n",
      "Embeddings Extracted for Slice 34250_34300\n",
      "Embeddings Extracted for Slice 34300_34350\n",
      "Embeddings Extracted for Slice 34350_34400\n",
      "Embeddings Extracted for Slice 34400_34450\n",
      "Embeddings Extracted for Slice 34450_34500\n",
      "Embeddings Extracted for Slice 34500_34550\n",
      "Embeddings Extracted for Slice 34550_34600\n",
      "Embeddings Extracted for Slice 34600_34650\n",
      "Embeddings Extracted for Slice 34650_34700\n",
      "Embeddings Extracted for Slice 34700_34750\n",
      "Embeddings Extracted for Slice 34750_34800\n",
      "Embeddings Extracted for Slice 34800_34850\n",
      "Embeddings Extracted for Slice 34850_34900\n",
      "Embeddings Extracted for Slice 34900_34950\n",
      "Embeddings Extracted for Slice 34950_35000\n",
      "Embeddings Extracted for Slice 35000\n",
      "Embeddings Extracted for Slice 36000\n",
      "Embeddings Extracted for Slice 37000\n",
      "Embeddings Extracted for Slice 38000\n",
      "343.4555525779724\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "file_save =  'ml_valid_embeddings_df'\n",
    "pytorch_bert_embeddings(df_valid, file_save)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Extracted for Slice 0\n",
      "Embeddings Extracted for Slice 1000\n",
      "Embeddings Extracted for Slice 2000\n",
      "Embeddings Extracted for Slice 3000\n",
      "Embeddings Extracted for Slice 4000\n",
      "Embeddings Extracted for Slice 5000\n",
      "Embeddings Extracted for Slice 6000\n",
      "Embeddings Extracted for Slice 7000\n",
      "Embeddings Extracted for Slice 8000\n",
      "Embeddings Extracted for Slice 9000\n",
      "Embeddings Extracted for Slice 10000\n",
      "Embeddings Extracted for Slice 11000\n",
      "Embeddings Extracted for Slice 12000\n",
      "Embeddings Extracted for Slice 13000\n",
      "Embeddings Extracted for Slice 14000\n",
      "Embeddings Extracted for Slice 15000\n",
      "Embeddings Extracted for Slice 16000\n",
      "Embeddings Extracted for Slice 17000\n",
      "Embeddings Extracted for Slice 18000\n",
      "Embeddings Extracted for Slice 19000\n",
      "Embeddings Extracted for Slice 20000\n",
      "Embeddings Extracted for Slice 21000\n",
      "Embeddings Extracted for Slice 22000\n",
      "Embeddings Extracted for Slice 23000\n",
      "Embeddings Extracted for Slice 24000\n",
      "Embeddings Extracted for Slice 25000\n",
      "Embeddings Extracted for Slice 26000\n",
      "Embeddings Extracted for Slice 27000\n",
      "Embeddings Extracted for Slice 28000\n",
      "Embeddings Extracted for Slice 29000\n",
      "534.1619329452515\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "file_save =  'ml_test_embeddings_df'\n",
    "pytorch_bert_embeddings(df_test, file_save)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Import Data\n",
    "def import_list_data(filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe. Concatenates multiple files if necessary. \n",
    "    Inputs: Directory path, number of rows to skip\n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "    dataframes = []\n",
    "    # Iterate through files of the directory\n",
    "    for filename in filelist:\n",
    "        object_key = filename.split('/', 1)[1]\n",
    "        csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "        body = csv_obj['Body']\n",
    "        csv_string = body.read().decode('utf-8')\n",
    "        dataframe = pd.read_csv(StringIO(csv_string))\n",
    "        dataframes.append(dataframe)\n",
    "    df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_0.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_1000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_10000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_100000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_101000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_102000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_103000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_104000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_105000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_106000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_107000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_11000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_12000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_13000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_14000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_15000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_16000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_17000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_18000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_19000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_2000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_20000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_21000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_22000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_23000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_24000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_25000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_26000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_27000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_28000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_29000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_3000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_30000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_31000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_32000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_33000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_34000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_35000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_36000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_37000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_38000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_39000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_4000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_40000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_41000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_42000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_43000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_44000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_45000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_46000_46300.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_46300_46500.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_46500_47000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_47000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_48000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_49000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_5000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_50000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_51000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_52000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_53000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_54000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_55000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_56000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_57000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_58000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_59000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_6000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_60000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_61000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_62000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_63000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64000_64050.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64050_64100.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64100_64150.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64150_64200.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64200_64250.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64250_64300.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64300_64350.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64350_64400.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64400_64450.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64450_64500.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64500_64550.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64550_64600.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64600_64650.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64650_64700.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64700_64750.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64750_64800.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64800_64850.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64850_64900.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64900_64950.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_64950_65000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_65000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_66000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_67000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_68000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69000_69050.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69050_69100.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69100_69150.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69150_69200.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69200_69250.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69250_69300.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69300_69350.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69350_69400.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69400_69450.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69450_69500.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69500_69550.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69550_69600.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69600_69650.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69650_69700.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69700_69750.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69750_69800.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69800_69850.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69850_69900.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69900_69950.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_69950_70000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_7000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_70000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_71000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_72000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_73000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_74000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_75000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_76000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_77000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_78000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_79000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_8000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_80000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_81000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_82000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_83000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_84000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_85000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_86000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_87000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_88000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_89000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_9000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_90000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_91000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_92000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_93000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_94000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_95000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_96000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_97000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_98000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_99000.csv']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"ml_train_embeddings\",obj.key)]\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_train = import_list_data(filelist)\n",
    "len(bert_embeddings_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.304799e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254089</td>\n",
       "      <td>-0.092161</td>\n",
       "      <td>-0.177658</td>\n",
       "      <td>-0.322701</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.462816</td>\n",
       "      <td>0.165087</td>\n",
       "      <td>-0.072143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114728</td>\n",
       "      <td>-0.077202</td>\n",
       "      <td>-0.269712</td>\n",
       "      <td>-0.151349</td>\n",
       "      <td>0.033660</td>\n",
       "      <td>0.276272</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>0.664379</td>\n",
       "      <td>-0.040229</td>\n",
       "      <td>-0.356698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062325</td>\n",
       "      <td>-0.218971</td>\n",
       "      <td>-0.567088</td>\n",
       "      <td>-0.617004</td>\n",
       "      <td>0.170469</td>\n",
       "      <td>0.163738</td>\n",
       "      <td>0.165489</td>\n",
       "      <td>0.253347</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>-0.174792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304795e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333985</td>\n",
       "      <td>-0.193174</td>\n",
       "      <td>-0.613174</td>\n",
       "      <td>-0.672665</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.253517</td>\n",
       "      <td>0.135646</td>\n",
       "      <td>0.110566</td>\n",
       "      <td>0.269420</td>\n",
       "      <td>-0.121226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304794e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215525</td>\n",
       "      <td>-0.281756</td>\n",
       "      <td>-0.346513</td>\n",
       "      <td>-0.911152</td>\n",
       "      <td>-0.005457</td>\n",
       "      <td>0.121276</td>\n",
       "      <td>0.145280</td>\n",
       "      <td>0.193568</td>\n",
       "      <td>0.342988</td>\n",
       "      <td>0.069493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  retweet_count  favorite_count  \\\n",
       "0  1.304799e+18  1.599923e+09            0.0             1.0   \n",
       "1  1.304796e+18  1.599923e+09            1.0             0.0   \n",
       "2  1.304796e+18  1.599923e+09          802.0             0.0   \n",
       "3  1.304795e+18  1.599922e+09            0.0            15.0   \n",
       "4  1.304794e+18  1.599922e+09            0.0            10.0   \n",
       "\n",
       "   quoted_status_id       user.id  user.created_at  user.favourites_count  \\\n",
       "0               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "1               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "2               0.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "3               1.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "4               1.0  1.278120e+18     1.593563e+09                25355.0   \n",
       "\n",
       "   user.followers_count  user.friends_count  ...       758       759  \\\n",
       "0                 377.0               774.0  ...  0.254089 -0.092161   \n",
       "1                 377.0               774.0  ...  0.114728 -0.077202   \n",
       "2                 377.0               774.0  ...  0.062325 -0.218971   \n",
       "3                 377.0               774.0  ...  0.333985 -0.193174   \n",
       "4                 377.0               774.0  ...  0.215525 -0.281756   \n",
       "\n",
       "        760       761       762       763       764       765       766  \\\n",
       "0 -0.177658 -0.322701  0.016352 -0.007543  0.031331  0.462816  0.165087   \n",
       "1 -0.269712 -0.151349  0.033660  0.276272  0.065734  0.664379 -0.040229   \n",
       "2 -0.567088 -0.617004  0.170469  0.163738  0.165489  0.253347  0.105800   \n",
       "3 -0.613174 -0.672665  0.012190  0.253517  0.135646  0.110566  0.269420   \n",
       "4 -0.346513 -0.911152 -0.005457  0.121276  0.145280  0.193568  0.342988   \n",
       "\n",
       "        767  \n",
       "0 -0.072143  \n",
       "1 -0.356698  \n",
       "2 -0.174792  \n",
       "3 -0.121226  \n",
       "4  0.069493  \n",
       "\n",
       "[5 rows x 929 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_0.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_1000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_10000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_11000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_12000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_13000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_14000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_15000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_16000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_17000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_18000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_19000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_2000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_20000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_21000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_22000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_23000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_24000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_25000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_26000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_27000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_28000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_29000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_30000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3000_3050.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3050_3100.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31000_31050.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3100_3150.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31050_31100.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31100_31150.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31150_31200.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31200_31250.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31250_31300.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31300_31350.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31350_31400.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31400_31450.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31450_31500.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31500_31550.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3150_3200.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31550_31600.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31600_31650.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31650_31700.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31700_31750.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31750_31800.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31800_31850.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31850_31900.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31900_31950.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_31950_32000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_32000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3200_3250.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3250_3300.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_33000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3300_3350.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3350_3400.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34000_34050.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3400_3450.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34050_34100.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34100_34150.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34150_34200.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34200_34250.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34250_34300.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34300_34350.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34350_34400.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34400_34450.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34450_34500.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34500_34550.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3450_3500.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34550_34600.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34600_34650.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34650_34700.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34700_34750.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34750_34800.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34800_34850.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34850_34900.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34900_34950.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_34950_35000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_35000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3500_3550.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3550_3600.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_36000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3600_3650.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3650_3700.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_37000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3700_3750.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3750_3800.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_38000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3800_3850.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3850_3900.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3900_3950.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_3950_4000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_4000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_5000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_6000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_7000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_8000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_9000.csv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"ml_valid_embeddings\",obj.key)]\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39537"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_valid = import_list_data(filelist)\n",
    "len(bert_embeddings_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.279757e+18</td>\n",
       "      <td>1.593953e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278122e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177418</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>-0.278031</td>\n",
       "      <td>-0.185530</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>0.522608</td>\n",
       "      <td>0.254073</td>\n",
       "      <td>0.107183</td>\n",
       "      <td>-0.213097</td>\n",
       "      <td>-0.048582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.278125e+18</td>\n",
       "      <td>1.593564e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278122e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265097</td>\n",
       "      <td>-0.048022</td>\n",
       "      <td>-0.359503</td>\n",
       "      <td>-0.175341</td>\n",
       "      <td>0.115412</td>\n",
       "      <td>0.233461</td>\n",
       "      <td>0.088037</td>\n",
       "      <td>0.116152</td>\n",
       "      <td>0.329816</td>\n",
       "      <td>0.027202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278845e+18</td>\n",
       "      <td>1.593735e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278129e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279357</td>\n",
       "      <td>-0.046791</td>\n",
       "      <td>-0.050131</td>\n",
       "      <td>-0.540188</td>\n",
       "      <td>0.453747</td>\n",
       "      <td>0.143290</td>\n",
       "      <td>-0.026080</td>\n",
       "      <td>0.072237</td>\n",
       "      <td>0.114676</td>\n",
       "      <td>-0.176587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.278851e+18</td>\n",
       "      <td>1.593737e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278129e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409402</td>\n",
       "      <td>0.247715</td>\n",
       "      <td>-0.528003</td>\n",
       "      <td>-0.368062</td>\n",
       "      <td>0.361968</td>\n",
       "      <td>0.259629</td>\n",
       "      <td>0.344309</td>\n",
       "      <td>0.421999</td>\n",
       "      <td>0.068539</td>\n",
       "      <td>-0.127544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.278847e+18</td>\n",
       "      <td>1.593736e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278129e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067151</td>\n",
       "      <td>0.025943</td>\n",
       "      <td>-0.470204</td>\n",
       "      <td>-0.401065</td>\n",
       "      <td>0.429876</td>\n",
       "      <td>0.340666</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.142442</td>\n",
       "      <td>-0.099972</td>\n",
       "      <td>-0.136322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  retweet_count  favorite_count  \\\n",
       "0  1.279757e+18  1.593953e+09            0.0             0.0   \n",
       "1  1.278125e+18  1.593564e+09            0.0             1.0   \n",
       "2  1.278845e+18  1.593735e+09            0.0             1.0   \n",
       "3  1.278851e+18  1.593737e+09            0.0             0.0   \n",
       "4  1.278847e+18  1.593736e+09            0.0             0.0   \n",
       "\n",
       "   quoted_status_id       user.id  user.created_at  user.favourites_count  \\\n",
       "0               0.0  1.278122e+18     1.593563e+09                    0.0   \n",
       "1               0.0  1.278122e+18     1.593563e+09                    0.0   \n",
       "2               0.0  1.278129e+18     1.593565e+09                   41.0   \n",
       "3               0.0  1.278129e+18     1.593565e+09                   41.0   \n",
       "4               0.0  1.278129e+18     1.593565e+09                   41.0   \n",
       "\n",
       "   user.followers_count  user.friends_count  ...       758       759  \\\n",
       "0                   2.0                53.0  ...  0.177418  0.054650   \n",
       "1                   2.0                53.0  ... -0.265097 -0.048022   \n",
       "2                   1.0                47.0  ...  0.279357 -0.046791   \n",
       "3                   1.0                47.0  ...  0.409402  0.247715   \n",
       "4                   1.0                47.0  ...  0.067151  0.025943   \n",
       "\n",
       "        760       761       762       763       764       765       766  \\\n",
       "0 -0.278031 -0.185530  0.475544  0.522608  0.254073  0.107183 -0.213097   \n",
       "1 -0.359503 -0.175341  0.115412  0.233461  0.088037  0.116152  0.329816   \n",
       "2 -0.050131 -0.540188  0.453747  0.143290 -0.026080  0.072237  0.114676   \n",
       "3 -0.528003 -0.368062  0.361968  0.259629  0.344309  0.421999  0.068539   \n",
       "4 -0.470204 -0.401065  0.429876  0.340666  0.063684  0.142442 -0.099972   \n",
       "\n",
       "        767  \n",
       "0 -0.048582  \n",
       "1  0.027202  \n",
       "2 -0.176587  \n",
       "3 -0.127544  \n",
       "4 -0.136322  \n",
       "\n",
       "[5 rows x 929 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_0.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_1000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_10000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_11000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_12000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_13000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_14000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_15000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_16000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_17000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_18000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_19000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_2000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_20000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_21000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_22000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_23000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_24000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_25000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_26000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_27000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_28000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_29000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_3000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_4000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_5000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_6000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_7000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_8000.csv',\n",
       " 'joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_9000.csv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "    for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "    if re.findall(\"ml_test_embeddings\",obj.key)]\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29139"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_test = import_list_data(filelist)\n",
    "len(bert_embeddings_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.278734e+18</td>\n",
       "      <td>1.593709e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278130e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200770</td>\n",
       "      <td>0.205723</td>\n",
       "      <td>-0.421350</td>\n",
       "      <td>-0.228305</td>\n",
       "      <td>0.457495</td>\n",
       "      <td>0.068868</td>\n",
       "      <td>0.081225</td>\n",
       "      <td>0.269605</td>\n",
       "      <td>-0.175302</td>\n",
       "      <td>-0.022077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.292049e+18</td>\n",
       "      <td>1.596884e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278156e+18</td>\n",
       "      <td>1.593571e+09</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270942</td>\n",
       "      <td>0.135556</td>\n",
       "      <td>-0.294425</td>\n",
       "      <td>-0.134652</td>\n",
       "      <td>0.138386</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.204863</td>\n",
       "      <td>0.453594</td>\n",
       "      <td>0.257261</td>\n",
       "      <td>-0.188632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278256e+18</td>\n",
       "      <td>1.593595e+09</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278212e+18</td>\n",
       "      <td>1.593584e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211961</td>\n",
       "      <td>-0.093005</td>\n",
       "      <td>-0.457276</td>\n",
       "      <td>-0.883988</td>\n",
       "      <td>0.138549</td>\n",
       "      <td>0.115750</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>0.371775</td>\n",
       "      <td>0.046258</td>\n",
       "      <td>-0.095041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.278295e+18</td>\n",
       "      <td>1.593604e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278212e+18</td>\n",
       "      <td>1.593584e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265097</td>\n",
       "      <td>-0.048022</td>\n",
       "      <td>-0.359503</td>\n",
       "      <td>-0.175341</td>\n",
       "      <td>0.115412</td>\n",
       "      <td>0.233461</td>\n",
       "      <td>0.088037</td>\n",
       "      <td>0.116152</td>\n",
       "      <td>0.329816</td>\n",
       "      <td>0.027202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.278346e+18</td>\n",
       "      <td>1.593616e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278212e+18</td>\n",
       "      <td>1.593584e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>-0.405240</td>\n",
       "      <td>-0.403547</td>\n",
       "      <td>-0.278130</td>\n",
       "      <td>0.169948</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.204909</td>\n",
       "      <td>0.457883</td>\n",
       "      <td>0.074630</td>\n",
       "      <td>-0.301944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at  retweet_count  favorite_count  \\\n",
       "0  1.278734e+18  1.593709e+09            0.0             0.0   \n",
       "1  1.292049e+18  1.596884e+09            1.0             3.0   \n",
       "2  1.278256e+18  1.593595e+09           76.0             0.0   \n",
       "3  1.278295e+18  1.593604e+09            0.0             0.0   \n",
       "4  1.278346e+18  1.593616e+09            0.0             0.0   \n",
       "\n",
       "   quoted_status_id       user.id  user.created_at  user.favourites_count  \\\n",
       "0               0.0  1.278130e+18     1.593565e+09                    5.0   \n",
       "1               0.0  1.278156e+18     1.593571e+09                   21.0   \n",
       "2               0.0  1.278212e+18     1.593584e+09                  133.0   \n",
       "3               0.0  1.278212e+18     1.593584e+09                  133.0   \n",
       "4               0.0  1.278212e+18     1.593584e+09                  133.0   \n",
       "\n",
       "   user.followers_count  user.friends_count  ...       758       759  \\\n",
       "0                   1.0                60.0  ...  0.200770  0.205723   \n",
       "1                   0.0                47.0  ...  0.270942  0.135556   \n",
       "2                   0.0                16.0  ...  0.211961 -0.093005   \n",
       "3                   0.0                16.0  ... -0.265097 -0.048022   \n",
       "4                   0.0                16.0  ...  0.064108 -0.405240   \n",
       "\n",
       "        760       761       762       763       764       765       766  \\\n",
       "0 -0.421350 -0.228305  0.457495  0.068868  0.081225  0.269605 -0.175302   \n",
       "1 -0.294425 -0.134652  0.138386 -0.151911  0.204863  0.453594  0.257261   \n",
       "2 -0.457276 -0.883988  0.138549  0.115750 -0.008013  0.371775  0.046258   \n",
       "3 -0.359503 -0.175341  0.115412  0.233461  0.088037  0.116152  0.329816   \n",
       "4 -0.403547 -0.278130  0.169948  0.180723  0.204909  0.457883  0.074630   \n",
       "\n",
       "        767  \n",
       "0 -0.022077  \n",
       "1 -0.188632  \n",
       "2 -0.095041  \n",
       "3  0.027202  \n",
       "4 -0.301944  \n",
       "\n",
       "[5 rows x 929 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Multilngual Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_df_train.to_csv('s3://joe-exotic-2020/modeling/embeddings/ml_train_embeddings_df_full.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_df_valid.to_csv('s3://joe-exotic-2020/modeling/embeddings/ml_valid_embeddings_df_full.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_df_test.to_csv('s3://joe-exotic-2020/modeling/embeddings/ml_test_embeddings_df_full.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABSE: Extracting the new BERT embeddings that learn across languages\n",
    "\n",
    "https://towardsdatascience.com/labse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_url, max_seq_length):\n",
    "    labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "\n",
    "    # Define input.\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                         name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                     name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                      name=\"segment_ids\")\n",
    "        \n",
    "    # LaBSE layer.\n",
    "    pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "    \n",
    "    # The embedding is l2 normalized.\n",
    "    pooled_output = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "\n",
    "    # Define model.\n",
    "    return tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, segment_ids],\n",
    "        outputs=pooled_output), labse_layer\n",
    "\n",
    "max_seq_length = 64\n",
    "labse_model, labse_layer = get_model(\n",
    "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-for-tf2\n",
    "\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "    '''\n",
    "    Set up function to create inputs needed for word embeddings extraction. \n",
    "    '''\n",
    "    input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
    "    for input_string in input_strings:\n",
    "        # Tokenize input.\n",
    "        input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        sequence_length = min(len(input_ids), max_seq_length)\n",
    "\n",
    "    # Padding or truncation.\n",
    "    if len(input_ids) >= max_seq_length:\n",
    "        input_ids = input_ids[:max_seq_length]\n",
    "    else:\n",
    "        input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "    input_ids_all.append(input_ids)\n",
    "    input_mask_all.append(input_mask)\n",
    "    segment_ids_all.append([0] * max_seq_length)\n",
    "\n",
    "    return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\n",
    "\n",
    "def encode(input_text):\n",
    "    '''\n",
    "    Function to extract word embedding for each sentence. \n",
    "    '''\n",
    "    input_ids, input_mask, segment_ids = create_input(\n",
    "        input_text, tokenizer, max_seq_length)\n",
    "    return labse_model([input_ids, input_mask, segment_ids])\n",
    "\n",
    "\n",
    "#input list of tweets with structure as [[tweet, screen_name],[tweet, screen_name],[tweet, screen_name],...]\n",
    "def get_bert_embeddings_labse(tweet_list):\n",
    "    embedded_tweets = list()\n",
    "    for sent in tweet_list:\n",
    "        a = np.array(encode(sent))\n",
    "        embedded_tweets.append(a[0])\n",
    "        if len(embedded_tweets) % 1000 == 0:\n",
    "            print(len(embedded_tweets))    \n",
    "    return embedded_tweets\n",
    "\n",
    "# https://tfhub.dev/google/LaBSE/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008150100708007812\n"
     ]
    }
   ],
   "source": [
    "#extracted first 10 tweets and check the time.\n",
    "start= time.time()\n",
    "list_of_tweets_train, id_tweet_train = get_tweets_list(df_train)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "2125.650132417679\n"
     ]
    }
   ],
   "source": [
    "#extracted first 10 tweets and check the time.\n",
    "start= time.time()\n",
    "bert_pooled_train_labse = get_bert_embeddings_labse(list_of_tweets_train)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "764.0787312984467\n"
     ]
    }
   ],
   "source": [
    "#extracted first 10 tweets and check the time.\n",
    "start= time.time()\n",
    "list_of_tweets_valid, id_tweet_valid = get_tweets_list(df_valid)\n",
    "bert_pooled_valid_labse = get_bert_embeddings_labse(list_of_tweets_valid)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "580.1269092559814\n"
     ]
    }
   ],
   "source": [
    "#extracted first 10 tweets and check the time.\n",
    "start= time.time()\n",
    "list_of_tweets_test, id_tweet_test = get_tweets_list(df_test)\n",
    "bert_pooled_test_labse = get_bert_embeddings_labse(list_of_tweets_test)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the text of each screen name in order to classify per screen name. An alternative that we'll have to try to incorporate is to fit in the time series element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.304799e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.045674</td>\n",
       "      <td>0.016957</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.055194</td>\n",
       "      <td>-0.044291</td>\n",
       "      <td>0.034094</td>\n",
       "      <td>-0.038885</td>\n",
       "      <td>-0.036568</td>\n",
       "      <td>-0.032016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304796e+18</td>\n",
       "      <td>1.599923e+09</td>\n",
       "      <td>802.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.026414</td>\n",
       "      <td>0.023692</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304795e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.030677</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>-0.000807</td>\n",
       "      <td>-0.029937</td>\n",
       "      <td>-0.075138</td>\n",
       "      <td>-0.037401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304794e+18</td>\n",
       "      <td>1.599922e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.278120e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>25355.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071624</td>\n",
       "      <td>0.034412</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.056166</td>\n",
       "      <td>-0.014121</td>\n",
       "      <td>-0.036367</td>\n",
       "      <td>0.041306</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>-0.051179</td>\n",
       "      <td>-0.029693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at retweet_count favorite_count  quoted_status_id  \\\n",
       "0  1.304799e+18  1.599923e+09           0.0            1.0               0.0   \n",
       "1  1.304796e+18  1.599923e+09           1.0            0.0               0.0   \n",
       "2  1.304796e+18  1.599923e+09         802.0            0.0               0.0   \n",
       "3  1.304795e+18  1.599922e+09           0.0           15.0               1.0   \n",
       "4  1.304794e+18  1.599922e+09           0.0           10.0               1.0   \n",
       "\n",
       "        user.id  user.created_at  user.favourites_count  user.followers_count  \\\n",
       "0  1.278120e+18     1.593563e+09                25355.0                 377.0   \n",
       "1  1.278120e+18     1.593563e+09                25355.0                 377.0   \n",
       "2  1.278120e+18     1.593563e+09                25355.0                 377.0   \n",
       "3  1.278120e+18     1.593563e+09                25355.0                 377.0   \n",
       "4  1.278120e+18     1.593563e+09                25355.0                 377.0   \n",
       "\n",
       "   user.friends_count  ...       758       759       760       761       762  \\\n",
       "0               774.0  ...  0.004597  0.045674  0.016957  0.003439  0.055194   \n",
       "1               774.0  ... -0.044256 -0.027914 -0.017669  0.026550  0.047473   \n",
       "2               774.0  ...  0.020261  0.026414  0.023692  0.006574  0.007752   \n",
       "3               774.0  ...  0.009558  0.030677  0.000332  0.065556  0.005900   \n",
       "4               774.0  ...  0.071624  0.034412  0.007865  0.056166 -0.014121   \n",
       "\n",
       "        763       764       765       766       767  \n",
       "0 -0.044291  0.034094 -0.038885 -0.036568 -0.032016  \n",
       "1  0.013402 -0.009568  0.000791  0.002250 -0.052819  \n",
       "2  0.001251  0.003044 -0.043307 -0.004868  0.001397  \n",
       "3  0.003982 -0.000807 -0.029937 -0.075138 -0.037401  \n",
       "4 -0.036367  0.041306  0.005384 -0.051179 -0.029693  \n",
       "\n",
       "[5 rows x 929 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert embedding\n",
    "bert_embeddings_df_train_labse = pd.DataFrame(list(zip(id_tweet_train, bert_pooled_train_labse)),columns=['id', 'Bert_embeddings'])\n",
    "bert_embeddings_df_train_labse = pd.concat([bert_embeddings_df_train_labse['id'], bert_embeddings_df_train_labse['Bert_embeddings'].apply(pd.Series)], axis = 1)\n",
    "bert_embeddings_df_train_labse = pd.merge(df_train.drop(['text'], axis=1), bert_embeddings_df_train_labse, left_on = [\"id\"], right_on = [\"id\"])\n",
    "bert_embeddings_df_train_labse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_train_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_train_labse['suspended'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.279757e+18</td>\n",
       "      <td>1.593953e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278122e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.02655</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.278125e+18</td>\n",
       "      <td>1.593564e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278122e+18</td>\n",
       "      <td>1.593563e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.02655</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278845e+18</td>\n",
       "      <td>1.593735e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278129e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.02655</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.278851e+18</td>\n",
       "      <td>1.593737e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278129e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.02655</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.278847e+18</td>\n",
       "      <td>1.593736e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278129e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.02655</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at retweet_count favorite_count  quoted_status_id  \\\n",
       "0  1.279757e+18  1.593953e+09           0.0            0.0               0.0   \n",
       "1  1.278125e+18  1.593564e+09           0.0            1.0               0.0   \n",
       "2  1.278845e+18  1.593735e+09           0.0            1.0               0.0   \n",
       "3  1.278851e+18  1.593737e+09           0.0            0.0               0.0   \n",
       "4  1.278847e+18  1.593736e+09           0.0            0.0               0.0   \n",
       "\n",
       "        user.id  user.created_at  user.favourites_count  user.followers_count  \\\n",
       "0  1.278122e+18     1.593563e+09                    0.0                   2.0   \n",
       "1  1.278122e+18     1.593563e+09                    0.0                   2.0   \n",
       "2  1.278129e+18     1.593565e+09                   41.0                   1.0   \n",
       "3  1.278129e+18     1.593565e+09                   41.0                   1.0   \n",
       "4  1.278129e+18     1.593565e+09                   41.0                   1.0   \n",
       "\n",
       "   user.friends_count  ...       758       759       760      761       762  \\\n",
       "0                53.0  ... -0.044256 -0.027914 -0.017669  0.02655  0.047473   \n",
       "1                53.0  ... -0.044256 -0.027914 -0.017669  0.02655  0.047473   \n",
       "2                47.0  ... -0.044256 -0.027914 -0.017669  0.02655  0.047473   \n",
       "3                47.0  ... -0.044256 -0.027914 -0.017669  0.02655  0.047473   \n",
       "4                47.0  ... -0.044256 -0.027914 -0.017669  0.02655  0.047473   \n",
       "\n",
       "        763       764       765      766       767  \n",
       "0  0.013402 -0.009568  0.000791  0.00225 -0.052819  \n",
       "1  0.013402 -0.009568  0.000791  0.00225 -0.052819  \n",
       "2  0.013402 -0.009568  0.000791  0.00225 -0.052819  \n",
       "3  0.013402 -0.009568  0.000791  0.00225 -0.052819  \n",
       "4  0.013402 -0.009568  0.000791  0.00225 -0.052819  \n",
       "\n",
       "[5 rows x 929 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert embedding\n",
    "bert_embeddings_df_valid_labse = pd.DataFrame(list(zip(id_tweet_valid, bert_pooled_valid_labse)),columns=['id', 'Bert_embeddings'])\n",
    "bert_embeddings_df_valid_labse = pd.concat([bert_embeddings_df_valid_labse['id'], bert_embeddings_df_valid_labse['Bert_embeddings'].apply(pd.Series)], axis = 1)\n",
    "bert_embeddings_df_valid_labse = pd.merge(df_valid.drop(['text'], axis=1), bert_embeddings_df_valid_labse, left_on = [\"id\"], right_on = [\"id\"])\n",
    "bert_embeddings_df_valid_labse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38537"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_valid_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_valid_labse['suspended'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.278734e+18</td>\n",
       "      <td>1.593709e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278130e+18</td>\n",
       "      <td>1.593565e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007161</td>\n",
       "      <td>0.039254</td>\n",
       "      <td>-0.029584</td>\n",
       "      <td>0.067115</td>\n",
       "      <td>0.046009</td>\n",
       "      <td>-0.025005</td>\n",
       "      <td>-0.002067</td>\n",
       "      <td>-0.001654</td>\n",
       "      <td>-0.080371</td>\n",
       "      <td>-0.024910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.292049e+18</td>\n",
       "      <td>1.596884e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278156e+18</td>\n",
       "      <td>1.593571e+09</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278256e+18</td>\n",
       "      <td>1.593595e+09</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278212e+18</td>\n",
       "      <td>1.593584e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.278295e+18</td>\n",
       "      <td>1.593604e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278212e+18</td>\n",
       "      <td>1.593584e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.278346e+18</td>\n",
       "      <td>1.593616e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.278212e+18</td>\n",
       "      <td>1.593584e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044256</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.052819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 929 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    created_at retweet_count favorite_count  quoted_status_id  \\\n",
       "0  1.278734e+18  1.593709e+09           0.0            0.0               0.0   \n",
       "1  1.292049e+18  1.596884e+09             1            3.0               0.0   \n",
       "2  1.278256e+18  1.593595e+09          76.0            0.0               0.0   \n",
       "3  1.278295e+18  1.593604e+09           0.0            0.0               0.0   \n",
       "4  1.278346e+18  1.593616e+09           0.0            0.0               0.0   \n",
       "\n",
       "        user.id  user.created_at  user.favourites_count  user.followers_count  \\\n",
       "0  1.278130e+18     1.593565e+09                    5.0                   1.0   \n",
       "1  1.278156e+18     1.593571e+09                   21.0                   0.0   \n",
       "2  1.278212e+18     1.593584e+09                  133.0                   0.0   \n",
       "3  1.278212e+18     1.593584e+09                  133.0                   0.0   \n",
       "4  1.278212e+18     1.593584e+09                  133.0                   0.0   \n",
       "\n",
       "   user.friends_count  ...       758       759       760       761       762  \\\n",
       "0                60.0  ... -0.007161  0.039254 -0.029584  0.067115  0.046009   \n",
       "1                47.0  ... -0.044256 -0.027914 -0.017669  0.026550  0.047473   \n",
       "2                16.0  ... -0.044256 -0.027914 -0.017669  0.026550  0.047473   \n",
       "3                16.0  ... -0.044256 -0.027914 -0.017669  0.026550  0.047473   \n",
       "4                16.0  ... -0.044256 -0.027914 -0.017669  0.026550  0.047473   \n",
       "\n",
       "        763       764       765       766       767  \n",
       "0 -0.025005 -0.002067 -0.001654 -0.080371 -0.024910  \n",
       "1  0.013402 -0.009568  0.000791  0.002250 -0.052819  \n",
       "2  0.013402 -0.009568  0.000791  0.002250 -0.052819  \n",
       "3  0.013402 -0.009568  0.000791  0.002250 -0.052819  \n",
       "4  0.013402 -0.009568  0.000791  0.002250 -0.052819  \n",
       "\n",
       "[5 rows x 929 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert embedding\n",
    "bert_embeddings_df_test_labse = pd.DataFrame(list(zip(id_tweet_test, bert_pooled_test_labse)),columns=['id', 'Bert_embeddings'])\n",
    "bert_embeddings_df_test_labse = pd.concat([bert_embeddings_df_test_labse['id'], bert_embeddings_df_test_labse['Bert_embeddings'].apply(pd.Series)], axis = 1)\n",
    "bert_embeddings_df_test_labse = pd.merge(df_test.drop(['text'], axis=1), bert_embeddings_df_test_labse, left_on = [\"id\"], right_on = [\"id\"])\n",
    "bert_embeddings_df_test_labse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29139"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_embeddings_df_test_labse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings_df_test_labse['suspended'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save LabSE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_df_train_labse.to_csv('s3://joe-exotic-2020/modeling/embeddings/labse_train_embeddings_df.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_df_valid_labse.to_csv('s3://joe-exotic-2020/modeling/embeddings/labse_valid_embeddings_df.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_df_test_labse.to_csv('s3://joe-exotic-2020/modeling/embeddings/labse_test_embeddings_df.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
